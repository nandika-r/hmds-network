{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPGbu1AijwNq",
        "outputId": "43c74730-3199-4660-d9bf-53ab72cb6c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-geometric --y\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAOV5lvzjypa",
        "outputId": "4ae52f93-5775-46e2-b5a8-d954de739d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-05yg04ve\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-05yg04ve\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 56d53d03a7326c7882d33345a759df1b02bcb4f2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.10.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.7.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric==2.7.0) (4.12.2)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1136511 sha256=ba71549aeee4a05ce4ba4a15a7b379704a45f47c56c8af8bf87b3243490a1e62\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_schx4yy/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import torch.nn.init as init\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4fHtB0eRj0xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_part = 128       #Number of features per body part in higher dimensional space\n",
        "n_graph_out = 16   #Number of output features from the GCN layer\n",
        "n_rnn = 128        #Hidden size of the LSTM\n",
        "n_rnn_out = 15     #Number of output features from the final BiRNN layer"
      ],
      "metadata": {
        "id": "HJJB0J9Pj5NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seeds(42)\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error reading JSON file: {file_path}\")\n",
        "        return None\n",
        "\n",
        "def calculate_skip_and_keep(frame_rate, target_fps):\n",
        "    \"\"\"\n",
        "    Calculate the number of frames to skip and the number of frames to keep based on the frame rate and target fps.\n",
        "\n",
        "    Parameters:\n",
        "    frame_rate (int): The original frame rate of the video.\n",
        "    target_fps (int): The target frame rate after downsampling.\n",
        "\n",
        "    Returns:\n",
        "    skip_every_n (int): The number of frames to skip in each block.\n",
        "    keep_every_m (int): The number of frames to keep in each block.\n",
        "    \"\"\"\n",
        "    # Calculate the total frames in a block (skip + keep)\n",
        "    total_frames_in_block = frame_rate\n",
        "\n",
        "    # Calculate the number of frames to keep in each block\n",
        "    keep_every_m = int((target_fps / frame_rate) * total_frames_in_block)\n",
        "\n",
        "    # Calculate the number of frames to skip in each block\n",
        "    skip_every_n = total_frames_in_block - keep_every_m\n",
        "\n",
        "    return skip_every_n, keep_every_m\n",
        "\n",
        "def process_openpose_output(directory, batch_size, frame_rate=25, target_fps=25, skip_every_n=0, keep_every_m=0, ignore_first_n=0):\n",
        "    json_files = sorted([f for f in os.listdir(directory) if f.endswith('.json')])\n",
        "    num_files = len(json_files)\n",
        "\n",
        "    if skip_every_n > 0 and keep_every_m > 0:\n",
        "        step_pattern = [(i % (skip_every_n + keep_every_m) < keep_every_m) for i in range(frame_rate)]\n",
        "    else:\n",
        "        step_pattern = [(i % (frame_rate // target_fps) == 0) for i in range(frame_rate)]\n",
        "\n",
        "    selected_files = [f for i, f in enumerate(json_files[ignore_first_n:]) if step_pattern[i % frame_rate]]\n",
        "\n",
        "    if len(selected_files) < (batch_size - 1) + 1:\n",
        "        raise ValueError(f\"Not enough files in the directory after applying the skip pattern. Found {len(selected_files)} files.\")\n",
        "\n",
        "    batches = [selected_files[i:i + batch_size] for i in range(0, len(selected_files), batch_size)]\n",
        "\n",
        "    if len(batches[-1]) < batch_size:\n",
        "        batches = batches[:-1]\n",
        "\n",
        "    all_batches_data = []\n",
        "    for batch in batches:\n",
        "        batch_data = []\n",
        "        for json_file in batch:\n",
        "            file_path = os.path.join(directory, json_file)\n",
        "            data = read_json_file(file_path)\n",
        "            if data:\n",
        "                batch_data.append(data)\n",
        "        if batch_data:\n",
        "            # Include the directory path with each batch\n",
        "            all_batches_data.append((batch_data, batch, directory))\n",
        "\n",
        "    return all_batches_data\n",
        "\n",
        "frame_rate = 25  # Original FPS of the data\n",
        "target_fps = 15  # Desired FPS\n",
        "\n",
        "# Calculate skip and keep parameters dynamically\n",
        "skip_every_n, keep_every_m = calculate_skip_and_keep(frame_rate, target_fps)\n",
        "\n",
        "batch_size = 75  # Number of frames per batch\n",
        "\n",
        "#Data augmentation to effectively double the dataset for more training examples.\n",
        "def flip_keypoints_horizontally(keypoints, image_width=1):\n",
        "    flipped_keypoints = keypoints.copy()\n",
        "    flip_pairs = [\n",
        "        (2, 5), (3, 6), (4, 7), (9, 12), (10, 13), (11, 14), (15, 16), (17, 18), (19, 22), (20, 23), (21, 24)\n",
        "    ]\n",
        "\n",
        "    for i in range(len(keypoints) // 3):\n",
        "        if keypoints[i * 3] >= 0:\n",
        "            flipped_keypoints[i * 3] = image_width - keypoints[i * 3]\n",
        "        else:\n",
        "            flipped_keypoints[i * 3] = keypoints[i * 3]\n",
        "\n",
        "    for (i, j) in flip_pairs:\n",
        "        flipped_keypoints[i * 3], flipped_keypoints[j * 3] = flipped_keypoints[j * 3], flipped_keypoints[i * 3]\n",
        "        flipped_keypoints[i * 3 + 1], flipped_keypoints[j * 3 + 1] = flipped_keypoints[j * 3 + 1], flipped_keypoints[i * 3 + 1]\n",
        "        flipped_keypoints[i * 3 + 2], flipped_keypoints[j * 3 + 2] = flipped_keypoints[j * 3 + 2], flipped_keypoints[i * 3 + 2]\n",
        "\n",
        "    return flipped_keypoints\n",
        "\n",
        "#Making a spatiotemporal matrix - Rows are body parts, columns are timesteps in a video.\n",
        "def create_matrix(data, selected_files, augment=False):\n",
        "    num_frames = len(selected_files)\n",
        "    matrix = [[None] * num_frames for _ in range(len(body_parts))]\n",
        "\n",
        "    for frame_number, (frame_data, json_file) in enumerate(zip(data, selected_files)):\n",
        "        if frame_data['people']:\n",
        "            keypoints = frame_data['people'][0]['pose_keypoints_2d']\n",
        "\n",
        "            if augment:\n",
        "                keypoints = flip_keypoints_horizontally(keypoints)\n",
        "\n",
        "            if len(keypoints) == len(body_parts) * 1.5:\n",
        "                for i in range(len(body_parts) // 2):\n",
        "                    x, y, confidence = keypoints[i * 3:(i + 1) * 3]\n",
        "\n",
        "                    if confidence > 0:\n",
        "                        matrix[i * 2][frame_number] = x\n",
        "                        matrix[i * 2 + 1][frame_number] = y\n",
        "                    else:\n",
        "                        matrix[i * 2][frame_number] = -1\n",
        "                        matrix[i * 2 + 1][frame_number] = -1\n",
        "            else:\n",
        "                for i in range(len(body_parts) // 2):\n",
        "                    matrix[i * 2][frame_number] = None\n",
        "                    matrix[i * 2 + 1][frame_number] = None\n",
        "        else:\n",
        "            for i in range(len(body_parts) // 2):\n",
        "                matrix[i * 2][frame_number] = None\n",
        "                matrix[i * 2 + 1][frame_number] = None\n",
        "\n",
        "    column_labels = [f'Frame_{os.path.splitext(f)[0]}' for f in selected_files]\n",
        "    df = pd.DataFrame(matrix, index=row_labels, columns=column_labels)\n",
        "    df.fillna(-1, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "#From OpenPose \"Body_25\"\n",
        "body_parts = [\n",
        "    \"x_Nose\", \"y_Nose\", \"x_Neck\", \"y_Neck\", \"x_RShoulder\", \"y_RShoulder\", \"x_RElbow\", \"y_RElbow\", \"x_RWrist\", \"y_RWrist\",\n",
        "    \"x_LShoulder\", \"y_LShoulder\", \"x_LElbow\", \"y_LElbow\", \"x_LWrist\", \"y_LWrist\", \"x_MidHip\", \"y_MidHip\", \"x_RHip\", \"y_RHip\",\n",
        "    \"x_RKnee\", \"y_RKnee\", \"x_RAnkle\", \"y_RAnkle\", \"x_LHip\", \"y_LHip\", \"x_LKnee\", \"y_LKnee\", \"x_LAnkle\", \"y_LAnkle\",\n",
        "    \"x_REye\", \"y_REye\", \"x_LEye\", \"y_LEye\", \"x_REar\", \"y_REar\", \"x_LEar\", \"y_LEar\", \"x_LBigToe\", \"y_LBigToe\",\n",
        "    \"x_LSmallToe\", \"y_LSmallToe\", \"x_LHeel\", \"y_LHeel\", \"x_RBigToe\", \"y_RBigToe\", \"x_RSmallToe\", \"y_RSmallToe\", \"x_RHeel\", \"y_RHeel\"\n",
        "]\n",
        "\n",
        "row_labels = body_parts\n",
        "\n",
        "#Can see uploaded directories in repository - each folder has json files which comprise body part coordinates of each frame of a patient video.\n",
        "directories = [\n",
        "    '/content/drive/My Drive/patient_openpose/00001',\n",
        "    '/content/drive/My Drive/patient_openpose/00002_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00003_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00004',\n",
        "    '/content/drive/My Drive/patient_openpose/00005',\n",
        "    '/content/drive/My Drive/patient_openpose/00006',\n",
        "    '/content/drive/My Drive/patient_openpose/00007',\n",
        "    '/content/drive/My Drive/patient_openpose/00008',\n",
        "    '/content/drive/My Drive/patient_openpose/00009',\n",
        "    '/content/drive/My Drive/patient_openpose/00010_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00011',\n",
        "    '/content/drive/My Drive/patient_openpose/00012',\n",
        "    '/content/drive/My Drive/patient_openpose/00013',\n",
        "    '/content/drive/My Drive/patient_openpose/00014',\n",
        "    '/content/drive/My Drive/patient_openpose/00016',\n",
        "    '/content/drive/My Drive/patient_openpose/00017',\n",
        "    '/content/drive/My Drive/patient_openpose/00018_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00019',\n",
        "    '/content/drive/My Drive/patient_openpose/00020',\n",
        "    '/content/drive/My Drive/patient_openpose/00021',\n",
        "    '/content/drive/My Drive/patient_openpose/00022',\n",
        "    '/content/drive/My Drive/patient_openpose/00024',\n",
        "    '/content/drive/My Drive/patient_openpose/00025',\n",
        "    '/content/drive/My Drive/patient_openpose/00026',\n",
        "    '/content/drive/My Drive/patient_openpose/00027',\n",
        "    '/content/drive/My Drive/patient_openpose/00028_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00029',\n",
        "    '/content/drive/My Drive/patient_openpose/00030',\n",
        "    '/content/drive/My Drive/patient_openpose/00031',\n",
        "    '/content/drive/My Drive/patient_openpose/00032',\n",
        "    '/content/drive/My Drive/patient_openpose/00033',\n",
        "    '/content/drive/My Drive/patient_openpose/00034',\n",
        "    '/content/drive/My Drive/patient_openpose/00035',\n",
        "    '/content/drive/My Drive/patient_openpose/00036',\n",
        "    '/content/drive/My Drive/patient_openpose/00037',\n",
        "    '/content/drive/My Drive/patient_openpose/00038',\n",
        "    '/content/drive/My Drive/patient_openpose/00039',\n",
        "    '/content/drive/My Drive/patient_openpose/00041',\n",
        "    '/content/drive/My Drive/patient_openpose/00042',\n",
        "    '/content/drive/My Drive/patient_openpose/00044',\n",
        "    '/content/drive/My Drive/patient_openpose/00045',\n",
        "    '/content/drive/My Drive/patient_openpose/00046',\n",
        "    '/content/drive/My Drive/patient_openpose/00047',\n",
        "    '/content/drive/My Drive/patient_openpose/00048',\n",
        "    '/content/drive/My Drive/patient_openpose/00049',\n",
        "    '/content/drive/My Drive/patient_openpose/00050',\n",
        "    '/content/drive/My Drive/patient_openpose/00051',\n",
        "    '/content/drive/My Drive/patient_openpose/00052',\n",
        "    '/content/drive/My Drive/patient_openpose/00053',\n",
        "    '/content/drive/My Drive/patient_openpose/00054',\n",
        "]\n",
        "\n",
        "#Forming batches of 75 frames.\n",
        "frame_nos = 75\n",
        "\n",
        "#Dystonia is class 0 and chorea is class 1.\n",
        "target = torch.tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
        "                       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1], dtype=torch.long)\n",
        "\n",
        "original_distribution = Counter(target.tolist())\n",
        "print(\"Original label distribution:\", original_distribution)\n",
        "\n",
        "minority_class_indices = [i for i, label in enumerate(target) if label == 1]\n",
        "majority_class_indices = [i for i, label in enumerate(target) if label == 0]\n",
        "\n",
        "np.random.seed(42)\n",
        "bootstrapped_minority_indices = np.random.choice(minority_class_indices, size=len(majority_class_indices), replace=True)\n",
        "\n",
        "balanced_indices = np.concatenate([majority_class_indices, bootstrapped_minority_indices])\n",
        "\n",
        "directories_balanced = [directories[i] for i in balanced_indices]\n",
        "target_balanced = target[balanced_indices]\n",
        "\n",
        "train_dirs, temp_dirs, train_labels, temp_labels = train_test_split(\n",
        "    directories_balanced, target_balanced, test_size=0.2, random_state=42, stratify=target_balanced\n",
        ")\n",
        "val_dirs, test_dirs, val_labels, test_labels = train_test_split(\n",
        "    temp_dirs, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "train_label_counts = Counter([label.item() for label in train_labels])\n",
        "val_label_counts = Counter([label.item() for label in val_labels])\n",
        "test_label_counts = Counter([label.item() for label in test_labels])\n",
        "\n",
        "print(\"Training label distribution:\", train_label_counts)\n",
        "print(\"Validation label distribution:\", val_label_counts)\n",
        "print(\"Testing label distribution:\", test_label_counts)\n",
        "\n",
        "def process_and_augment_directories_with_labels(directories, labels, frame_rate=25, target_fps=25, batch_size=75):\n",
        "    matrices = []\n",
        "    batch_labels = []\n",
        "    batch_directories = []\n",
        "    skip_every_n, keep_every_m = calculate_skip_and_keep(frame_rate, target_fps)\n",
        "\n",
        "    for directory, label in zip(directories, labels):\n",
        "        try:\n",
        "            batches_data = process_openpose_output(directory, batch_size=batch_size, frame_rate=frame_rate, target_fps=target_fps, skip_every_n=skip_every_n, keep_every_m=keep_every_m)\n",
        "            for batch_data, batch_files, batch_directory in batches_data:\n",
        "                df = create_matrix(batch_data, batch_files)\n",
        "                df_aug = create_matrix(batch_data, batch_files, augment=True)\n",
        "                matrices.append(df)\n",
        "                matrices.append(df_aug)\n",
        "                batch_labels.extend([label, label])\n",
        "                batch_directories.extend([batch_directory, batch_directory])\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping directory {directory} due to insufficient frames: {e}\")\n",
        "            continue\n",
        "    return matrices, batch_labels, batch_directories\n",
        "\n",
        "train_matrices, train_batch_labels, train_batch_dirs = process_and_augment_directories_with_labels(train_dirs, train_labels, frame_rate=frame_rate, target_fps=target_fps, batch_size=batch_size)\n",
        "val_matrices, val_batch_labels, val_batch_dirs = process_and_augment_directories_with_labels(val_dirs, val_labels, frame_rate=frame_rate, target_fps=target_fps, batch_size=batch_size)\n",
        "test_matrices, test_batch_labels, test_batch_dirs = process_and_augment_directories_with_labels(test_dirs, test_labels, frame_rate=frame_rate, target_fps=target_fps, batch_size=batch_size)\n",
        "\n",
        "print(\"Directories for training batches:\", train_batch_dirs)\n",
        "print(\"Directories for validation batches:\", val_batch_dirs)\n",
        "print(\"Directories for test batches:\", test_batch_dirs)\n",
        "\n",
        "train_batch_labels = torch.tensor(train_batch_labels, dtype=torch.long)\n",
        "val_batch_labels = torch.tensor(val_batch_labels, dtype=torch.long)\n",
        "test_batch_labels = torch.tensor(test_batch_labels, dtype=torch.long)\n",
        "train_tensor = np.stack([df.values for df in train_matrices], axis=0)\n",
        "val_tensor = np.stack([df.values for df in val_matrices], axis=0)\n",
        "test_tensor = np.stack([df.values for df in test_matrices], axis=0)\n",
        "\n",
        "print(\"Train tensor shape:\", train_tensor.shape)\n",
        "print(\"Train labels shape:\", train_batch_labels.shape)\n",
        "print(\"Validation tensor shape:\", val_tensor.shape)\n",
        "print(\"Validation labels shape:\", val_batch_labels.shape)\n",
        "print(\"Test tensor shape:\", test_tensor.shape)\n",
        "print(\"Test labels shape:\", test_batch_labels.shape)\n"
      ],
      "metadata": {
        "id": "VkOOiNU7fAJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b349847-2f5b-47f4-dc0f-b622925f501f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original label distribution: Counter({0: 33, 1: 17})\n",
            "Training label distribution: Counter({1: 26, 0: 26})\n",
            "Validation label distribution: Counter({1: 4, 0: 3})\n",
            "Testing label distribution: Counter({0: 4, 1: 3})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-af5d95a34504>:135: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.fillna(-1, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories for training batches: ['/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00010_s', '/content/drive/My Drive/patient_openpose/00010_s', '/content/drive/My Drive/patient_openpose/00010_s', '/content/drive/My Drive/patient_openpose/00010_s', '/content/drive/My Drive/patient_openpose/00054', '/content/drive/My Drive/patient_openpose/00054', '/content/drive/My Drive/patient_openpose/00054', '/content/drive/My Drive/patient_openpose/00054', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00006', '/content/drive/My Drive/patient_openpose/00006', '/content/drive/My Drive/patient_openpose/00006', '/content/drive/My Drive/patient_openpose/00006', '/content/drive/My Drive/patient_openpose/00006', '/content/drive/My Drive/patient_openpose/00006', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00036', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00041', '/content/drive/My Drive/patient_openpose/00041', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00025', '/content/drive/My Drive/patient_openpose/00041', '/content/drive/My Drive/patient_openpose/00041', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00012', '/content/drive/My Drive/patient_openpose/00028_s', '/content/drive/My Drive/patient_openpose/00028_s', '/content/drive/My Drive/patient_openpose/00028_s', '/content/drive/My Drive/patient_openpose/00028_s', '/content/drive/My Drive/patient_openpose/00028_s', '/content/drive/My Drive/patient_openpose/00028_s', '/content/drive/My Drive/patient_openpose/00042', '/content/drive/My Drive/patient_openpose/00042', '/content/drive/My Drive/patient_openpose/00042', '/content/drive/My Drive/patient_openpose/00042', '/content/drive/My Drive/patient_openpose/00042', '/content/drive/My Drive/patient_openpose/00042', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00049', '/content/drive/My Drive/patient_openpose/00004', '/content/drive/My Drive/patient_openpose/00004', '/content/drive/My Drive/patient_openpose/00004', '/content/drive/My Drive/patient_openpose/00004', '/content/drive/My Drive/patient_openpose/00032', '/content/drive/My Drive/patient_openpose/00032', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00020', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00031', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00046', '/content/drive/My Drive/patient_openpose/00046', '/content/drive/My Drive/patient_openpose/00046', '/content/drive/My Drive/patient_openpose/00046', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00007', '/content/drive/My Drive/patient_openpose/00033', '/content/drive/My Drive/patient_openpose/00033', '/content/drive/My Drive/patient_openpose/00033', '/content/drive/My Drive/patient_openpose/00033', '/content/drive/My Drive/patient_openpose/00033', '/content/drive/My Drive/patient_openpose/00033', '/content/drive/My Drive/patient_openpose/00037', '/content/drive/My Drive/patient_openpose/00037', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00027', '/content/drive/My Drive/patient_openpose/00013', '/content/drive/My Drive/patient_openpose/00013', '/content/drive/My Drive/patient_openpose/00048', '/content/drive/My Drive/patient_openpose/00048', '/content/drive/My Drive/patient_openpose/00053', '/content/drive/My Drive/patient_openpose/00053', '/content/drive/My Drive/patient_openpose/00053', '/content/drive/My Drive/patient_openpose/00053', '/content/drive/My Drive/patient_openpose/00053', '/content/drive/My Drive/patient_openpose/00053', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00030', '/content/drive/My Drive/patient_openpose/00019', '/content/drive/My Drive/patient_openpose/00019', '/content/drive/My Drive/patient_openpose/00019', '/content/drive/My Drive/patient_openpose/00019', '/content/drive/My Drive/patient_openpose/00019', '/content/drive/My Drive/patient_openpose/00019', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00001', '/content/drive/My Drive/patient_openpose/00001', '/content/drive/My Drive/patient_openpose/00001', '/content/drive/My Drive/patient_openpose/00001', '/content/drive/My Drive/patient_openpose/00001', '/content/drive/My Drive/patient_openpose/00001', '/content/drive/My Drive/patient_openpose/00018_s', '/content/drive/My Drive/patient_openpose/00018_s', '/content/drive/My Drive/patient_openpose/00018_s', '/content/drive/My Drive/patient_openpose/00018_s', '/content/drive/My Drive/patient_openpose/00035', '/content/drive/My Drive/patient_openpose/00035', '/content/drive/My Drive/patient_openpose/00035', '/content/drive/My Drive/patient_openpose/00035', '/content/drive/My Drive/patient_openpose/00035', '/content/drive/My Drive/patient_openpose/00035', '/content/drive/My Drive/patient_openpose/00003_s', '/content/drive/My Drive/patient_openpose/00003_s', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00039', '/content/drive/My Drive/patient_openpose/00017', '/content/drive/My Drive/patient_openpose/00017', '/content/drive/My Drive/patient_openpose/00017', '/content/drive/My Drive/patient_openpose/00017', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00014', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00024', '/content/drive/My Drive/patient_openpose/00026', '/content/drive/My Drive/patient_openpose/00026', '/content/drive/My Drive/patient_openpose/00026', '/content/drive/My Drive/patient_openpose/00026', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00011', '/content/drive/My Drive/patient_openpose/00013', '/content/drive/My Drive/patient_openpose/00013', '/content/drive/My Drive/patient_openpose/00002_s', '/content/drive/My Drive/patient_openpose/00002_s', '/content/drive/My Drive/patient_openpose/00021', '/content/drive/My Drive/patient_openpose/00021', '/content/drive/My Drive/patient_openpose/00021', '/content/drive/My Drive/patient_openpose/00021', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045', '/content/drive/My Drive/patient_openpose/00045']\n",
            "Directories for validation batches: ['/content/drive/My Drive/patient_openpose/00044', '/content/drive/My Drive/patient_openpose/00044', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00051', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00022', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00016', '/content/drive/My Drive/patient_openpose/00052', '/content/drive/My Drive/patient_openpose/00052', '/content/drive/My Drive/patient_openpose/00050', '/content/drive/My Drive/patient_openpose/00050', '/content/drive/My Drive/patient_openpose/00046', '/content/drive/My Drive/patient_openpose/00046']\n",
            "Directories for test batches: ['/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00034', '/content/drive/My Drive/patient_openpose/00038', '/content/drive/My Drive/patient_openpose/00038', '/content/drive/My Drive/patient_openpose/00038', '/content/drive/My Drive/patient_openpose/00038', '/content/drive/My Drive/patient_openpose/00038', '/content/drive/My Drive/patient_openpose/00038', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00008', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00005', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00029', '/content/drive/My Drive/patient_openpose/00046', '/content/drive/My Drive/patient_openpose/00046', '/content/drive/My Drive/patient_openpose/00009', '/content/drive/My Drive/patient_openpose/00009', '/content/drive/My Drive/patient_openpose/00009', '/content/drive/My Drive/patient_openpose/00009', '/content/drive/My Drive/patient_openpose/00009', '/content/drive/My Drive/patient_openpose/00009']\n",
            "Train tensor shape: (318, 50, 75)\n",
            "Train labels shape: torch.Size([318])\n",
            "Validation tensor shape: (32, 50, 75)\n",
            "Validation labels shape: torch.Size([32])\n",
            "Test tensor shape: (88, 50, 75)\n",
            "Test labels shape: torch.Size([88])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Concatenating according to body part arrangements - can look at \"Body_25\" for reference\n",
        "left_arm_train_np = np.concatenate((train_tensor[:, 2:4, :], train_tensor[:, 10:16, :]), axis=1)\n",
        "print(left_arm_train_np.shape)\n",
        "right_arm_train_np = np.concatenate((train_tensor[:, 2:4, :], train_tensor[:, 4:10, :]), axis=1)\n",
        "print(right_arm_train_np.shape)\n",
        "left_leg_train_np = np.concatenate((train_tensor[:, 16:18, :], train_tensor[:, 24:30, :], train_tensor[:, 38:44, :]), axis=1)\n",
        "print(left_leg_train_np.shape)\n",
        "right_leg_train_np = np.concatenate((train_tensor[:, 16:24, :], train_tensor[:, 44:50, :]), axis=1)\n",
        "print(right_leg_train_np.shape)\n",
        "torso_train_np = np.concatenate((train_tensor[:, 0:4, :], train_tensor[:, 30:38, :]), axis=1)\n",
        "print(torso_train_np.shape)\n",
        "\n",
        "left_arm_test_np = np.concatenate((test_tensor[:, 2:4, :], test_tensor[:, 10:16, :]), axis=1)\n",
        "print(left_arm_test_np.shape)\n",
        "right_arm_test_np = np.concatenate((test_tensor[:, 2:4, :], test_tensor[:, 4:10, :]), axis=1)\n",
        "print(right_arm_test_np.shape)\n",
        "left_leg_test_np = np.concatenate((test_tensor[:, 16:18, :], test_tensor[:, 24:30, :], test_tensor[:, 38:44, :]), axis=1)\n",
        "print(left_leg_test_np.shape)\n",
        "right_leg_test_np = np.concatenate((test_tensor[:, 16:24, :], test_tensor[:, 44:50, :]), axis=1)\n",
        "print(right_leg_test_np.shape)\n",
        "torso_test_np = np.concatenate((test_tensor[:, 0:4, :], test_tensor[:, 30:38, :]), axis=1)\n",
        "print(torso_test_np.shape)\n",
        "\n",
        "left_arm_val_np = np.concatenate((val_tensor[:, 2:4, :], val_tensor[:, 10:16, :]), axis=1)\n",
        "print(left_arm_val_np.shape)\n",
        "right_arm_val_np = np.concatenate((val_tensor[:, 2:4, :], val_tensor[:, 4:10, :]), axis=1)\n",
        "print(right_arm_val_np.shape)\n",
        "left_leg_val_np = np.concatenate((val_tensor[:, 16:18, :], val_tensor[:, 24:30, :], val_tensor[:, 38:44, :]), axis=1)\n",
        "print(left_leg_val_np.shape)\n",
        "right_leg_val_np = np.concatenate((val_tensor[:, 16:24, :], val_tensor[:, 44:50, :]), axis=1)\n",
        "print(right_leg_val_np.shape)\n",
        "torso_val_np = np.concatenate((val_tensor[:, 0:4, :], val_tensor[:, 30:38, :]), axis=1)\n",
        "print(torso_val_np.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV9hns7Oj8lu",
        "outputId": "839d66aa-4514-4d6d-deb3-4f9a7f04227a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(318, 8, 75)\n",
            "(318, 8, 75)\n",
            "(318, 14, 75)\n",
            "(318, 14, 75)\n",
            "(318, 12, 75)\n",
            "(88, 8, 75)\n",
            "(88, 8, 75)\n",
            "(88, 14, 75)\n",
            "(88, 14, 75)\n",
            "(88, 12, 75)\n",
            "(32, 8, 75)\n",
            "(32, 8, 75)\n",
            "(32, 14, 75)\n",
            "(32, 14, 75)\n",
            "(32, 12, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "left_arm_train_tensor = torch.from_numpy(left_arm_train_np).float()\n",
        "right_arm_train_tensor = torch.from_numpy(right_arm_train_np).float()\n",
        "left_leg_train_tensor = torch.from_numpy(left_leg_train_np).float()\n",
        "right_leg_train_tensor = torch.from_numpy(right_leg_train_np).float()\n",
        "torso_train_tensor = torch.from_numpy(torso_train_np).float()\n",
        "\n",
        "left_arm_test_tensor = torch.from_numpy(left_arm_test_np).float()\n",
        "right_arm_test_tensor = torch.from_numpy(right_arm_test_np).float()\n",
        "left_leg_test_tensor = torch.from_numpy(left_leg_test_np).float()\n",
        "right_leg_test_tensor = torch.from_numpy(right_leg_test_np).float()\n",
        "torso_test_tensor = torch.from_numpy(torso_test_np).float()\n",
        "\n",
        "left_arm_val_tensor = torch.from_numpy(left_arm_val_np).float()\n",
        "right_arm_val_tensor = torch.from_numpy(right_arm_val_np).float()\n",
        "left_leg_val_tensor = torch.from_numpy(left_leg_val_np).float()\n",
        "right_leg_val_tensor = torch.from_numpy(right_leg_val_np).float()\n",
        "torso_val_tensor = torch.from_numpy(torso_val_np).float()"
      ],
      "metadata": {
        "id": "3cqXIeCxFf3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spatiotemporal Co-Attention Recurrent Neural Networks for Human-Skeleton Motion Prediction explains travelling and surrounding sequences for improved information for RNN training.\n",
        "rnn_sequence_train = np.concatenate((train_tensor[:, 2:4, :], right_leg_train_np, train_tensor[:, 20:22, :], train_tensor[:, 18:20, :], left_leg_train_np, train_tensor[:, 26:28, :], train_tensor[:, 24:26, :], train_tensor[:, 16:18, :], torso_train_np, right_arm_train_np, train_tensor[:, 6:8, :], train_tensor[:, 4:6, :], left_arm_train_np, train_tensor[:, 12:14, :], train_tensor[:, 10:12, :], train_tensor[:, 2:4, :]), axis=1)\n",
        "rnn_sequence_train = np.array(rnn_sequence_train, dtype=float)\n",
        "rnn_sequence_train_tensor = torch.from_numpy(rnn_sequence_train).float()\n",
        "reshaped_rnn_train_tensor = rnn_sequence_train_tensor.permute(0, 2, 1)\n",
        "print(reshaped_rnn_train_tensor.shape)\n",
        "\n",
        "rnn_sequence_test = np.concatenate((test_tensor[:, 2:4, :], right_leg_test_np, test_tensor[:, 20:22, :], test_tensor[:, 18:20, :], left_leg_test_np, test_tensor[:, 26:28, :], test_tensor[:, 24:26, :], test_tensor[:, 16:18, :], torso_test_np, right_arm_test_np, test_tensor[:, 6:8, :], test_tensor[:, 4:6, :], left_arm_test_np, test_tensor[:, 12:14, :], test_tensor[:, 10:12, :], test_tensor[:, 2:4, :]), axis=1)\n",
        "rnn_sequence_test = np.array(rnn_sequence_test, dtype=float)\n",
        "rnn_sequence_test_tensor = torch.from_numpy(rnn_sequence_test).float()\n",
        "reshaped_rnn_test_tensor = rnn_sequence_test_tensor.permute(0, 2, 1)\n",
        "print(reshaped_rnn_test_tensor.shape)\n",
        "\n",
        "rnn_sequence_val = np.concatenate((val_tensor[:, 2:4, :], right_leg_val_np, val_tensor[:, 20:22, :], val_tensor[:, 18:20, :], left_leg_val_np, val_tensor[:, 26:28, :], val_tensor[:, 24:26, :], val_tensor[:, 16:18, :], torso_val_np, right_arm_val_np, val_tensor[:, 6:8, :], val_tensor[:, 4:6, :], left_arm_val_np, val_tensor[:, 12:14, :], val_tensor[:, 10:12, :], val_tensor[:, 2:4, :]), axis=1)\n",
        "rnn_sequence_val = np.array(rnn_sequence_val, dtype=float)\n",
        "rnn_sequence_val_tensor = torch.from_numpy(rnn_sequence_val).float()\n",
        "reshaped_rnn_val_tensor = rnn_sequence_val_tensor.permute(0, 2, 1)\n",
        "print(reshaped_rnn_val_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmqT7tjEv4Mt",
        "outputId": "9d1a8797-df79-4cbb-a23e-517a6e6773cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([318, 75, 78])\n",
            "torch.Size([88, 75, 78])\n",
            "torch.Size([32, 75, 78])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, num_parts, input_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_parts = num_parts\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape [batch_size, num_parts, input_dim] (e.g., [318, 5, 128])\n",
        "        queries = self.query(x)\n",
        "        keys = self.key(x)\n",
        "        values = self.value(x)\n",
        "\n",
        "        # Compute attention scores across the body parts\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.num_parts ** 0.5)\n",
        "        attention_weights = self.softmax(scores)\n",
        "\n",
        "        # Compute weighted sum of values based on attention weights\n",
        "        weighted_values = torch.bmm(attention_weights, values)\n",
        "\n",
        "        return weighted_values, scores, attention_weights\n"
      ],
      "metadata": {
        "id": "zI6jPNOHkEkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes = 5\n",
        "edges = []\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i != j:\n",
        "            edges.append([i, j])\n",
        "\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "print(edge_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gUCGWbPkHbH",
        "outputId": "82bbf0e3-e894-454a-d47e-35fd27948ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
            "        [1, 2, 3, 4, 0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4, 0, 1, 2, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, leaky_relu_alpha):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.bn = nn.BatchNorm1d(output_dim)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_alpha)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        if input_dim != output_dim:\n",
        "            self.residual_connection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.residual_connection = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.fc(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        if self.residual_connection is not None:\n",
        "            residual = self.residual_connection(residual)\n",
        "\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "class SpatialNetwork(nn.Module):\n",
        "    def __init__(self, n_part, n_graph_out, frame_nos, leaky_relu_alpha):\n",
        "        super(SpatialNetwork, self).__init__()\n",
        "\n",
        "        self.n_part = n_part\n",
        "        self.n_graph_out = n_graph_out\n",
        "        self.frame_nos = frame_nos\n",
        "\n",
        "        self.body_parts_fc = nn.ModuleDict({\n",
        "            'arms': nn.Linear(8, n_part),\n",
        "            'legs': nn.Linear(14, n_part),\n",
        "            'torso': nn.Linear(12, n_part),\n",
        "        })\n",
        "\n",
        "        self.self_attn = SelfAttention(num_parts=5, input_dim=n_part)\n",
        "        self.gcn1 = GCNConv(in_channels=n_part, out_channels=n_graph_out, aggr='max', bias=True)\n",
        "        self.bn_gcn1 = nn.BatchNorm1d(n_graph_out)\n",
        "        self.fc6 = ResidualBlock(n_graph_out * frame_nos, 512, leaky_relu_alpha)\n",
        "        self.fc7 = ResidualBlock(512, 256, leaky_relu_alpha)\n",
        "\n",
        "        self.training_attention_scores = []\n",
        "        self.training_attention_weights = []\n",
        "\n",
        "    def reset_attention_scores(self):\n",
        "        self.training_attention_scores = []\n",
        "        self.training_attention_weights = []\n",
        "\n",
        "    def forward_body_part(self, x, part):\n",
        "        x = self.body_parts_fc[part](x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x\n",
        "\n",
        "    def forward(self, left_arm, right_arm, left_leg, right_leg, torso, edge_index, is_training=True):\n",
        "        batch_size, features, seq_len = left_arm.shape\n",
        "\n",
        "        outputs = torch.zeros((batch_size, seq_len, self.n_graph_out), device=left_arm.device)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            left_arm_out = self.forward_body_part(left_arm[:, :, t], 'arms')\n",
        "            right_arm_out = self.forward_body_part(right_arm[:, :, t], 'arms')\n",
        "            left_leg_out = self.forward_body_part(left_leg[:, :, t], 'legs')\n",
        "            right_leg_out = self.forward_body_part(right_leg[:, :, t], 'legs')\n",
        "            torso_out = self.forward_body_part(torso[:, :, t], 'torso')\n",
        "\n",
        "            concatenated = torch.stack((left_arm_out, right_arm_out, left_leg_out, right_leg_out, torso_out), dim=1)\n",
        "\n",
        "            attn_output, attn_scores, attn_weights = self.self_attn(concatenated)\n",
        "\n",
        "            if is_training:\n",
        "                self.training_attention_scores.append(attn_scores)\n",
        "                self.training_attention_weights.append(attn_weights)\n",
        "\n",
        "            batch_size, num_nodes, feature_dim = attn_output.shape\n",
        "            attn_output = attn_output.reshape(batch_size * num_nodes, feature_dim)\n",
        "\n",
        "            gcn_output = self.gcn1(attn_output, edge_index)\n",
        "            gcn_output = self.bn_gcn1(gcn_output)\n",
        "            gcn_output = F.leaky_relu(gcn_output)\n",
        "            gcn_output = F.dropout(gcn_output, p=0.5, training=self.training)\n",
        "            gcn_output = gcn_output.view(batch_size, num_nodes, -1)\n",
        "            pooled_output = torch.max(gcn_output, dim=1)[0]\n",
        "\n",
        "            outputs[:, t, :] = pooled_output\n",
        "\n",
        "        flattened_outputs = outputs.view(batch_size, -1)\n",
        "        fc6_output = self.fc6(flattened_outputs)\n",
        "        fc7_output = self.fc7(fc6_output)\n",
        "\n",
        "        return fc7_output\n",
        "\n",
        "    def get_last_epoch_attention_scores(self):\n",
        "        attention_scores = torch.cat(self.training_attention_scores, dim=0)\n",
        "\n",
        "    def get_last_epoch_attention_weights(self):\n",
        "        attention_weights = torch.cat(self.training_attention_weights, dim=0)\n"
      ],
      "metadata": {
        "id": "cz5qrR_SkKBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_prob, leaky_relu_alpha):\n",
        "        super(TemporalNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout_prob)\n",
        "\n",
        "        self.self_attn = SelfAttention(num_parts=75, input_dim=hidden_size * 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, output_size * input_size)\n",
        "        self.bn1 = nn.BatchNorm1d(output_size * input_size)\n",
        "        self.fc2 = nn.Linear(output_size * input_size, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = ResidualBlock(512, 256, leaky_relu_alpha)\n",
        "\n",
        "        self.stored_attention_scores = []\n",
        "        self.stored_attention_weights = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        out, scores, weights = self.self_attn(out)\n",
        "\n",
        "        self.stored_attention_scores.append(scores)\n",
        "        self.stored_attention_weights.append(weights)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc1(out[:, -1, :])  # Use the last time step's output\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = self.bn1(out)\n",
        "        out = F.leaky_relu(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def clear_attention_weights(self):\n",
        "        self.stored_attention_scores = []\n",
        "        self.stored_attention_weights = []\n",
        "\n",
        "    def get_attention_scores(self):\n",
        "        return self.stored_attention_scores\n",
        "\n",
        "    def get_attention_weights(self):\n",
        "        return self.stored_attention_weights\n"
      ],
      "metadata": {
        "id": "6RLKDlyTwEDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads=4):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert input_dim % num_heads == 0, \"input_dim must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = input_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv_proj = nn.Linear(input_dim, input_dim * 3)\n",
        "        self.fc_out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, input_dim = x.shape\n",
        "\n",
        "        qkv = self.qkv_proj(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        attn_out = (attn_probs @ v).permute(0, 2, 1, 3).reshape(batch_size, seq_length, input_dim)\n",
        "        return self.fc_out(attn_out)\n",
        "\n",
        "class FinalConcatenation(nn.Module):\n",
        "    def __init__(self, input_dim=512, output_dim=2, dropout_prob=0.5, num_heads=4):\n",
        "        super(FinalConcatenation, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(input_dim=input_dim, num_heads=num_heads)\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(256, output_dim)\n",
        "\n",
        "        self.layer_norm = LayerNorm(input_dim)\n",
        "        self.residual_fc = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # [batch_size, seq_length, input_dim] -> [batch_size, 1, input_dim]\n",
        "\n",
        "        res = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.attention(x)\n",
        "        x += self.residual_fc(res)\n",
        "\n",
        "        x = x.squeeze(1)  # [batch_size, input_dim]\n",
        "\n",
        "        x = self.fc1(x)  # [batch_size, 256]\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)  # [batch_size, output_dim]\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "f-7nNZwGxbjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            if param.dim() >= 2:\n",
        "                nn.init.kaiming_normal_(param.data, nonlinearity='leaky_relu')\n",
        "            else:\n",
        "                param.data.fill_(1)\n",
        "        elif 'bias' in name:\n",
        "            param.data.fill_(0)"
      ],
      "metadata": {
        "id": "-KH5es_LwLQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "#got this from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py"
      ],
      "metadata": {
        "id": "0bATk0zxokFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_initialize_weights(model, gcn_constant=0.0001):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, GCNConv):\n",
        "            init.constant_(module.lin.weight, gcn_constant)\n",
        "            if module.lin.bias is not None:\n",
        "                init.constant_(module.lin.bias, 0)\n",
        "        elif isinstance(module, SelfAttention):\n",
        "            init.kaiming_normal_(module.query.weight, nonlinearity='leaky_relu')\n",
        "            init.kaiming_normal_(module.key.weight, nonlinearity='leaky_relu')\n",
        "            init.kaiming_normal_(module.value.weight, nonlinearity='leaky_relu')\n",
        "            if module.query.bias is not None:\n",
        "                init.constant_(module.query.bias, 0)\n",
        "            if module.key.bias is not None:\n",
        "                init.constant_(module.key.bias, 0)\n",
        "            if module.value.bias is not None:\n",
        "                init.constant_(module.value.bias, 0)\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            init.kaiming_normal_(module.weight, nonlinearity='leaky_relu')\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.BatchNorm1d):\n",
        "            init.constant_(module.weight, 1)\n",
        "            init.constant_(module.bias, 0)"
      ],
      "metadata": {
        "id": "_lK3KYqa8cby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leaky_relu_alpha = 0.01\n",
        "input_size = 78\n",
        "hidden_size = n_rnn\n",
        "output_size = n_rnn_out\n",
        "num_layers = 4\n",
        "dropout_prob = 0.5"
      ],
      "metadata": {
        "id": "FBbhshTA7Pok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.1\n",
        "gamma=0\n",
        "learning_rate=0.0001\n",
        "weight_decay=0.001"
      ],
      "metadata": {
        "id": "VIkVO3HX49N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=2, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        log_probs = nn.functional.log_softmax(inputs, dim=1)\n",
        "        probs = torch.exp(log_probs)\n",
        "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=log_probs.size(1)).float()\n",
        "\n",
        "        focal_weight = torch.pow(1 - probs, self.gamma)\n",
        "        focal_loss = -self.alpha * focal_weight * targets_one_hot * log_probs\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            focal_loss = focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            focal_loss = focal_loss.sum()\n",
        "\n",
        "        return focal_loss\n"
      ],
      "metadata": {
        "id": "-bPrgM9cXA3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_val_significance_test(dataset, labels, n_splits=5, batch_size=32, baseline=None, **train_params):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    performance_metrics = []\n",
        "\n",
        "    for train_index, val_index in skf.split(labels, labels):\n",
        "        train_loader = create_data_loader(dataset, train_index, batch_size)\n",
        "        val_loader = create_data_loader(dataset, val_index, batch_size)\n",
        "\n",
        "        # Train and evaluate the model on this fold\n",
        "        _, accuracy, _, _ = train_model(**train_params, train_loader=train_loader, val_loader=val_loader)\n",
        "        performance_metrics.append(accuracy)\n",
        "\n",
        "    performance_metrics = np.array(performance_metrics)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    mean_accuracy = np.mean(performance_metrics)\n",
        "    ci_lower, ci_upper = bootstrap_confidence_interval(performance_metrics)\n",
        "\n",
        "    if baseline is not None:\n",
        "        t_stat, p_value = ttest_1samp(performance_metrics, baseline)\n",
        "        print(f\"Mean Performance: {mean_accuracy}, 95% CI: ({ci_lower}, {ci_upper}), Baseline: {baseline}, p-value: {p_value}\")\n",
        "        return mean_accuracy, ci_lower, ci_upper, p_value\n",
        "    else:\n",
        "        print(f\"Mean Performance: {mean_accuracy}, 95% CI: ({ci_lower}, {ci_upper})\")\n",
        "        return mean_accuracy, ci_lower, ci_upper\n"
      ],
      "metadata": {
        "id": "sucwF63kkmCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import ttest_1samp\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "\n",
        "class_names = ['Class 0', 'Class 1']\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def bootstrap_confidence_interval(data, num_bootstrap_samples=1000, confidence_level=0.95):\n",
        "    n = len(data)\n",
        "    bootstrap_samples = np.random.choice(data, (num_bootstrap_samples, n), replace=True)\n",
        "    bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
        "    lower_bound = np.percentile(bootstrap_means, (1-confidence_level)/2 * 100)\n",
        "    upper_bound = np.percentile(bootstrap_means, (1 + confidence_level)/2 * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "def create_data_loader(dataset, indices, batch_size):\n",
        "    subset = Subset(dataset, indices)\n",
        "    return DataLoader(subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "def calculate_metrics(all_labels, all_preds):\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "    recall = sensitivity  # recall is the same as sensitivity\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    return accuracy, sensitivity, specificity, f1\n",
        "\n",
        "def train_model(alpha, gamma, learning_rate, weight_decay, train_loader, val_loader, edge_index, class_names,\n",
        "                spatial_net, temporal_net, final_model):\n",
        "    def set_seeds(seed=42):\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    spatial_net = SpatialNetwork(n_part, n_graph_out, frame_nos, leaky_relu_alpha)\n",
        "    custom_initialize_weights(spatial_net)\n",
        "\n",
        "    temporal_net = TemporalNetwork(input_size, hidden_size, output_size, num_layers, dropout_prob, leaky_relu_alpha)\n",
        "    initialize_weights(temporal_net)\n",
        "\n",
        "    final_model = FinalConcatenation()\n",
        "    initialize_weights(final_model)\n",
        "\n",
        "    optimizer = optim.Adam(list(spatial_net.parameters()) + list(temporal_net.parameters()) + list(final_model.parameters()),\n",
        "                           lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.001, step_size_up=2000, mode='triangular2')\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "    num_epochs = 47\n",
        "    criterion = FocalLoss(alpha=alpha, gamma=gamma)\n",
        "\n",
        "    train_metrics = {'accuracy': [], 'sensitivity': [], 'specificity': [], 'f1': []}\n",
        "    val_metrics = {'accuracy': [], 'sensitivity': [], 'specificity': [], 'f1': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        temporal_net.clear_attention_weights()\n",
        "\n",
        "        spatial_net.train()\n",
        "        temporal_net.train()\n",
        "        final_model.train()\n",
        "\n",
        "        all_train_preds = []\n",
        "        all_train_labels = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            left_arm_train_tensor, right_arm_train_tensor, left_leg_train_tensor, right_leg_train_tensor, torso_train_tensor, reshaped_rnn_train_tensor, train_batch_labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output_spatial_train = spatial_net(left_arm_train_tensor, right_arm_train_tensor, left_leg_train_tensor,\n",
        "                                               right_leg_train_tensor, torso_train_tensor, edge_index)\n",
        "            output_temporal_train = temporal_net(reshaped_rnn_train_tensor)\n",
        "            concatenated_input_train = torch.cat((output_spatial_train, output_temporal_train), dim=1)\n",
        "            final_output_train = final_model(concatenated_input_train)\n",
        "\n",
        "            loss = criterion(final_output_train, train_batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            _, train_preds = torch.max(final_output_train, 1)\n",
        "            all_train_preds.extend(train_preds.cpu().numpy())\n",
        "            all_train_labels.extend(train_batch_labels.cpu().numpy())\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(spatial_net.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(temporal_net.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
        "\n",
        "        train_accuracy, train_sensitivity, train_specificity, train_f1 = calculate_metrics(all_train_labels, all_train_preds)\n",
        "        train_metrics['accuracy'].append(train_accuracy)\n",
        "        train_metrics['sensitivity'].append(train_sensitivity)\n",
        "        train_metrics['specificity'].append(train_specificity)\n",
        "        train_metrics['f1'].append(train_f1)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}, Training Accuracy: {train_accuracy:.4f}, Training Sensitivity: {train_sensitivity:.4f}, Training Specificity: {train_specificity:.4f}, Training F1: {train_f1:.4f}\")\n",
        "\n",
        "        spatial_net.eval()\n",
        "        temporal_net.eval()\n",
        "        final_model.eval()\n",
        "\n",
        "        all_val_preds = []\n",
        "        all_val_labels = []\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                left_arm_val_tensor, right_arm_val_tensor, left_leg_val_tensor, right_leg_val_tensor, torso_val_tensor, reshaped_rnn_val_tensor, val_batch_labels = batch\n",
        "\n",
        "                output_spatial_val = spatial_net(left_arm_val_tensor, right_arm_val_tensor, left_leg_val_tensor,\n",
        "                                                 right_leg_val_tensor, torso_val_tensor, edge_index)\n",
        "                output_temporal_val = temporal_net(reshaped_rnn_val_tensor)\n",
        "                concatenated_input_val = torch.cat((output_spatial_val, output_temporal_val), dim=1)\n",
        "                final_output_val = final_model(concatenated_input_val)\n",
        "\n",
        "                val_loss += criterion(final_output_val, val_batch_labels).item()\n",
        "\n",
        "                _, val_preds = torch.max(final_output_val, 1)\n",
        "                all_val_preds.extend(val_preds.cpu().numpy())\n",
        "                all_val_labels.extend(val_batch_labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_accuracy, val_sensitivity, val_specificity, val_f1 = calculate_metrics(all_val_labels, all_val_preds)\n",
        "        val_metrics['accuracy'].append(val_accuracy)\n",
        "        val_metrics['sensitivity'].append(val_sensitivity)\n",
        "        val_metrics['specificity'].append(val_specificity)\n",
        "        val_metrics['f1'].append(val_f1)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Sensitivity: {val_sensitivity:.4f}, Validation Specificity: {val_specificity:.4f}, Validation F1: {val_f1:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "        early_stopping(val_loss, final_model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # Return all five values: validation loss, accuracy, sensitivity, specificity, and F1\n",
        "    final_val_accuracy = val_metrics['accuracy'][-1]\n",
        "    final_val_sensitivity = val_metrics['sensitivity'][-1]\n",
        "    final_val_specificity = val_metrics['specificity'][-1]\n",
        "    final_val_f1 = val_metrics['f1'][-1]\n",
        "\n",
        "    return val_loss, final_val_accuracy, final_val_sensitivity, final_val_specificity, final_val_f1\n",
        "\n",
        "def cross_val_significance_test(dataset, labels, n_splits, batch_size, alpha, gamma, learning_rate, weight_decay,\n",
        "                                edge_index, class_names, spatial_net, temporal_net, final_model, baseline=None):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    accuracies, sensitivities, specificities, f1_scores = [], [], [], []\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "\n",
        "        train_loader = create_data_loader(dataset, train_index, batch_size)\n",
        "        test_loader = create_data_loader(dataset, test_index, batch_size)\n",
        "\n",
        "        val_loss, accuracy, sensitivity, specificity, f1 = train_model(\n",
        "            alpha=alpha,\n",
        "            gamma=gamma,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=test_loader,\n",
        "            edge_index=edge_index,\n",
        "            class_names=class_names,\n",
        "            spatial_net=spatial_net,\n",
        "            temporal_net=temporal_net,\n",
        "            final_model=final_model\n",
        "        )\n",
        "        accuracies.append(accuracy)\n",
        "        sensitivities.append(sensitivity)\n",
        "        specificities.append(specificity)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold {fold + 1}: Accuracy: {accuracy:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # Calculate mean values for each metric directly from stored values\n",
        "    mean_accuracy = np.mean(accuracies)\n",
        "    mean_sensitivity = np.mean(sensitivities)\n",
        "    mean_specificity = np.mean(specificities)\n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "\n",
        "    # Calculate confidence intervals for each metric\n",
        "    ci_accuracy = bootstrap_confidence_interval(accuracies)\n",
        "    ci_sensitivity = bootstrap_confidence_interval(sensitivities)\n",
        "    ci_specificity = bootstrap_confidence_interval(specificities)\n",
        "    ci_f1 = bootstrap_confidence_interval(f1_scores)\n",
        "\n",
        "    # Calculate p-values if baseline is provided\n",
        "    if baseline is not None:\n",
        "        p_accuracy = ttest_1samp(accuracies, baseline).pvalue\n",
        "        p_sensitivity = ttest_1samp(sensitivities, baseline).pvalue\n",
        "        p_specificity = ttest_1samp(specificities, baseline).pvalue\n",
        "        p_f1 = ttest_1samp(f1_scores, baseline).pvalue\n",
        "    else:\n",
        "        p_accuracy = p_sensitivity = p_specificity = p_f1 = None\n",
        "\n",
        "    # Print results with the specified format\n",
        "    print(f\"Mean Accuracy: {mean_accuracy:.4f}, CI: {ci_accuracy}, p-value: {p_accuracy:.4f}\" if p_accuracy is not None else f\"Mean Accuracy: {mean_accuracy:.4f}, CI: {ci_accuracy}, p-value: None\")\n",
        "    print(f\"Mean Sensitivity: {mean_sensitivity:.4f}, CI: {ci_sensitivity}, p-value: {p_sensitivity:.4f}\" if p_sensitivity is not None else f\"Mean Sensitivity: {mean_sensitivity:.4f}, CI: {ci_sensitivity}, p-value: None\")\n",
        "    print(f\"Mean Specificity: {mean_specificity:.4f}, CI: {ci_specificity}, p-value: {p_specificity:.4f}\" if p_specificity is not None else f\"Mean Specificity: {mean_specificity:.4f}, CI: {ci_specificity}, p-value: None\")\n",
        "    print(f\"Mean F1 Score: {mean_f1:.4f}, CI: {ci_f1}, p-value: {p_f1:.4f}\" if p_f1 is not None else f\"Mean F1 Score: {mean_f1:.4f}, CI: {ci_f1}, p-value: None\")\n",
        "\n",
        "    return (mean_accuracy, ci_accuracy, p_accuracy,\n",
        "            mean_sensitivity, ci_sensitivity, p_sensitivity,\n",
        "            mean_specificity, ci_specificity, p_specificity,\n",
        "            mean_f1, ci_f1, p_f1)\n",
        "\n",
        "# Train the models globally\n",
        "spatial_net = SpatialNetwork(n_part, n_graph_out, frame_nos, leaky_relu_alpha)\n",
        "temporal_net = TemporalNetwork(input_size, hidden_size, output_size, num_layers, dropout_prob, leaky_relu_alpha)\n",
        "final_model = FinalConcatenation()\n",
        "\n",
        "# Initialize the models\n",
        "custom_initialize_weights(spatial_net)\n",
        "initialize_weights(temporal_net)\n",
        "initialize_weights(final_model)\n",
        "\n",
        "# Example usage\n",
        "dataset = TensorDataset(left_arm_train_tensor, right_arm_train_tensor, left_leg_train_tensor,\n",
        "                        right_leg_train_tensor, torso_train_tensor, reshaped_rnn_train_tensor, train_batch_labels)\n",
        "\n",
        "mean_accuracy, ci_accuracy, p_accuracy, mean_sensitivity, ci_sensitivity, p_sensitivity, mean_specificity, ci_specificity, p_specificity, mean_f1, ci_f1, p_f1 = cross_val_significance_test(\n",
        "    dataset=dataset,\n",
        "    labels=train_batch_labels,\n",
        "    n_splits=5,\n",
        "    batch_size=32,\n",
        "    baseline=0.5,\n",
        "    alpha=alpha,\n",
        "    gamma=gamma,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    edge_index=edge_index,\n",
        "    class_names=class_names,\n",
        "    spatial_net=spatial_net,\n",
        "    temporal_net=temporal_net,\n",
        "    final_model=final_model\n",
        ")\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy:.4f}, CI: {ci_accuracy}, p-value: {p_accuracy:.4f}\")\n",
        "print(f\"Mean Sensitivity: {mean_sensitivity:.4f}, CI: {ci_sensitivity}, p-value: {p_sensitivity:.4f}\")\n",
        "print(f\"Mean Specificity: {mean_specificity:.4f}, CI: {ci_specificity}, p-value: {p_specificity:.4f}\")\n",
        "print(f\"Mean F1 Score: {mean_f1:.4f}, CI: {ci_f1}, p-value: {p_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1KdwLZDk30N",
        "outputId": "9f09dd31-8786-40dc-f33a-05531903c7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold 1/5...\n",
            "Epoch [1/47], Training Loss: 0.06557958573102951, Training Accuracy: 0.4732, Training Sensitivity: 0.6277, Training Specificity: 0.3615, Training F1: 0.5000\n",
            "Epoch [1/47], Validation Loss: 0.1092, Validation Accuracy: 0.4062, Validation Sensitivity: 0.9286, Validation Specificity: 0.0000, Validation F1: 0.5778\n",
            "Validation loss decreased (inf --> 0.109161).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.03973062336444855, Training Accuracy: 0.5357, Training Sensitivity: 0.7113, Training Specificity: 0.4016, Training F1: 0.5702\n",
            "Epoch [2/47], Validation Loss: 0.0996, Validation Accuracy: 0.4531, Validation Sensitivity: 1.0000, Validation Specificity: 0.0278, Validation F1: 0.6154\n",
            "Validation loss decreased (0.109161 --> 0.099561).  Saving model ...\n",
            "Epoch [3/47], Training Loss: 0.03828791528940201, Training Accuracy: 0.5045, Training Sensitivity: 0.6289, Training Specificity: 0.4094, Training F1: 0.5236\n",
            "Epoch [3/47], Validation Loss: 0.0923, Validation Accuracy: 0.4531, Validation Sensitivity: 1.0000, Validation Specificity: 0.0278, Validation F1: 0.6154\n",
            "Validation loss decreased (0.099561 --> 0.092284).  Saving model ...\n",
            "Epoch [4/47], Training Loss: 0.04137400537729263, Training Accuracy: 0.4866, Training Sensitivity: 0.5326, Training Specificity: 0.4545, Training F1: 0.4601\n",
            "Epoch [4/47], Validation Loss: 0.0838, Validation Accuracy: 0.4531, Validation Sensitivity: 1.0000, Validation Specificity: 0.0278, Validation F1: 0.6154\n",
            "Validation loss decreased (0.092284 --> 0.083836).  Saving model ...\n",
            "Epoch [5/47], Training Loss: 0.04890838637948036, Training Accuracy: 0.6161, Training Sensitivity: 0.7347, Training Specificity: 0.5238, Training F1: 0.6261\n",
            "Epoch [5/47], Validation Loss: 0.0822, Validation Accuracy: 0.5312, Validation Sensitivity: 1.0000, Validation Specificity: 0.1667, Validation F1: 0.6512\n",
            "Validation loss decreased (0.083836 --> 0.082158).  Saving model ...\n",
            "Epoch [6/47], Training Loss: 0.03576091304421425, Training Accuracy: 0.5893, Training Sensitivity: 0.6484, Training Specificity: 0.5489, Training F1: 0.5619\n",
            "Epoch [6/47], Validation Loss: 0.0761, Validation Accuracy: 0.5938, Validation Sensitivity: 1.0000, Validation Specificity: 0.2778, Validation F1: 0.6829\n",
            "Validation loss decreased (0.082158 --> 0.076111).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.03115544281899929, Training Accuracy: 0.6473, Training Sensitivity: 0.6957, Training Specificity: 0.6136, Training F1: 0.6184\n",
            "Epoch [7/47], Validation Loss: 0.0693, Validation Accuracy: 0.5781, Validation Sensitivity: 0.9286, Validation Specificity: 0.3056, Validation F1: 0.6582\n",
            "Validation loss decreased (0.076111 --> 0.069328).  Saving model ...\n",
            "Epoch [8/47], Training Loss: 0.03601668402552605, Training Accuracy: 0.6116, Training Sensitivity: 0.6344, Training Specificity: 0.5954, Training F1: 0.5756\n",
            "Epoch [8/47], Validation Loss: 0.0728, Validation Accuracy: 0.6094, Validation Sensitivity: 0.9286, Validation Specificity: 0.3611, Validation F1: 0.6753\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [9/47], Training Loss: 0.028682928532361984, Training Accuracy: 0.5938, Training Sensitivity: 0.5699, Training Specificity: 0.6107, Training F1: 0.5381\n",
            "Epoch [9/47], Validation Loss: 0.0714, Validation Accuracy: 0.5938, Validation Sensitivity: 0.7500, Validation Specificity: 0.4722, Validation F1: 0.6176\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [10/47], Training Loss: 0.04535731300711632, Training Accuracy: 0.5670, Training Sensitivity: 0.5978, Training Specificity: 0.5455, Training F1: 0.5314\n",
            "Epoch [10/47], Validation Loss: 0.0739, Validation Accuracy: 0.5469, Validation Sensitivity: 0.6429, Validation Specificity: 0.4722, Validation F1: 0.5538\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [11/47], Training Loss: 0.045500364154577255, Training Accuracy: 0.6339, Training Sensitivity: 0.6154, Training Specificity: 0.6466, Training F1: 0.5773\n",
            "Epoch [11/47], Validation Loss: 0.0756, Validation Accuracy: 0.6094, Validation Sensitivity: 0.8214, Validation Specificity: 0.4444, Validation F1: 0.6479\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [12/47], Training Loss: 0.024214906617999077, Training Accuracy: 0.6071, Training Sensitivity: 0.5895, Training Specificity: 0.6202, Training F1: 0.5600\n",
            "Epoch [12/47], Validation Loss: 0.0700, Validation Accuracy: 0.6250, Validation Sensitivity: 0.7857, Validation Specificity: 0.5000, Validation F1: 0.6471\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [13/47], Training Loss: 0.03693758696317673, Training Accuracy: 0.6875, Training Sensitivity: 0.7158, Training Specificity: 0.6667, Training F1: 0.6602\n",
            "Epoch [13/47], Validation Loss: 0.0695, Validation Accuracy: 0.6094, Validation Sensitivity: 0.5714, Validation Specificity: 0.6389, Validation F1: 0.5614\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [14/47], Training Loss: 0.030826101079583168, Training Accuracy: 0.6920, Training Sensitivity: 0.6632, Training Specificity: 0.7132, Training F1: 0.6462\n",
            "Epoch [14/47], Validation Loss: 0.0654, Validation Accuracy: 0.6719, Validation Sensitivity: 0.6071, Validation Specificity: 0.7222, Validation F1: 0.6182\n",
            "Validation loss decreased (0.069328 --> 0.065393).  Saving model ...\n",
            "Epoch [15/47], Training Loss: 0.033476121723651886, Training Accuracy: 0.6830, Training Sensitivity: 0.7053, Training Specificity: 0.6667, Training F1: 0.6537\n",
            "Epoch [15/47], Validation Loss: 0.0645, Validation Accuracy: 0.6875, Validation Sensitivity: 0.6429, Validation Specificity: 0.7222, Validation F1: 0.6429\n",
            "Validation loss decreased (0.065393 --> 0.064456).  Saving model ...\n",
            "Epoch [16/47], Training Loss: 0.03422265499830246, Training Accuracy: 0.6741, Training Sensitivity: 0.6702, Training Specificity: 0.6769, Training F1: 0.6332\n",
            "Epoch [16/47], Validation Loss: 0.0559, Validation Accuracy: 0.7031, Validation Sensitivity: 0.6429, Validation Specificity: 0.7500, Validation F1: 0.6545\n",
            "Validation loss decreased (0.064456 --> 0.055931).  Saving model ...\n",
            "Epoch [17/47], Training Loss: 0.03956550732254982, Training Accuracy: 0.7188, Training Sensitivity: 0.7263, Training Specificity: 0.7132, Training F1: 0.6866\n",
            "Epoch [17/47], Validation Loss: 0.0613, Validation Accuracy: 0.6719, Validation Sensitivity: 0.5714, Validation Specificity: 0.7500, Validation F1: 0.6038\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [18/47], Training Loss: 0.03715083375573158, Training Accuracy: 0.6652, Training Sensitivity: 0.6316, Training Specificity: 0.6899, Training F1: 0.6154\n",
            "Epoch [18/47], Validation Loss: 0.0629, Validation Accuracy: 0.7188, Validation Sensitivity: 0.7143, Validation Specificity: 0.7222, Validation F1: 0.6897\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [19/47], Training Loss: 0.022672463208436966, Training Accuracy: 0.7188, Training Sensitivity: 0.6771, Training Specificity: 0.7500, Training F1: 0.6736\n",
            "Epoch [19/47], Validation Loss: 0.0688, Validation Accuracy: 0.7031, Validation Sensitivity: 0.5357, Validation Specificity: 0.8333, Validation F1: 0.6122\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [20/47], Training Loss: 0.02938837558031082, Training Accuracy: 0.7321, Training Sensitivity: 0.7347, Training Specificity: 0.7302, Training F1: 0.7059\n",
            "Epoch [20/47], Validation Loss: 0.0763, Validation Accuracy: 0.6406, Validation Sensitivity: 0.3571, Validation Specificity: 0.8611, Validation F1: 0.4651\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [21/47], Training Loss: 0.01972275599837303, Training Accuracy: 0.7723, Training Sensitivity: 0.7600, Training Specificity: 0.7823, Training F1: 0.7488\n",
            "Epoch [21/47], Validation Loss: 0.0712, Validation Accuracy: 0.7031, Validation Sensitivity: 0.5714, Validation Specificity: 0.8056, Validation F1: 0.6275\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [22/47], Training Loss: 0.015245809219777584, Training Accuracy: 0.7991, Training Sensitivity: 0.8152, Training Specificity: 0.7879, Training F1: 0.7692\n",
            "Epoch [22/47], Validation Loss: 0.0640, Validation Accuracy: 0.7500, Validation Sensitivity: 0.6786, Validation Specificity: 0.8056, Validation F1: 0.7037\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [23/47], Training Loss: 0.030040651559829712, Training Accuracy: 0.7634, Training Sensitivity: 0.7692, Training Specificity: 0.7594, Training F1: 0.7254\n",
            "Epoch [23/47], Validation Loss: 0.0761, Validation Accuracy: 0.7344, Validation Sensitivity: 0.6071, Validation Specificity: 0.8333, Validation F1: 0.6667\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [24/47], Training Loss: 0.048335153609514236, Training Accuracy: 0.7991, Training Sensitivity: 0.7263, Training Specificity: 0.8527, Training F1: 0.7541\n",
            "Epoch [24/47], Validation Loss: 0.0784, Validation Accuracy: 0.7031, Validation Sensitivity: 0.4286, Validation Specificity: 0.9167, Validation F1: 0.5581\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [25/47], Training Loss: 0.02652394026517868, Training Accuracy: 0.7991, Training Sensitivity: 0.7629, Training Specificity: 0.8268, Training F1: 0.7668\n",
            "Epoch [25/47], Validation Loss: 0.0867, Validation Accuracy: 0.6875, Validation Sensitivity: 0.3929, Validation Specificity: 0.9167, Validation F1: 0.5238\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [26/47], Training Loss: 0.03630641847848892, Training Accuracy: 0.8170, Training Sensitivity: 0.7857, Training Specificity: 0.8413, Training F1: 0.7897\n",
            "Epoch [26/47], Validation Loss: 0.0765, Validation Accuracy: 0.7344, Validation Sensitivity: 0.6786, Validation Specificity: 0.7778, Validation F1: 0.6909\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 1: Accuracy: 0.7344, Sensitivity: 0.6786, Specificity: 0.7778, F1: 0.6909\n",
            "Processing fold 2/5...\n",
            "Epoch [1/47], Training Loss: 0.04308244213461876, Training Accuracy: 0.5268, Training Sensitivity: 0.4731, Training Specificity: 0.5649, Training F1: 0.4536\n",
            "Epoch [1/47], Validation Loss: 0.0666, Validation Accuracy: 0.6250, Validation Sensitivity: 0.5185, Validation Specificity: 0.7027, Validation F1: 0.5385\n",
            "Validation loss decreased (inf --> 0.066602).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.04873945564031601, Training Accuracy: 0.5580, Training Sensitivity: 0.5816, Training Specificity: 0.5397, Training F1: 0.5352\n",
            "Epoch [2/47], Validation Loss: 0.0670, Validation Accuracy: 0.5625, Validation Sensitivity: 0.7407, Validation Specificity: 0.4324, Validation F1: 0.5882\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [3/47], Training Loss: 0.05118619650602341, Training Accuracy: 0.5134, Training Sensitivity: 0.4479, Training Specificity: 0.5625, Training F1: 0.4410\n",
            "Epoch [3/47], Validation Loss: 0.0621, Validation Accuracy: 0.6562, Validation Sensitivity: 0.2963, Validation Specificity: 0.9189, Validation F1: 0.4211\n",
            "Validation loss decreased (0.066602 --> 0.062061).  Saving model ...\n",
            "Epoch [4/47], Training Loss: 0.04528822377324104, Training Accuracy: 0.5804, Training Sensitivity: 0.5670, Training Specificity: 0.5906, Training F1: 0.5392\n",
            "Epoch [4/47], Validation Loss: 0.0609, Validation Accuracy: 0.6875, Validation Sensitivity: 0.2963, Validation Specificity: 0.9730, Validation F1: 0.4444\n",
            "Validation loss decreased (0.062061 --> 0.060936).  Saving model ...\n",
            "Epoch [5/47], Training Loss: 0.062116771936416626, Training Accuracy: 0.4777, Training Sensitivity: 0.4737, Training Specificity: 0.4806, Training F1: 0.4348\n",
            "Epoch [5/47], Validation Loss: 0.0626, Validation Accuracy: 0.5625, Validation Sensitivity: 0.0000, Validation Specificity: 0.9730, Validation F1: 0.0000\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [6/47], Training Loss: 0.0268919188529253, Training Accuracy: 0.5491, Training Sensitivity: 0.5263, Training Specificity: 0.5659, Training F1: 0.4975\n",
            "Epoch [6/47], Validation Loss: 0.0608, Validation Accuracy: 0.5781, Validation Sensitivity: 0.0370, Validation Specificity: 0.9730, Validation F1: 0.0690\n",
            "Validation loss decreased (0.060936 --> 0.060778).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.049984827637672424, Training Accuracy: 0.5536, Training Sensitivity: 0.4848, Training Specificity: 0.6080, Training F1: 0.4898\n",
            "Epoch [7/47], Validation Loss: 0.0595, Validation Accuracy: 0.5625, Validation Sensitivity: 0.0000, Validation Specificity: 0.9730, Validation F1: 0.0000\n",
            "Validation loss decreased (0.060778 --> 0.059536).  Saving model ...\n",
            "Epoch [8/47], Training Loss: 0.041506577283144, Training Accuracy: 0.6071, Training Sensitivity: 0.5842, Training Specificity: 0.6260, Training F1: 0.5728\n",
            "Epoch [8/47], Validation Loss: 0.0606, Validation Accuracy: 0.5938, Validation Sensitivity: 0.0370, Validation Specificity: 1.0000, Validation F1: 0.0714\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [9/47], Training Loss: 0.04606221988797188, Training Accuracy: 0.5625, Training Sensitivity: 0.5155, Training Specificity: 0.5984, Training F1: 0.5051\n",
            "Epoch [9/47], Validation Loss: 0.0628, Validation Accuracy: 0.5938, Validation Sensitivity: 0.0741, Validation Specificity: 0.9730, Validation F1: 0.1333\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [10/47], Training Loss: 0.03918416053056717, Training Accuracy: 0.5893, Training Sensitivity: 0.5354, Training Specificity: 0.6320, Training F1: 0.5354\n",
            "Epoch [10/47], Validation Loss: 0.0616, Validation Accuracy: 0.5938, Validation Sensitivity: 0.0741, Validation Specificity: 0.9730, Validation F1: 0.1333\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [11/47], Training Loss: 0.05450024455785751, Training Accuracy: 0.5848, Training Sensitivity: 0.5612, Training Specificity: 0.6032, Training F1: 0.5419\n",
            "Epoch [11/47], Validation Loss: 0.0584, Validation Accuracy: 0.6875, Validation Sensitivity: 0.3704, Validation Specificity: 0.9189, Validation F1: 0.5000\n",
            "Validation loss decreased (0.059536 --> 0.058367).  Saving model ...\n",
            "Epoch [12/47], Training Loss: 0.03363136202096939, Training Accuracy: 0.5982, Training Sensitivity: 0.5941, Training Specificity: 0.6016, Training F1: 0.5714\n",
            "Epoch [12/47], Validation Loss: 0.0543, Validation Accuracy: 0.8125, Validation Sensitivity: 0.7037, Validation Specificity: 0.8919, Validation F1: 0.7600\n",
            "Validation loss decreased (0.058367 --> 0.054337).  Saving model ...\n",
            "Epoch [13/47], Training Loss: 0.037112198770046234, Training Accuracy: 0.6607, Training Sensitivity: 0.5758, Training Specificity: 0.7280, Training F1: 0.6000\n",
            "Epoch [13/47], Validation Loss: 0.0554, Validation Accuracy: 0.7969, Validation Sensitivity: 0.6667, Validation Specificity: 0.8919, Validation F1: 0.7347\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [14/47], Training Loss: 0.03414561599493027, Training Accuracy: 0.6250, Training Sensitivity: 0.4792, Training Specificity: 0.7344, Training F1: 0.5227\n",
            "Epoch [14/47], Validation Loss: 0.0558, Validation Accuracy: 0.7969, Validation Sensitivity: 0.6667, Validation Specificity: 0.8919, Validation F1: 0.7347\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [15/47], Training Loss: 0.03563511371612549, Training Accuracy: 0.6562, Training Sensitivity: 0.5870, Training Specificity: 0.7045, Training F1: 0.5838\n",
            "Epoch [15/47], Validation Loss: 0.0593, Validation Accuracy: 0.7812, Validation Sensitivity: 0.6667, Validation Specificity: 0.8649, Validation F1: 0.7200\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [16/47], Training Loss: 0.03304211050271988, Training Accuracy: 0.7143, Training Sensitivity: 0.6344, Training Specificity: 0.7710, Training F1: 0.6484\n",
            "Epoch [16/47], Validation Loss: 0.0656, Validation Accuracy: 0.7812, Validation Sensitivity: 0.6667, Validation Specificity: 0.8649, Validation F1: 0.7200\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [17/47], Training Loss: 0.03377090021967888, Training Accuracy: 0.6652, Training Sensitivity: 0.6224, Training Specificity: 0.6984, Training F1: 0.6193\n",
            "Epoch [17/47], Validation Loss: 0.0694, Validation Accuracy: 0.7812, Validation Sensitivity: 0.8148, Validation Specificity: 0.7568, Validation F1: 0.7586\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [18/47], Training Loss: 0.030065644532442093, Training Accuracy: 0.7411, Training Sensitivity: 0.7113, Training Specificity: 0.7638, Training F1: 0.7041\n",
            "Epoch [18/47], Validation Loss: 0.0700, Validation Accuracy: 0.7969, Validation Sensitivity: 0.8148, Validation Specificity: 0.7838, Validation F1: 0.7719\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [19/47], Training Loss: 0.038468122482299805, Training Accuracy: 0.7277, Training Sensitivity: 0.6735, Training Specificity: 0.7698, Training F1: 0.6839\n",
            "Epoch [19/47], Validation Loss: 0.0697, Validation Accuracy: 0.8125, Validation Sensitivity: 0.8148, Validation Specificity: 0.8108, Validation F1: 0.7857\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [20/47], Training Loss: 0.01789649948477745, Training Accuracy: 0.7946, Training Sensitivity: 0.7677, Training Specificity: 0.8160, Training F1: 0.7677\n",
            "Epoch [20/47], Validation Loss: 0.0730, Validation Accuracy: 0.7812, Validation Sensitivity: 0.7407, Validation Specificity: 0.8108, Validation F1: 0.7407\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [21/47], Training Loss: 0.0341838113963604, Training Accuracy: 0.7991, Training Sensitivity: 0.7895, Training Specificity: 0.8062, Training F1: 0.7692\n",
            "Epoch [21/47], Validation Loss: 0.0844, Validation Accuracy: 0.7344, Validation Sensitivity: 0.8148, Validation Specificity: 0.6757, Validation F1: 0.7213\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [22/47], Training Loss: 0.02810472808778286, Training Accuracy: 0.7857, Training Sensitivity: 0.8041, Training Specificity: 0.7717, Training F1: 0.7647\n",
            "Epoch [22/47], Validation Loss: 0.0752, Validation Accuracy: 0.7344, Validation Sensitivity: 0.8148, Validation Specificity: 0.6757, Validation F1: 0.7213\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 2: Accuracy: 0.7344, Sensitivity: 0.8148, Specificity: 0.6757, F1: 0.7213\n",
            "Processing fold 3/5...\n",
            "Epoch [1/47], Training Loss: 0.045926257967948914, Training Accuracy: 0.4554, Training Sensitivity: 0.4731, Training Specificity: 0.4427, Training F1: 0.4190\n",
            "Epoch [1/47], Validation Loss: 0.0754, Validation Accuracy: 0.4219, Validation Sensitivity: 0.6296, Validation Specificity: 0.2703, Validation F1: 0.4789\n",
            "Validation loss decreased (inf --> 0.075401).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.047964561730623245, Training Accuracy: 0.5089, Training Sensitivity: 0.5870, Training Specificity: 0.4545, Training F1: 0.4954\n",
            "Epoch [2/47], Validation Loss: 0.0758, Validation Accuracy: 0.3438, Validation Sensitivity: 0.6296, Validation Specificity: 0.1351, Validation F1: 0.4474\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [3/47], Training Loss: 0.044694699347019196, Training Accuracy: 0.5536, Training Sensitivity: 0.6000, Training Specificity: 0.5161, Training F1: 0.5455\n",
            "Epoch [3/47], Validation Loss: 0.0724, Validation Accuracy: 0.4375, Validation Sensitivity: 0.4815, Validation Specificity: 0.4054, Validation F1: 0.4194\n",
            "Validation loss decreased (0.075401 --> 0.072415).  Saving model ...\n",
            "Epoch [4/47], Training Loss: 0.060924969613552094, Training Accuracy: 0.4688, Training Sensitivity: 0.4947, Training Specificity: 0.4496, Training F1: 0.4413\n",
            "Epoch [4/47], Validation Loss: 0.0715, Validation Accuracy: 0.5156, Validation Sensitivity: 0.7407, Validation Specificity: 0.3514, Validation F1: 0.5634\n",
            "Validation loss decreased (0.072415 --> 0.071477).  Saving model ...\n",
            "Epoch [5/47], Training Loss: 0.05545646697282791, Training Accuracy: 0.5357, Training Sensitivity: 0.5474, Training Specificity: 0.5271, Training F1: 0.5000\n",
            "Epoch [5/47], Validation Loss: 0.0658, Validation Accuracy: 0.6094, Validation Sensitivity: 0.4074, Validation Specificity: 0.7568, Validation F1: 0.4681\n",
            "Validation loss decreased (0.071477 --> 0.065793).  Saving model ...\n",
            "Epoch [6/47], Training Loss: 0.028635932132601738, Training Accuracy: 0.6250, Training Sensitivity: 0.6701, Training Specificity: 0.5906, Training F1: 0.6075\n",
            "Epoch [6/47], Validation Loss: 0.0634, Validation Accuracy: 0.6719, Validation Sensitivity: 0.8519, Validation Specificity: 0.5405, Validation F1: 0.6866\n",
            "Validation loss decreased (0.065793 --> 0.063362).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.034611791372299194, Training Accuracy: 0.5536, Training Sensitivity: 0.5464, Training Specificity: 0.5591, Training F1: 0.5146\n",
            "Epoch [7/47], Validation Loss: 0.0593, Validation Accuracy: 0.6250, Validation Sensitivity: 0.8519, Validation Specificity: 0.4595, Validation F1: 0.6571\n",
            "Validation loss decreased (0.063362 --> 0.059305).  Saving model ...\n",
            "Epoch [8/47], Training Loss: 0.04753383621573448, Training Accuracy: 0.6027, Training Sensitivity: 0.5684, Training Specificity: 0.6279, Training F1: 0.5482\n",
            "Epoch [8/47], Validation Loss: 0.0676, Validation Accuracy: 0.5781, Validation Sensitivity: 0.9259, Validation Specificity: 0.3243, Validation F1: 0.6494\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [9/47], Training Loss: 0.045762017369270325, Training Accuracy: 0.5536, Training Sensitivity: 0.5714, Training Specificity: 0.5397, Training F1: 0.5283\n",
            "Epoch [9/47], Validation Loss: 0.0698, Validation Accuracy: 0.5938, Validation Sensitivity: 0.9259, Validation Specificity: 0.3514, Validation F1: 0.6579\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [10/47], Training Loss: 0.03956516832113266, Training Accuracy: 0.5536, Training Sensitivity: 0.5408, Training Specificity: 0.5635, Training F1: 0.5146\n",
            "Epoch [10/47], Validation Loss: 0.0624, Validation Accuracy: 0.6250, Validation Sensitivity: 0.8889, Validation Specificity: 0.4324, Validation F1: 0.6667\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [11/47], Training Loss: 0.03635040670633316, Training Accuracy: 0.6429, Training Sensitivity: 0.6562, Training Specificity: 0.6328, Training F1: 0.6117\n",
            "Epoch [11/47], Validation Loss: 0.0601, Validation Accuracy: 0.6250, Validation Sensitivity: 0.8889, Validation Specificity: 0.4324, Validation F1: 0.6667\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [12/47], Training Loss: 0.03688995540142059, Training Accuracy: 0.5804, Training Sensitivity: 0.5455, Training Specificity: 0.6080, Training F1: 0.5347\n",
            "Epoch [12/47], Validation Loss: 0.0596, Validation Accuracy: 0.6406, Validation Sensitivity: 0.9630, Validation Specificity: 0.4054, Validation F1: 0.6933\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [13/47], Training Loss: 0.0419350266456604, Training Accuracy: 0.6295, Training Sensitivity: 0.6000, Training Specificity: 0.6512, Training F1: 0.5787\n",
            "Epoch [13/47], Validation Loss: 0.0563, Validation Accuracy: 0.6562, Validation Sensitivity: 0.9630, Validation Specificity: 0.4324, Validation F1: 0.7027\n",
            "Validation loss decreased (0.059305 --> 0.056301).  Saving model ...\n",
            "Epoch [14/47], Training Loss: 0.04051406681537628, Training Accuracy: 0.6339, Training Sensitivity: 0.6000, Training Specificity: 0.6589, Training F1: 0.5816\n",
            "Epoch [14/47], Validation Loss: 0.0523, Validation Accuracy: 0.7031, Validation Sensitivity: 0.9630, Validation Specificity: 0.5135, Validation F1: 0.7324\n",
            "Validation loss decreased (0.056301 --> 0.052282).  Saving model ...\n",
            "Epoch [15/47], Training Loss: 0.03216836601495743, Training Accuracy: 0.5759, Training Sensitivity: 0.5000, Training Specificity: 0.6288, Training F1: 0.4920\n",
            "Epoch [15/47], Validation Loss: 0.0600, Validation Accuracy: 0.6250, Validation Sensitivity: 1.0000, Validation Specificity: 0.3514, Validation F1: 0.6923\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [16/47], Training Loss: 0.020494239404797554, Training Accuracy: 0.7277, Training Sensitivity: 0.6701, Training Specificity: 0.7717, Training F1: 0.6806\n",
            "Epoch [16/47], Validation Loss: 0.0479, Validation Accuracy: 0.8281, Validation Sensitivity: 0.9630, Validation Specificity: 0.7297, Validation F1: 0.8254\n",
            "Validation loss decreased (0.052282 --> 0.047933).  Saving model ...\n",
            "Epoch [17/47], Training Loss: 0.02872214838862419, Training Accuracy: 0.6964, Training Sensitivity: 0.6186, Training Specificity: 0.7559, Training F1: 0.6383\n",
            "Epoch [17/47], Validation Loss: 0.0463, Validation Accuracy: 0.8281, Validation Sensitivity: 0.9630, Validation Specificity: 0.7297, Validation F1: 0.8254\n",
            "Validation loss decreased (0.047933 --> 0.046324).  Saving model ...\n",
            "Epoch [18/47], Training Loss: 0.029579175636172295, Training Accuracy: 0.7634, Training Sensitivity: 0.7200, Training Specificity: 0.7984, Training F1: 0.7310\n",
            "Epoch [18/47], Validation Loss: 0.0501, Validation Accuracy: 0.8281, Validation Sensitivity: 1.0000, Validation Specificity: 0.7027, Validation F1: 0.8308\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [19/47], Training Loss: 0.030110130086541176, Training Accuracy: 0.7723, Training Sensitivity: 0.7717, Training Specificity: 0.7727, Training F1: 0.7358\n",
            "Epoch [19/47], Validation Loss: 0.0497, Validation Accuracy: 0.8438, Validation Sensitivity: 0.9259, Validation Specificity: 0.7838, Validation F1: 0.8333\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [20/47], Training Loss: 0.022655392065644264, Training Accuracy: 0.7589, Training Sensitivity: 0.7653, Training Specificity: 0.7540, Training F1: 0.7353\n",
            "Epoch [20/47], Validation Loss: 0.0605, Validation Accuracy: 0.8438, Validation Sensitivity: 0.8519, Validation Specificity: 0.8378, Validation F1: 0.8214\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [21/47], Training Loss: 0.043983519077301025, Training Accuracy: 0.7589, Training Sensitivity: 0.6915, Training Specificity: 0.8077, Training F1: 0.7065\n",
            "Epoch [21/47], Validation Loss: 0.0718, Validation Accuracy: 0.7188, Validation Sensitivity: 0.9630, Validation Specificity: 0.5405, Validation F1: 0.7429\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [22/47], Training Loss: 0.03380078077316284, Training Accuracy: 0.7455, Training Sensitivity: 0.6923, Training Specificity: 0.7820, Training F1: 0.6885\n",
            "Epoch [22/47], Validation Loss: 0.0548, Validation Accuracy: 0.8438, Validation Sensitivity: 0.8889, Validation Specificity: 0.8108, Validation F1: 0.8276\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [23/47], Training Loss: 0.01434682309627533, Training Accuracy: 0.7902, Training Sensitivity: 0.8367, Training Specificity: 0.7540, Training F1: 0.7773\n",
            "Epoch [23/47], Validation Loss: 0.0795, Validation Accuracy: 0.7656, Validation Sensitivity: 0.8889, Validation Specificity: 0.6757, Validation F1: 0.7619\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [24/47], Training Loss: 0.010745160281658173, Training Accuracy: 0.8125, Training Sensitivity: 0.8182, Training Specificity: 0.8080, Training F1: 0.7941\n",
            "Epoch [24/47], Validation Loss: 0.0471, Validation Accuracy: 0.8750, Validation Sensitivity: 0.8889, Validation Specificity: 0.8649, Validation F1: 0.8571\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [25/47], Training Loss: 0.01160489022731781, Training Accuracy: 0.7902, Training Sensitivity: 0.7629, Training Specificity: 0.8110, Training F1: 0.7590\n",
            "Epoch [25/47], Validation Loss: 0.0681, Validation Accuracy: 0.7656, Validation Sensitivity: 0.9259, Validation Specificity: 0.6486, Validation F1: 0.7692\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [26/47], Training Loss: 0.010811456479132175, Training Accuracy: 0.8214, Training Sensitivity: 0.8061, Training Specificity: 0.8333, Training F1: 0.7980\n",
            "Epoch [26/47], Validation Loss: 0.0520, Validation Accuracy: 0.8438, Validation Sensitivity: 0.9259, Validation Specificity: 0.7838, Validation F1: 0.8333\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [27/47], Training Loss: 0.02276064269244671, Training Accuracy: 0.8214, Training Sensitivity: 0.8571, Training Specificity: 0.7970, Training F1: 0.7959\n",
            "Epoch [27/47], Validation Loss: 0.0637, Validation Accuracy: 0.7969, Validation Sensitivity: 0.9259, Validation Specificity: 0.7027, Validation F1: 0.7937\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 3: Accuracy: 0.7969, Sensitivity: 0.9259, Specificity: 0.7027, F1: 0.7937\n",
            "Processing fold 4/5...\n",
            "Epoch [1/47], Training Loss: 0.056665729731321335, Training Accuracy: 0.4375, Training Sensitivity: 0.3235, Training Specificity: 0.5328, Training F1: 0.3438\n",
            "Epoch [1/47], Validation Loss: 0.0375, Validation Accuracy: 0.5938, Validation Sensitivity: 0.0000, Validation Specificity: 1.0000, Validation F1: 0.0000\n",
            "Validation loss decreased (inf --> 0.037546).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.05784919112920761, Training Accuracy: 0.4643, Training Sensitivity: 0.3478, Training Specificity: 0.5455, Training F1: 0.3478\n",
            "Epoch [2/47], Validation Loss: 0.0336, Validation Accuracy: 0.5625, Validation Sensitivity: 0.1333, Validation Specificity: 0.9412, Validation F1: 0.2222\n",
            "Validation loss decreased (0.037546 --> 0.033611).  Saving model ...\n",
            "Epoch [3/47], Training Loss: 0.03872150927782059, Training Accuracy: 0.5938, Training Sensitivity: 0.5567, Training Specificity: 0.6220, Training F1: 0.5427\n",
            "Epoch [3/47], Validation Loss: 0.0313, Validation Accuracy: 0.6562, Validation Sensitivity: 0.2308, Validation Specificity: 0.9474, Validation F1: 0.3529\n",
            "Validation loss decreased (0.033611 --> 0.031339).  Saving model ...\n",
            "Epoch [4/47], Training Loss: 0.05049201846122742, Training Accuracy: 0.5312, Training Sensitivity: 0.5053, Training Specificity: 0.5504, Training F1: 0.4776\n",
            "Epoch [4/47], Validation Loss: 0.0324, Validation Accuracy: 0.5625, Validation Sensitivity: 0.1250, Validation Specificity: 1.0000, Validation F1: 0.2222\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [5/47], Training Loss: 0.0519978292286396, Training Accuracy: 0.5402, Training Sensitivity: 0.4000, Training Specificity: 0.6434, Training F1: 0.4246\n",
            "Epoch [5/47], Validation Loss: 0.0332, Validation Accuracy: 0.5312, Validation Sensitivity: 0.1765, Validation Specificity: 0.9333, Validation F1: 0.2857\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [6/47], Training Loss: 0.048436831682920456, Training Accuracy: 0.5759, Training Sensitivity: 0.5000, Training Specificity: 0.6349, Training F1: 0.5078\n",
            "Epoch [6/47], Validation Loss: 0.0282, Validation Accuracy: 0.6562, Validation Sensitivity: 0.1667, Validation Specificity: 0.9500, Validation F1: 0.2667\n",
            "Validation loss decreased (0.031339 --> 0.028242).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.03432143107056618, Training Accuracy: 0.5848, Training Sensitivity: 0.5100, Training Specificity: 0.6452, Training F1: 0.5231\n",
            "Epoch [7/47], Validation Loss: 0.0257, Validation Accuracy: 0.8438, Validation Sensitivity: 0.6364, Validation Specificity: 0.9524, Validation F1: 0.7368\n",
            "Validation loss decreased (0.028242 --> 0.025706).  Saving model ...\n",
            "Epoch [8/47], Training Loss: 0.03456027805805206, Training Accuracy: 0.6384, Training Sensitivity: 0.5745, Training Specificity: 0.6846, Training F1: 0.5714\n",
            "Epoch [8/47], Validation Loss: 0.0278, Validation Accuracy: 0.6875, Validation Sensitivity: 0.6667, Validation Specificity: 0.7059, Validation F1: 0.6667\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [9/47], Training Loss: 0.037549328058958054, Training Accuracy: 0.5714, Training Sensitivity: 0.4375, Training Specificity: 0.6719, Training F1: 0.4667\n",
            "Epoch [9/47], Validation Loss: 0.0275, Validation Accuracy: 0.6875, Validation Sensitivity: 0.6154, Validation Specificity: 0.7368, Validation F1: 0.6154\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [10/47], Training Loss: 0.0416308268904686, Training Accuracy: 0.5625, Training Sensitivity: 0.4848, Training Specificity: 0.6240, Training F1: 0.4948\n",
            "Epoch [10/47], Validation Loss: 0.0266, Validation Accuracy: 0.7500, Validation Sensitivity: 0.7500, Validation Specificity: 0.7500, Validation F1: 0.6923\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [11/47], Training Loss: 0.03674400597810745, Training Accuracy: 0.6071, Training Sensitivity: 0.4787, Training Specificity: 0.7000, Training F1: 0.5056\n",
            "Epoch [11/47], Validation Loss: 0.0242, Validation Accuracy: 0.7188, Validation Sensitivity: 0.6875, Validation Specificity: 0.7500, Validation F1: 0.7097\n",
            "Validation loss decreased (0.025706 --> 0.024232).  Saving model ...\n",
            "Epoch [12/47], Training Loss: 0.025636592879891396, Training Accuracy: 0.6250, Training Sensitivity: 0.5556, Training Specificity: 0.6800, Training F1: 0.5670\n",
            "Epoch [12/47], Validation Loss: 0.0218, Validation Accuracy: 0.7812, Validation Sensitivity: 0.6250, Validation Specificity: 0.9375, Validation F1: 0.7407\n",
            "Validation loss decreased (0.024232 --> 0.021782).  Saving model ...\n",
            "Epoch [13/47], Training Loss: 0.03189250826835632, Training Accuracy: 0.6339, Training Sensitivity: 0.6327, Training Specificity: 0.6349, Training F1: 0.6019\n",
            "Epoch [13/47], Validation Loss: 0.0274, Validation Accuracy: 0.6562, Validation Sensitivity: 0.5556, Validation Specificity: 0.7857, Validation F1: 0.6452\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [14/47], Training Loss: 0.028183139860630035, Training Accuracy: 0.6562, Training Sensitivity: 0.6250, Training Specificity: 0.6797, Training F1: 0.6091\n",
            "Epoch [14/47], Validation Loss: 0.0243, Validation Accuracy: 0.7188, Validation Sensitivity: 0.5625, Validation Specificity: 0.8750, Validation F1: 0.6667\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [15/47], Training Loss: 0.03643323481082916, Training Accuracy: 0.6205, Training Sensitivity: 0.5510, Training Specificity: 0.6746, Training F1: 0.5596\n",
            "Epoch [15/47], Validation Loss: 0.0250, Validation Accuracy: 0.7500, Validation Sensitivity: 0.5625, Validation Specificity: 0.9375, Validation F1: 0.6923\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [16/47], Training Loss: 0.03153745457530022, Training Accuracy: 0.6473, Training Sensitivity: 0.5625, Training Specificity: 0.7109, Training F1: 0.5775\n",
            "Epoch [16/47], Validation Loss: 0.0193, Validation Accuracy: 0.8438, Validation Sensitivity: 0.6923, Validation Specificity: 0.9474, Validation F1: 0.7826\n",
            "Validation loss decreased (0.021782 --> 0.019292).  Saving model ...\n",
            "Epoch [17/47], Training Loss: 0.03625497594475746, Training Accuracy: 0.6875, Training Sensitivity: 0.6701, Training Specificity: 0.7008, Training F1: 0.6500\n",
            "Epoch [17/47], Validation Loss: 0.0191, Validation Accuracy: 0.8125, Validation Sensitivity: 0.6154, Validation Specificity: 0.9474, Validation F1: 0.7273\n",
            "Validation loss decreased (0.019292 --> 0.019088).  Saving model ...\n",
            "Epoch [18/47], Training Loss: 0.023557065054774284, Training Accuracy: 0.6518, Training Sensitivity: 0.6300, Training Specificity: 0.6694, Training F1: 0.6176\n",
            "Epoch [18/47], Validation Loss: 0.0213, Validation Accuracy: 0.8438, Validation Sensitivity: 0.8333, Validation Specificity: 0.8571, Validation F1: 0.8571\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [19/47], Training Loss: 0.027347426861524582, Training Accuracy: 0.7098, Training Sensitivity: 0.7340, Training Specificity: 0.6923, Training F1: 0.6798\n",
            "Epoch [19/47], Validation Loss: 0.0143, Validation Accuracy: 0.8750, Validation Sensitivity: 0.9231, Validation Specificity: 0.8421, Validation F1: 0.8571\n",
            "Validation loss decreased (0.019088 --> 0.014266).  Saving model ...\n",
            "Epoch [20/47], Training Loss: 0.028534943237900734, Training Accuracy: 0.7545, Training Sensitivity: 0.7188, Training Specificity: 0.7812, Training F1: 0.7150\n",
            "Epoch [20/47], Validation Loss: 0.0169, Validation Accuracy: 0.8125, Validation Sensitivity: 0.5714, Validation Specificity: 1.0000, Validation F1: 0.7273\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [21/47], Training Loss: 0.021271217614412308, Training Accuracy: 0.7589, Training Sensitivity: 0.7423, Training Specificity: 0.7717, Training F1: 0.7273\n",
            "Epoch [21/47], Validation Loss: 0.0164, Validation Accuracy: 0.8750, Validation Sensitivity: 0.9091, Validation Specificity: 0.8571, Validation F1: 0.8333\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [22/47], Training Loss: 0.025329476222395897, Training Accuracy: 0.7500, Training Sensitivity: 0.7283, Training Specificity: 0.7652, Training F1: 0.7053\n",
            "Epoch [22/47], Validation Loss: 0.0158, Validation Accuracy: 0.8125, Validation Sensitivity: 0.6667, Validation Specificity: 0.9412, Validation F1: 0.7692\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [23/47], Training Loss: 0.016653044149279594, Training Accuracy: 0.7500, Training Sensitivity: 0.7188, Training Specificity: 0.7734, Training F1: 0.7113\n",
            "Epoch [23/47], Validation Loss: 0.0091, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9524, Validation F1: 0.9565\n",
            "Validation loss decreased (0.014266 --> 0.009097).  Saving model ...\n",
            "Epoch [24/47], Training Loss: 0.02283211424946785, Training Accuracy: 0.7723, Training Sensitivity: 0.7368, Training Specificity: 0.7984, Training F1: 0.7330\n",
            "Epoch [24/47], Validation Loss: 0.0160, Validation Accuracy: 0.8438, Validation Sensitivity: 0.7333, Validation Specificity: 0.9412, Validation F1: 0.8148\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [25/47], Training Loss: 0.021855061873793602, Training Accuracy: 0.8080, Training Sensitivity: 0.7917, Training Specificity: 0.8203, Training F1: 0.7795\n",
            "Epoch [25/47], Validation Loss: 0.0126, Validation Accuracy: 0.9375, Validation Sensitivity: 1.0000, Validation Specificity: 0.8947, Validation F1: 0.9286\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [26/47], Training Loss: 0.017076874151825905, Training Accuracy: 0.8348, Training Sensitivity: 0.8125, Training Specificity: 0.8516, Training F1: 0.8083\n",
            "Epoch [26/47], Validation Loss: 0.0087, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9333, Validation F1: 0.9714\n",
            "Validation loss decreased (0.009097 --> 0.008702).  Saving model ...\n",
            "Epoch [27/47], Training Loss: 0.03483198583126068, Training Accuracy: 0.7991, Training Sensitivity: 0.7551, Training Specificity: 0.8333, Training F1: 0.7668\n",
            "Epoch [27/47], Validation Loss: 0.0268, Validation Accuracy: 0.8125, Validation Sensitivity: 0.6250, Validation Specificity: 1.0000, Validation F1: 0.7692\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [28/47], Training Loss: 0.023764126002788544, Training Accuracy: 0.8036, Training Sensitivity: 0.8065, Training Specificity: 0.8015, Training F1: 0.7732\n",
            "Epoch [28/47], Validation Loss: 0.0168, Validation Accuracy: 0.8438, Validation Sensitivity: 0.6429, Validation Specificity: 1.0000, Validation F1: 0.7826\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [29/47], Training Loss: 0.02373735047876835, Training Accuracy: 0.8214, Training Sensitivity: 0.8144, Training Specificity: 0.8268, Training F1: 0.7980\n",
            "Epoch [29/47], Validation Loss: 0.0082, Validation Accuracy: 0.9375, Validation Sensitivity: 0.8667, Validation Specificity: 1.0000, Validation F1: 0.9286\n",
            "Validation loss decreased (0.008702 --> 0.008230).  Saving model ...\n",
            "Epoch [30/47], Training Loss: 0.015956936404109, Training Accuracy: 0.8348, Training Sensitivity: 0.8617, Training Specificity: 0.8154, Training F1: 0.8141\n",
            "Epoch [30/47], Validation Loss: 0.0140, Validation Accuracy: 0.9375, Validation Sensitivity: 0.9474, Validation Specificity: 0.9231, Validation F1: 0.9474\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [31/47], Training Loss: 0.01817978173494339, Training Accuracy: 0.8036, Training Sensitivity: 0.8351, Training Specificity: 0.7795, Training F1: 0.7864\n",
            "Epoch [31/47], Validation Loss: 0.0147, Validation Accuracy: 0.8750, Validation Sensitivity: 0.9231, Validation Specificity: 0.8421, Validation F1: 0.8571\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [32/47], Training Loss: 0.01837167888879776, Training Accuracy: 0.8259, Training Sensitivity: 0.8333, Training Specificity: 0.8203, Training F1: 0.8040\n",
            "Epoch [32/47], Validation Loss: 0.0147, Validation Accuracy: 0.8750, Validation Sensitivity: 0.8333, Validation Specificity: 0.9000, Validation F1: 0.8333\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [33/47], Training Loss: 0.02481880784034729, Training Accuracy: 0.8527, Training Sensitivity: 0.8687, Training Specificity: 0.8400, Training F1: 0.8390\n",
            "Epoch [33/47], Validation Loss: 0.0166, Validation Accuracy: 0.8438, Validation Sensitivity: 0.5000, Validation Specificity: 1.0000, Validation F1: 0.6667\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [34/47], Training Loss: 0.02196470834314823, Training Accuracy: 0.8348, Training Sensitivity: 0.8105, Training Specificity: 0.8527, Training F1: 0.8063\n",
            "Epoch [34/47], Validation Loss: 0.0171, Validation Accuracy: 0.8750, Validation Sensitivity: 0.7143, Validation Specificity: 1.0000, Validation F1: 0.8333\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [35/47], Training Loss: 0.01875346153974533, Training Accuracy: 0.8839, Training Sensitivity: 0.8947, Training Specificity: 0.8760, Training F1: 0.8673\n",
            "Epoch [35/47], Validation Loss: 0.0209, Validation Accuracy: 0.8125, Validation Sensitivity: 0.6875, Validation Specificity: 0.9375, Validation F1: 0.7857\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [36/47], Training Loss: 0.012356637977063656, Training Accuracy: 0.8571, Training Sensitivity: 0.8438, Training Specificity: 0.8672, Training F1: 0.8351\n",
            "Epoch [36/47], Validation Loss: 0.0193, Validation Accuracy: 0.8750, Validation Sensitivity: 0.7143, Validation Specificity: 1.0000, Validation F1: 0.8333\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [37/47], Training Loss: 0.027485474944114685, Training Accuracy: 0.8438, Training Sensitivity: 0.8804, Training Specificity: 0.8182, Training F1: 0.8223\n",
            "Epoch [37/47], Validation Loss: 0.0078, Validation Accuracy: 0.9375, Validation Sensitivity: 0.9091, Validation Specificity: 0.9524, Validation F1: 0.9091\n",
            "Validation loss decreased (0.008230 --> 0.007835).  Saving model ...\n",
            "Epoch [38/47], Training Loss: 0.024638207629323006, Training Accuracy: 0.8929, Training Sensitivity: 0.8854, Training Specificity: 0.8984, Training F1: 0.8763\n",
            "Epoch [38/47], Validation Loss: 0.0096, Validation Accuracy: 0.9375, Validation Sensitivity: 0.9231, Validation Specificity: 0.9474, Validation F1: 0.9231\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [39/47], Training Loss: 0.012366857379674911, Training Accuracy: 0.8348, Training Sensitivity: 0.8817, Training Specificity: 0.8015, Training F1: 0.8159\n",
            "Epoch [39/47], Validation Loss: 0.0143, Validation Accuracy: 0.8125, Validation Sensitivity: 0.9091, Validation Specificity: 0.7619, Validation F1: 0.7692\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [40/47], Training Loss: 0.01513521745800972, Training Accuracy: 0.8571, Training Sensitivity: 0.9082, Training Specificity: 0.8175, Training F1: 0.8476\n",
            "Epoch [40/47], Validation Loss: 0.0084, Validation Accuracy: 0.9375, Validation Sensitivity: 0.9231, Validation Specificity: 0.9474, Validation F1: 0.9231\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [41/47], Training Loss: 0.01830490119755268, Training Accuracy: 0.8705, Training Sensitivity: 0.9375, Training Specificity: 0.8203, Training F1: 0.8612\n",
            "Epoch [41/47], Validation Loss: 0.0052, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9412, Validation F1: 0.9677\n",
            "Validation loss decreased (0.007835 --> 0.005196).  Saving model ...\n",
            "Epoch [42/47], Training Loss: 0.013092911802232265, Training Accuracy: 0.8795, Training Sensitivity: 0.8936, Training Specificity: 0.8692, Training F1: 0.8615\n",
            "Epoch [42/47], Validation Loss: 0.0036, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9375, Validation F1: 0.9697\n",
            "Validation loss decreased (0.005196 --> 0.003630).  Saving model ...\n",
            "Epoch [43/47], Training Loss: 0.020346783101558685, Training Accuracy: 0.9018, Training Sensitivity: 0.8989, Training Specificity: 0.9037, Training F1: 0.8791\n",
            "Epoch [43/47], Validation Loss: 0.0140, Validation Accuracy: 0.9062, Validation Sensitivity: 0.8000, Validation Specificity: 1.0000, Validation F1: 0.8889\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [44/47], Training Loss: 0.008609858341515064, Training Accuracy: 0.8973, Training Sensitivity: 0.9032, Training Specificity: 0.8931, Training F1: 0.8796\n",
            "Epoch [44/47], Validation Loss: 0.0161, Validation Accuracy: 0.8125, Validation Sensitivity: 0.5714, Validation Specificity: 1.0000, Validation F1: 0.7273\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [45/47], Training Loss: 0.010744169354438782, Training Accuracy: 0.9062, Training Sensitivity: 0.8990, Training Specificity: 0.9120, Training F1: 0.8945\n",
            "Epoch [45/47], Validation Loss: 0.0123, Validation Accuracy: 0.9688, Validation Sensitivity: 0.9231, Validation Specificity: 1.0000, Validation F1: 0.9600\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [46/47], Training Loss: 0.013660885393619537, Training Accuracy: 0.9107, Training Sensitivity: 0.8990, Training Specificity: 0.9200, Training F1: 0.8990\n",
            "Epoch [46/47], Validation Loss: 0.0132, Validation Accuracy: 0.9062, Validation Sensitivity: 0.7857, Validation Specificity: 1.0000, Validation F1: 0.8800\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [47/47], Training Loss: 0.020608507096767426, Training Accuracy: 0.9107, Training Sensitivity: 0.9375, Training Specificity: 0.8906, Training F1: 0.9000\n",
            "Epoch [47/47], Validation Loss: 0.0089, Validation Accuracy: 0.9062, Validation Sensitivity: 0.9286, Validation Specificity: 0.8889, Validation F1: 0.8966\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Fold 4: Accuracy: 0.9062, Sensitivity: 0.9286, Specificity: 0.8889, F1: 0.8966\n",
            "Processing fold 5/5...\n",
            "Epoch [1/47], Training Loss: 0.04766089469194412, Training Accuracy: 0.4821, Training Sensitivity: 0.6957, Training Specificity: 0.3333, Training F1: 0.5246\n",
            "Epoch [1/47], Validation Loss: 0.0305, Validation Accuracy: 0.7188, Validation Sensitivity: 0.8462, Validation Specificity: 0.6316, Validation F1: 0.7097\n",
            "Validation loss decreased (inf --> 0.030529).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.05061211809515953, Training Accuracy: 0.4777, Training Sensitivity: 0.6632, Training Specificity: 0.3411, Training F1: 0.5185\n",
            "Epoch [2/47], Validation Loss: 0.0419, Validation Accuracy: 0.4688, Validation Sensitivity: 1.0000, Validation Specificity: 0.1500, Validation F1: 0.5854\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [3/47], Training Loss: 0.050856418907642365, Training Accuracy: 0.5312, Training Sensitivity: 0.6146, Training Specificity: 0.4688, Training F1: 0.5291\n",
            "Epoch [3/47], Validation Loss: 0.0432, Validation Accuracy: 0.4688, Validation Sensitivity: 0.9231, Validation Specificity: 0.1579, Validation F1: 0.5854\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [4/47], Training Loss: 0.03826718404889107, Training Accuracy: 0.5759, Training Sensitivity: 0.6022, Training Specificity: 0.5573, Training F1: 0.5411\n",
            "Epoch [4/47], Validation Loss: 0.0301, Validation Accuracy: 0.6562, Validation Sensitivity: 1.0000, Validation Specificity: 0.3529, Validation F1: 0.7317\n",
            "Validation loss decreased (0.030529 --> 0.030142).  Saving model ...\n",
            "Epoch [5/47], Training Loss: 0.043104901909828186, Training Accuracy: 0.5804, Training Sensitivity: 0.6875, Training Specificity: 0.5000, Training F1: 0.5841\n",
            "Epoch [5/47], Validation Loss: 0.0332, Validation Accuracy: 0.5938, Validation Sensitivity: 0.9167, Validation Specificity: 0.4000, Validation F1: 0.6286\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [6/47], Training Loss: 0.03897063434123993, Training Accuracy: 0.6071, Training Sensitivity: 0.7474, Training Specificity: 0.5039, Training F1: 0.6174\n",
            "Epoch [6/47], Validation Loss: 0.0331, Validation Accuracy: 0.6250, Validation Sensitivity: 1.0000, Validation Specificity: 0.2500, Validation F1: 0.7273\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [7/47], Training Loss: 0.052497830241918564, Training Accuracy: 0.6205, Training Sensitivity: 0.7128, Training Specificity: 0.5538, Training F1: 0.6119\n",
            "Epoch [7/47], Validation Loss: 0.0435, Validation Accuracy: 0.5000, Validation Sensitivity: 1.0000, Validation Specificity: 0.2727, Validation F1: 0.5556\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [8/47], Training Loss: 0.027694283053278923, Training Accuracy: 0.7009, Training Sensitivity: 0.7300, Training Specificity: 0.6774, Training F1: 0.6854\n",
            "Epoch [8/47], Validation Loss: 0.0325, Validation Accuracy: 0.6875, Validation Sensitivity: 0.9167, Validation Specificity: 0.5500, Validation F1: 0.6875\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [9/47], Training Loss: 0.04052590951323509, Training Accuracy: 0.6339, Training Sensitivity: 0.6170, Training Specificity: 0.6462, Training F1: 0.5859\n",
            "Epoch [9/47], Validation Loss: 0.0297, Validation Accuracy: 0.6875, Validation Sensitivity: 1.0000, Validation Specificity: 0.4737, Validation F1: 0.7222\n",
            "Validation loss decreased (0.030142 --> 0.029671).  Saving model ...\n",
            "Epoch [10/47], Training Loss: 0.03027922287583351, Training Accuracy: 0.6562, Training Sensitivity: 0.5833, Training Specificity: 0.7109, Training F1: 0.5926\n",
            "Epoch [10/47], Validation Loss: 0.0334, Validation Accuracy: 0.7188, Validation Sensitivity: 0.9286, Validation Specificity: 0.5556, Validation F1: 0.7429\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [11/47], Training Loss: 0.03205835446715355, Training Accuracy: 0.6920, Training Sensitivity: 0.7097, Training Specificity: 0.6794, Training F1: 0.6567\n",
            "Epoch [11/47], Validation Loss: 0.0359, Validation Accuracy: 0.7500, Validation Sensitivity: 1.0000, Validation Specificity: 0.6000, Validation F1: 0.7500\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [12/47], Training Loss: 0.032077010720968246, Training Accuracy: 0.7009, Training Sensitivity: 0.6848, Training Specificity: 0.7121, Training F1: 0.6528\n",
            "Epoch [12/47], Validation Loss: 0.0436, Validation Accuracy: 0.7500, Validation Sensitivity: 1.0000, Validation Specificity: 0.6190, Validation F1: 0.7333\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [13/47], Training Loss: 0.023026835173368454, Training Accuracy: 0.7634, Training Sensitivity: 0.7396, Training Specificity: 0.7812, Training F1: 0.7282\n",
            "Epoch [13/47], Validation Loss: 0.0479, Validation Accuracy: 0.6875, Validation Sensitivity: 1.0000, Validation Specificity: 0.4444, Validation F1: 0.7368\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [14/47], Training Loss: 0.017597271129488945, Training Accuracy: 0.7991, Training Sensitivity: 0.8061, Training Specificity: 0.7937, Training F1: 0.7783\n",
            "Epoch [14/47], Validation Loss: 0.0229, Validation Accuracy: 0.8438, Validation Sensitivity: 0.9375, Validation Specificity: 0.7500, Validation F1: 0.8571\n",
            "Validation loss decreased (0.029671 --> 0.022864).  Saving model ...\n",
            "Epoch [15/47], Training Loss: 0.02159576117992401, Training Accuracy: 0.8170, Training Sensitivity: 0.7959, Training Specificity: 0.8333, Training F1: 0.7919\n",
            "Epoch [15/47], Validation Loss: 0.0358, Validation Accuracy: 0.7812, Validation Sensitivity: 0.8571, Validation Specificity: 0.7222, Validation F1: 0.7742\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [16/47], Training Loss: 0.023115700110793114, Training Accuracy: 0.7812, Training Sensitivity: 0.7980, Training Specificity: 0.7680, Training F1: 0.7633\n",
            "Epoch [16/47], Validation Loss: 0.0245, Validation Accuracy: 0.8125, Validation Sensitivity: 0.9167, Validation Specificity: 0.7500, Validation F1: 0.7857\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [17/47], Training Loss: 0.010694591328501701, Training Accuracy: 0.8036, Training Sensitivity: 0.7857, Training Specificity: 0.8175, Training F1: 0.7778\n",
            "Epoch [17/47], Validation Loss: 0.0273, Validation Accuracy: 0.7500, Validation Sensitivity: 0.9231, Validation Specificity: 0.6316, Validation F1: 0.7500\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [18/47], Training Loss: 0.039421238005161285, Training Accuracy: 0.7991, Training Sensitivity: 0.8021, Training Specificity: 0.7969, Training F1: 0.7739\n",
            "Epoch [18/47], Validation Loss: 0.0125, Validation Accuracy: 0.8750, Validation Sensitivity: 1.0000, Validation Specificity: 0.7500, Validation F1: 0.8889\n",
            "Validation loss decreased (0.022864 --> 0.012530).  Saving model ...\n",
            "Epoch [19/47], Training Loss: 0.012588939629495144, Training Accuracy: 0.8482, Training Sensitivity: 0.8602, Training Specificity: 0.8397, Training F1: 0.8247\n",
            "Epoch [19/47], Validation Loss: 0.0261, Validation Accuracy: 0.8125, Validation Sensitivity: 0.9167, Validation Specificity: 0.7500, Validation F1: 0.7857\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [20/47], Training Loss: 0.01157218124717474, Training Accuracy: 0.8438, Training Sensitivity: 0.8333, Training Specificity: 0.8516, Training F1: 0.8205\n",
            "Epoch [20/47], Validation Loss: 0.0248, Validation Accuracy: 0.8438, Validation Sensitivity: 1.0000, Validation Specificity: 0.7500, Validation F1: 0.8276\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [21/47], Training Loss: 0.020045755431056023, Training Accuracy: 0.8571, Training Sensitivity: 0.9032, Training Specificity: 0.8244, Training F1: 0.8400\n",
            "Epoch [21/47], Validation Loss: 0.0337, Validation Accuracy: 0.8125, Validation Sensitivity: 1.0000, Validation Specificity: 0.6471, Validation F1: 0.8333\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [22/47], Training Loss: 0.014603469520807266, Training Accuracy: 0.8259, Training Sensitivity: 0.8421, Training Specificity: 0.8140, Training F1: 0.8040\n",
            "Epoch [22/47], Validation Loss: 0.0361, Validation Accuracy: 0.7500, Validation Sensitivity: 0.9286, Validation Specificity: 0.6111, Validation F1: 0.7647\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [23/47], Training Loss: 0.030673684552311897, Training Accuracy: 0.8482, Training Sensitivity: 0.8352, Training Specificity: 0.8571, Training F1: 0.8172\n",
            "Epoch [23/47], Validation Loss: 0.0310, Validation Accuracy: 0.8125, Validation Sensitivity: 0.9231, Validation Specificity: 0.7368, Validation F1: 0.8000\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [24/47], Training Loss: 0.03635566681623459, Training Accuracy: 0.8438, Training Sensitivity: 0.8478, Training Specificity: 0.8409, Training F1: 0.8168\n",
            "Epoch [24/47], Validation Loss: 0.0194, Validation Accuracy: 0.8438, Validation Sensitivity: 0.9333, Validation Specificity: 0.7647, Validation F1: 0.8485\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [25/47], Training Loss: 0.01165824756026268, Training Accuracy: 0.8571, Training Sensitivity: 0.8660, Training Specificity: 0.8504, Training F1: 0.8400\n",
            "Epoch [25/47], Validation Loss: 0.0236, Validation Accuracy: 0.8438, Validation Sensitivity: 1.0000, Validation Specificity: 0.7500, Validation F1: 0.8276\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [26/47], Training Loss: 0.03333006054162979, Training Accuracy: 0.8839, Training Sensitivity: 0.8969, Training Specificity: 0.8740, Training F1: 0.8700\n",
            "Epoch [26/47], Validation Loss: 0.0160, Validation Accuracy: 0.9062, Validation Sensitivity: 1.0000, Validation Specificity: 0.8125, Validation F1: 0.9143\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [27/47], Training Loss: 0.01651868224143982, Training Accuracy: 0.8750, Training Sensitivity: 0.9121, Training Specificity: 0.8496, Training F1: 0.8557\n",
            "Epoch [27/47], Validation Loss: 0.0197, Validation Accuracy: 0.8125, Validation Sensitivity: 0.7273, Validation Specificity: 0.8571, Validation F1: 0.7273\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [28/47], Training Loss: 0.015267523936927319, Training Accuracy: 0.8929, Training Sensitivity: 0.8854, Training Specificity: 0.8984, Training F1: 0.8763\n",
            "Epoch [28/47], Validation Loss: 0.0199, Validation Accuracy: 0.8750, Validation Sensitivity: 0.9231, Validation Specificity: 0.8421, Validation F1: 0.8571\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 5: Accuracy: 0.8750, Sensitivity: 0.9231, Specificity: 0.8421, F1: 0.8571\n",
            "Mean Accuracy: 0.8094, CI: (0.746875, 0.871875), p-value: 0.0009\n",
            "Mean Sensitivity: 0.8542, CI: (0.7552910052910053, 0.9258852258852259), p-value: 0.0019\n",
            "Mean Specificity: 0.7774, CI: (0.7123123123123123, 0.8479532163742689), p-value: 0.0023\n",
            "Mean F1 Score: 0.7919, CI: (0.7236183852577295, 0.860207991242474), p-value: 0.0017\n",
            "Mean Accuracy: 0.8094, CI: (0.746875, 0.871875), p-value: 0.0009\n",
            "Mean Sensitivity: 0.8542, CI: (0.7552910052910053, 0.9258852258852259), p-value: 0.0019\n",
            "Mean Specificity: 0.7774, CI: (0.7123123123123123, 0.8479532163742689), p-value: 0.0023\n",
            "Mean F1 Score: 0.7919, CI: (0.7236183852577295, 0.860207991242474), p-value: 0.0017\n"
          ]
        }
      ]
    }
  ]
}