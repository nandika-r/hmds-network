{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "SPGbu1AijwNq",
        "outputId": "2507f478-1945-4320-bc5a-01f88e561600"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-geometric --y\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAOV5lvzjypa",
        "outputId": "0f17aad4-757a-4f59-8149-d615349e2477"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch-geometric 2.7.0\n",
            "Uninstalling torch-geometric-2.7.0:\n",
            "  Successfully uninstalled torch-geometric-2.7.0\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-5kfa61l7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-5kfa61l7\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 56d53d03a7326c7882d33345a759df1b02bcb4f2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.10.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.7.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric==2.7.0) (4.12.2)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1136511 sha256=55de5ae379756334835e185caa381ca3e1b777eba52bbcaba0689054a0b346f7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gfm3fumx/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import torch.nn.init as init\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4fHtB0eRj0xx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_part = 128\n",
        "n_graph_out = 16\n",
        "n_rnn = 128\n",
        "n_rnn_out = 15\n",
        "frame_nos = 75"
      ],
      "metadata": {
        "id": "HJJB0J9Pj5NM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error reading JSON file: {file_path}\")\n",
        "        return None\n",
        "\n",
        "def calculate_skip_and_keep(frame_rate, target_fps):\n",
        "    \"\"\"\n",
        "    Calculate the number of frames to skip and the number of frames to keep based on the frame rate and target fps.\n",
        "\n",
        "    Parameters:\n",
        "    frame_rate (int): The original frame rate of the video.\n",
        "    target_fps (int): The target frame rate after downsampling.\n",
        "\n",
        "    Returns:\n",
        "    skip_every_n (int): The number of frames to skip in each block.\n",
        "    keep_every_m (int): The number of frames to keep in each block.\n",
        "    \"\"\"\n",
        "    # Calculate the total frames in a block (skip + keep)\n",
        "    total_frames_in_block = frame_rate\n",
        "\n",
        "    # Calculate the number of frames to keep in each block\n",
        "    keep_every_m = int((target_fps / frame_rate) * total_frames_in_block)\n",
        "\n",
        "    # Calculate the number of frames to skip in each block\n",
        "    skip_every_n = total_frames_in_block - keep_every_m\n",
        "\n",
        "    return skip_every_n, keep_every_m\n",
        "\n",
        "def process_openpose_output(directory, batch_size, frame_rate=25, target_fps=25, skip_every_n=0, keep_every_m=0, ignore_first_n=0):\n",
        "    json_files = sorted([f for f in os.listdir(directory) if f.endswith('.json')])\n",
        "    num_files = len(json_files)\n",
        "\n",
        "    if skip_every_n > 0 and keep_every_m > 0:\n",
        "        step_pattern = [(i % (skip_every_n + keep_every_m) < keep_every_m) for i in range(frame_rate)]\n",
        "    else:\n",
        "        step_pattern = [(i % (frame_rate // target_fps) == 0) for i in range(frame_rate)]\n",
        "\n",
        "    selected_files = [f for i, f in enumerate(json_files[ignore_first_n:]) if step_pattern[i % frame_rate]]\n",
        "\n",
        "    if len(selected_files) < (batch_size - 1) + 1:\n",
        "        raise ValueError(f\"Not enough files in the directory after applying the skip pattern. Found {len(selected_files)} files.\")\n",
        "\n",
        "    batches = [selected_files[i:i + batch_size] for i in range(0, len(selected_files), batch_size)]\n",
        "\n",
        "    if len(batches[-1]) < batch_size:\n",
        "        batches = batches[:-1]\n",
        "\n",
        "    all_batches_data = []\n",
        "    for batch in batches:\n",
        "        batch_data = []\n",
        "        for json_file in batch:\n",
        "            file_path = os.path.join(directory, json_file)\n",
        "            data = read_json_file(file_path)\n",
        "            if data:\n",
        "                batch_data.append(data)\n",
        "        if batch_data:\n",
        "            # Include the directory path with each batch\n",
        "            all_batches_data.append((batch_data, batch, directory))\n",
        "\n",
        "    return all_batches_data\n",
        "\n",
        "frame_rate = 25  # Original FPS of the data\n",
        "target_fps = 15  # Desired FPS\n",
        "\n",
        "# Calculate skip and keep parameters dynamically\n",
        "skip_every_n, keep_every_m = calculate_skip_and_keep(frame_rate, target_fps)\n",
        "\n",
        "batch_size = 75  # Number of frames per batch\n",
        "\n",
        "def flip_keypoints_horizontally(keypoints, image_width=1):\n",
        "    flipped_keypoints = keypoints.copy()\n",
        "    flip_pairs = [\n",
        "        (2, 5), (3, 6), (4, 7), (9, 12), (10, 13), (11, 14), (15, 16), (17, 18), (19, 22), (20, 23), (21, 24)\n",
        "    ]\n",
        "\n",
        "    for i in range(len(keypoints) // 3):\n",
        "        if keypoints[i * 3] >= 0:\n",
        "            flipped_keypoints[i * 3] = image_width - keypoints[i * 3]\n",
        "        else:\n",
        "            flipped_keypoints[i * 3] = keypoints[i * 3]\n",
        "\n",
        "    for (i, j) in flip_pairs:\n",
        "        flipped_keypoints[i * 3], flipped_keypoints[j * 3] = flipped_keypoints[j * 3], flipped_keypoints[i * 3]\n",
        "        flipped_keypoints[i * 3 + 1], flipped_keypoints[j * 3 + 1] = flipped_keypoints[j * 3 + 1], flipped_keypoints[i * 3 + 1]\n",
        "        flipped_keypoints[i * 3 + 2], flipped_keypoints[j * 3 + 2] = flipped_keypoints[j * 3 + 2], flipped_keypoints[i * 3 + 2]\n",
        "\n",
        "    return flipped_keypoints\n",
        "\n",
        "def create_matrix(data, selected_files, augment=False):\n",
        "    num_frames = len(selected_files)\n",
        "    matrix = [[None] * num_frames for _ in range(len(body_parts))]\n",
        "\n",
        "    for frame_number, (frame_data, json_file) in enumerate(zip(data, selected_files)):\n",
        "        if frame_data['people']:\n",
        "            keypoints = frame_data['people'][0]['pose_keypoints_2d']\n",
        "\n",
        "            if augment:\n",
        "                keypoints = flip_keypoints_horizontally(keypoints)\n",
        "\n",
        "            if len(keypoints) == len(body_parts) * 1.5:\n",
        "                for i in range(len(body_parts) // 2):\n",
        "                    x, y, confidence = keypoints[i * 3:(i + 1) * 3]\n",
        "\n",
        "                    if confidence > 0:\n",
        "                        matrix[i * 2][frame_number] = x\n",
        "                        matrix[i * 2 + 1][frame_number] = y\n",
        "                    else:\n",
        "                        matrix[i * 2][frame_number] = -1\n",
        "                        matrix[i * 2 + 1][frame_number] = -1\n",
        "            else:\n",
        "                for i in range(len(body_parts) // 2):\n",
        "                    matrix[i * 2][frame_number] = None\n",
        "                    matrix[i * 2 + 1][frame_number] = None\n",
        "        else:\n",
        "            for i in range(len(body_parts) // 2):\n",
        "                matrix[i * 2][frame_number] = None\n",
        "                matrix[i * 2 + 1][frame_number] = None\n",
        "\n",
        "    column_labels = [f'Frame_{os.path.splitext(f)[0]}' for f in selected_files]\n",
        "    df = pd.DataFrame(matrix, index=row_labels, columns=column_labels)\n",
        "    df.fillna(-1, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "body_parts = [\n",
        "    \"x_Nose\", \"y_Nose\", \"x_Neck\", \"y_Neck\", \"x_RShoulder\", \"y_RShoulder\", \"x_RElbow\", \"y_RElbow\", \"x_RWrist\", \"y_RWrist\",\n",
        "    \"x_LShoulder\", \"y_LShoulder\", \"x_LElbow\", \"y_LElbow\", \"x_LWrist\", \"y_LWrist\", \"x_MidHip\", \"y_MidHip\", \"x_RHip\", \"y_RHip\",\n",
        "    \"x_RKnee\", \"y_RKnee\", \"x_RAnkle\", \"y_RAnkle\", \"x_LHip\", \"y_LHip\", \"x_LKnee\", \"y_LKnee\", \"x_LAnkle\", \"y_LAnkle\",\n",
        "    \"x_REye\", \"y_REye\", \"x_LEye\", \"y_LEye\", \"x_REar\", \"y_REar\", \"x_LEar\", \"y_LEar\", \"x_LBigToe\", \"y_LBigToe\",\n",
        "    \"x_LSmallToe\", \"y_LSmallToe\", \"x_LHeel\", \"y_LHeel\", \"x_RBigToe\", \"y_RBigToe\", \"x_RSmallToe\", \"y_RSmallToe\", \"x_RHeel\", \"y_RHeel\"\n",
        "]\n",
        "\n",
        "row_labels = body_parts\n",
        "\n",
        "directories = [\n",
        "    '/content/drive/My Drive/patient_openpose/00001',\n",
        "    '/content/drive/My Drive/patient_openpose/00002_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00003_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00004',\n",
        "    '/content/drive/My Drive/patient_openpose/00005',\n",
        "    '/content/drive/My Drive/patient_openpose/00006',\n",
        "    '/content/drive/My Drive/patient_openpose/00007',\n",
        "    '/content/drive/My Drive/patient_openpose/00008',\n",
        "    '/content/drive/My Drive/patient_openpose/00009',\n",
        "    '/content/drive/My Drive/patient_openpose/00010_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00011',\n",
        "    '/content/drive/My Drive/patient_openpose/00012',\n",
        "    '/content/drive/My Drive/patient_openpose/00013',\n",
        "    '/content/drive/My Drive/patient_openpose/00014',\n",
        "    '/content/drive/My Drive/patient_openpose/00016',\n",
        "    '/content/drive/My Drive/patient_openpose/00017',\n",
        "    '/content/drive/My Drive/patient_openpose/00018_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00019',\n",
        "    '/content/drive/My Drive/patient_openpose/00020',\n",
        "    '/content/drive/My Drive/patient_openpose/00021',\n",
        "    '/content/drive/My Drive/patient_openpose/00022',\n",
        "    '/content/drive/My Drive/patient_openpose/00024',\n",
        "    '/content/drive/My Drive/patient_openpose/00025',\n",
        "    '/content/drive/My Drive/patient_openpose/00026',\n",
        "    '/content/drive/My Drive/patient_openpose/00027',\n",
        "    '/content/drive/My Drive/patient_openpose/00028_s',\n",
        "    '/content/drive/My Drive/patient_openpose/00029',\n",
        "    '/content/drive/My Drive/patient_openpose/00030',\n",
        "    '/content/drive/My Drive/patient_openpose/00031',\n",
        "    '/content/drive/My Drive/patient_openpose/00032',\n",
        "    '/content/drive/My Drive/patient_openpose/00033',\n",
        "    '/content/drive/My Drive/patient_openpose/00034',\n",
        "    '/content/drive/My Drive/patient_openpose/00035',\n",
        "    '/content/drive/My Drive/patient_openpose/00036',\n",
        "    '/content/drive/My Drive/patient_openpose/00037',\n",
        "    '/content/drive/My Drive/patient_openpose/00038',\n",
        "    '/content/drive/My Drive/patient_openpose/00039',\n",
        "    '/content/drive/My Drive/patient_openpose/00041',\n",
        "    '/content/drive/My Drive/patient_openpose/00042',\n",
        "    '/content/drive/My Drive/patient_openpose/00044',\n",
        "    '/content/drive/My Drive/patient_openpose/00045',\n",
        "    '/content/drive/My Drive/patient_openpose/00046',\n",
        "    '/content/drive/My Drive/patient_openpose/00047',\n",
        "    '/content/drive/My Drive/patient_openpose/00048',\n",
        "    '/content/drive/My Drive/patient_openpose/00049',\n",
        "    '/content/drive/My Drive/patient_openpose/00050',\n",
        "    '/content/drive/My Drive/patient_openpose/00051',\n",
        "    '/content/drive/My Drive/patient_openpose/00052',\n",
        "    '/content/drive/My Drive/patient_openpose/00053',\n",
        "    '/content/drive/My Drive/patient_openpose/00054',\n",
        "]\n",
        "frame_nos = 75\n",
        "target = torch.tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
        "                       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1], dtype=torch.long)\n",
        "\n",
        "original_distribution = Counter(target.tolist())\n",
        "print(\"Original label distribution:\", original_distribution)\n",
        "\n",
        "minority_class_indices = [i for i, label in enumerate(target) if label == 1]\n",
        "majority_class_indices = [i for i, label in enumerate(target) if label == 0]\n",
        "\n",
        "np.random.seed(42)\n",
        "bootstrapped_minority_indices = np.random.choice(minority_class_indices, size=len(majority_class_indices), replace=True)\n",
        "\n",
        "balanced_indices = np.concatenate([majority_class_indices, bootstrapped_minority_indices])\n",
        "\n",
        "directories_balanced = [directories[i] for i in balanced_indices]\n",
        "target_balanced = target[balanced_indices]\n",
        "\n",
        "train_dirs, temp_dirs, train_labels, temp_labels = train_test_split(\n",
        "    directories_balanced, target_balanced, test_size=0.2, random_state=42, stratify=target_balanced\n",
        ")\n",
        "val_dirs, test_dirs, val_labels, test_labels = train_test_split(\n",
        "    temp_dirs, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "train_label_counts = Counter([label.item() for label in train_labels])\n",
        "val_label_counts = Counter([label.item() for label in val_labels])\n",
        "test_label_counts = Counter([label.item() for label in test_labels])\n",
        "\n",
        "print(\"Training label distribution:\", train_label_counts)\n",
        "print(\"Validation label distribution:\", val_label_counts)\n",
        "print(\"Testing label distribution:\", test_label_counts)\n",
        "\n",
        "def process_and_augment_directories_with_labels(directories, labels, frame_rate=25, target_fps=25, batch_size=75):\n",
        "    matrices = []\n",
        "    batch_labels = []\n",
        "    batch_directories = []\n",
        "    skip_every_n, keep_every_m = calculate_skip_and_keep(frame_rate, target_fps)\n",
        "\n",
        "    for directory, label in zip(directories, labels):\n",
        "        try:\n",
        "            batches_data = process_openpose_output(directory, batch_size=batch_size, frame_rate=frame_rate, target_fps=target_fps, skip_every_n=skip_every_n, keep_every_m=keep_every_m)\n",
        "            for batch_data, batch_files, batch_directory in batches_data:\n",
        "                df = create_matrix(batch_data, batch_files)\n",
        "                df_aug = create_matrix(batch_data, batch_files, augment=True)\n",
        "                matrices.append(df)\n",
        "                matrices.append(df_aug)\n",
        "                batch_labels.extend([label, label])\n",
        "                batch_directories.extend([batch_directory, batch_directory])\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping directory {directory} due to insufficient frames: {e}\")\n",
        "            continue\n",
        "    return matrices, batch_labels, batch_directories\n",
        "\n",
        "train_matrices, train_batch_labels, train_batch_dirs = process_and_augment_directories_with_labels(train_dirs, train_labels, frame_rate=frame_rate, target_fps=target_fps, batch_size=batch_size)\n",
        "val_matrices, val_batch_labels, val_batch_dirs = process_and_augment_directories_with_labels(val_dirs, val_labels, frame_rate=frame_rate, target_fps=target_fps, batch_size=batch_size)\n",
        "test_matrices, test_batch_labels, test_batch_dirs = process_and_augment_directories_with_labels(test_dirs, test_labels, frame_rate=frame_rate, target_fps=target_fps, batch_size=batch_size)\n",
        "\n",
        "print(\"Directories for training batches:\", train_batch_dirs)\n",
        "print(\"Directories for validation batches:\", val_batch_dirs)\n",
        "print(\"Directories for test batches:\", test_batch_dirs)\n",
        "\n",
        "train_batch_labels = torch.tensor(train_batch_labels, dtype=torch.long)\n",
        "val_batch_labels = torch.tensor(val_batch_labels, dtype=torch.long)\n",
        "test_batch_labels = torch.tensor(test_batch_labels, dtype=torch.long)\n",
        "train_tensor = np.stack([df.values for df in train_matrices], axis=0)\n",
        "val_tensor = np.stack([df.values for df in val_matrices], axis=0)\n",
        "test_tensor = np.stack([df.values for df in test_matrices], axis=0)\n",
        "\n",
        "print(\"Train tensor shape:\", train_tensor.shape)\n",
        "print(\"Train labels shape:\", train_batch_labels.shape)\n",
        "print(\"Validation tensor shape:\", val_tensor.shape)\n",
        "print(\"Validation labels shape:\", val_batch_labels.shape)\n",
        "print(\"Test tensor shape:\", test_tensor.shape)\n",
        "print(\"Test labels shape:\", test_batch_labels.shape)\n"
      ],
      "metadata": {
        "id": "VkOOiNU7fAJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train, validation, and test tensors\n",
        "train_tensor = np.load('/content/drive/My Drive/patient_openpose/train_tensor.npy')\n",
        "val_tensor = np.load('/content/drive/My Drive/patient_openpose/val_tensor.npy')\n",
        "test_tensor = np.load('/content/drive/My Drive/patient_openpose/test_tensor.npy')\n",
        "\n",
        "# Load the corresponding labels\n",
        "train_labels = np.load('/content/drive/My Drive/patient_openpose/train_labels.npy')\n",
        "val_labels = np.load('/content/drive/My Drive/patient_openpose/val_labels.npy')\n",
        "test_labels = np.load('/content/drive/My Drive/patient_openpose/test_labels.npy')\n",
        "\n",
        "# If you need them as PyTorch tensors, convert them\n",
        "train_tensor = torch.tensor(train_tensor)\n",
        "val_tensor = torch.tensor(val_tensor)\n",
        "test_tensor = torch.tensor(test_tensor)\n",
        "\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Now you can use train_tensor, val_tensor, test_tensor, train_labels, val_labels, test_labels in your code\n",
        "print(\"Train tensor shape:\", train_tensor.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Validation tensor shape:\", val_tensor.shape)\n",
        "print(\"Validation labels shape:\", val_labels.shape)\n",
        "print(\"Test tensor shape:\", test_tensor.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAFzFz2K_31o",
        "outputId": "f193dc99-60de-4a27-e770-6a48a4d50a59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tensor shape: torch.Size([318, 50, 75])\n",
            "Train labels shape: torch.Size([318])\n",
            "Validation tensor shape: torch.Size([32, 50, 75])\n",
            "Validation labels shape: torch.Size([32])\n",
            "Test tensor shape: torch.Size([88, 50, 75])\n",
            "Test labels shape: torch.Size([88])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "left_arm_train_np = np.concatenate((train_tensor[:, 2:4, :], train_tensor[:, 10:16, :]), axis=1)\n",
        "print(left_arm_train_np.shape)\n",
        "right_arm_train_np = np.concatenate((train_tensor[:, 2:4, :], train_tensor[:, 4:10, :]), axis=1)\n",
        "print(right_arm_train_np.shape)\n",
        "left_leg_train_np = np.concatenate((train_tensor[:, 16:18, :], train_tensor[:, 24:30, :], train_tensor[:, 38:44, :]), axis=1)\n",
        "print(left_leg_train_np.shape)\n",
        "right_leg_train_np = np.concatenate((train_tensor[:, 16:24, :], train_tensor[:, 44:50, :]), axis=1)\n",
        "print(right_leg_train_np.shape)\n",
        "torso_train_np = np.concatenate((train_tensor[:, 0:4, :], train_tensor[:, 30:38, :]), axis=1)\n",
        "print(torso_train_np.shape)\n",
        "\n",
        "left_arm_test_np = np.concatenate((test_tensor[:, 2:4, :], test_tensor[:, 10:16, :]), axis=1)\n",
        "print(left_arm_test_np.shape)\n",
        "right_arm_test_np = np.concatenate((test_tensor[:, 2:4, :], test_tensor[:, 4:10, :]), axis=1)\n",
        "print(right_arm_test_np.shape)\n",
        "left_leg_test_np = np.concatenate((test_tensor[:, 16:18, :], test_tensor[:, 24:30, :], test_tensor[:, 38:44, :]), axis=1)\n",
        "print(left_leg_test_np.shape)\n",
        "right_leg_test_np = np.concatenate((test_tensor[:, 16:24, :], test_tensor[:, 44:50, :]), axis=1)\n",
        "print(right_leg_test_np.shape)\n",
        "torso_test_np = np.concatenate((test_tensor[:, 0:4, :], test_tensor[:, 30:38, :]), axis=1)\n",
        "print(torso_test_np.shape)\n",
        "\n",
        "left_arm_val_np = np.concatenate((val_tensor[:, 2:4, :], val_tensor[:, 10:16, :]), axis=1)\n",
        "print(left_arm_val_np.shape)\n",
        "right_arm_val_np = np.concatenate((val_tensor[:, 2:4, :], val_tensor[:, 4:10, :]), axis=1)\n",
        "print(right_arm_val_np.shape)\n",
        "left_leg_val_np = np.concatenate((val_tensor[:, 16:18, :], val_tensor[:, 24:30, :], val_tensor[:, 38:44, :]), axis=1)\n",
        "print(left_leg_val_np.shape)\n",
        "right_leg_val_np = np.concatenate((val_tensor[:, 16:24, :], val_tensor[:, 44:50, :]), axis=1)\n",
        "print(right_leg_val_np.shape)\n",
        "torso_val_np = np.concatenate((val_tensor[:, 0:4, :], val_tensor[:, 30:38, :]), axis=1)\n",
        "print(torso_val_np.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV9hns7Oj8lu",
        "outputId": "6c939433-6aa9-4f8e-b0f9-bf4dea55ec33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(318, 8, 75)\n",
            "(318, 8, 75)\n",
            "(318, 14, 75)\n",
            "(318, 14, 75)\n",
            "(318, 12, 75)\n",
            "(88, 8, 75)\n",
            "(88, 8, 75)\n",
            "(88, 14, 75)\n",
            "(88, 14, 75)\n",
            "(88, 12, 75)\n",
            "(32, 8, 75)\n",
            "(32, 8, 75)\n",
            "(32, 14, 75)\n",
            "(32, 14, 75)\n",
            "(32, 12, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "left_arm_train_tensor = torch.from_numpy(left_arm_train_np).float()\n",
        "right_arm_train_tensor = torch.from_numpy(right_arm_train_np).float()\n",
        "left_leg_train_tensor = torch.from_numpy(left_leg_train_np).float()\n",
        "right_leg_train_tensor = torch.from_numpy(right_leg_train_np).float()\n",
        "torso_train_tensor = torch.from_numpy(torso_train_np).float()\n",
        "\n",
        "left_arm_test_tensor = torch.from_numpy(left_arm_test_np).float()\n",
        "right_arm_test_tensor = torch.from_numpy(right_arm_test_np).float()\n",
        "left_leg_test_tensor = torch.from_numpy(left_leg_test_np).float()\n",
        "right_leg_test_tensor = torch.from_numpy(right_leg_test_np).float()\n",
        "torso_test_tensor = torch.from_numpy(torso_test_np).float()\n",
        "\n",
        "left_arm_val_tensor = torch.from_numpy(left_arm_val_np).float()\n",
        "right_arm_val_tensor = torch.from_numpy(right_arm_val_np).float()\n",
        "left_leg_val_tensor = torch.from_numpy(left_leg_val_np).float()\n",
        "right_leg_val_tensor = torch.from_numpy(right_leg_val_np).float()\n",
        "torso_val_tensor = torch.from_numpy(torso_val_np).float()"
      ],
      "metadata": {
        "id": "3cqXIeCxFf3M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_sequence_train = np.concatenate((train_tensor[:, 2:4, :], right_leg_train_np, train_tensor[:, 20:22, :], train_tensor[:, 18:20, :], left_leg_train_np, train_tensor[:, 26:28, :], train_tensor[:, 24:26, :], train_tensor[:, 16:18, :], torso_train_np, right_arm_train_np, train_tensor[:, 6:8, :], train_tensor[:, 4:6, :], left_arm_train_np, train_tensor[:, 12:14, :], train_tensor[:, 10:12, :], train_tensor[:, 2:4, :]), axis=1)\n",
        "rnn_sequence_train = np.array(rnn_sequence_train, dtype=float)\n",
        "rnn_sequence_train_tensor = torch.from_numpy(rnn_sequence_train).float()\n",
        "reshaped_rnn_train_tensor = rnn_sequence_train_tensor.permute(0, 2, 1)\n",
        "print(reshaped_rnn_train_tensor.shape)\n",
        "\n",
        "rnn_sequence_test = np.concatenate((test_tensor[:, 2:4, :], right_leg_test_np, test_tensor[:, 20:22, :], test_tensor[:, 18:20, :], left_leg_test_np, test_tensor[:, 26:28, :], test_tensor[:, 24:26, :], test_tensor[:, 16:18, :], torso_test_np, right_arm_test_np, test_tensor[:, 6:8, :], test_tensor[:, 4:6, :], left_arm_test_np, test_tensor[:, 12:14, :], test_tensor[:, 10:12, :], test_tensor[:, 2:4, :]), axis=1)\n",
        "rnn_sequence_test = np.array(rnn_sequence_test, dtype=float)\n",
        "rnn_sequence_test_tensor = torch.from_numpy(rnn_sequence_test).float()\n",
        "reshaped_rnn_test_tensor = rnn_sequence_test_tensor.permute(0, 2, 1)\n",
        "print(reshaped_rnn_test_tensor.shape)\n",
        "\n",
        "rnn_sequence_val = np.concatenate((val_tensor[:, 2:4, :], right_leg_val_np, val_tensor[:, 20:22, :], val_tensor[:, 18:20, :], left_leg_val_np, val_tensor[:, 26:28, :], val_tensor[:, 24:26, :], val_tensor[:, 16:18, :], torso_val_np, right_arm_val_np, val_tensor[:, 6:8, :], val_tensor[:, 4:6, :], left_arm_val_np, val_tensor[:, 12:14, :], val_tensor[:, 10:12, :], val_tensor[:, 2:4, :]), axis=1)\n",
        "rnn_sequence_val = np.array(rnn_sequence_val, dtype=float)\n",
        "rnn_sequence_val_tensor = torch.from_numpy(rnn_sequence_val).float()\n",
        "reshaped_rnn_val_tensor = rnn_sequence_val_tensor.permute(0, 2, 1)\n",
        "print(reshaped_rnn_val_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmqT7tjEv4Mt",
        "outputId": "88235a9d-be10-4733-b16a-cf88936084bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([318, 75, 78])\n",
            "torch.Size([88, 75, 78])\n",
            "torch.Size([32, 75, 78])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, num_parts, input_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_parts = num_parts\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "        self.softmax = nn.Softmax(dim=2)  # Apply softmax over the parts\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape [batch_size, num_parts, input_dim] (e.g., [318, 5, 128])\n",
        "        queries = self.query(x)  # Shape: [batch_size, num_parts, input_dim]\n",
        "        keys = self.key(x)        # Shape: [batch_size, num_parts, input_dim]\n",
        "        values = self.value(x)    # Shape: [batch_size, num_parts, input_dim]\n",
        "\n",
        "        # Compute attention scores across the body parts\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.num_parts ** 0.5)  # Shape: [batch_size, num_parts, num_parts]\n",
        "        attention_weights = self.softmax(scores)  # Shape: [batch_size, num_parts, num_parts]\n",
        "\n",
        "        # Compute weighted sum of values based on attention weights\n",
        "        weighted_values = torch.bmm(attention_weights, values)  # Shape: [batch_size, num_parts, input_dim]\n",
        "\n",
        "        return weighted_values, scores, attention_weights\n"
      ],
      "metadata": {
        "id": "zI6jPNOHkEkk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes = 5\n",
        "edges = []\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i != j:\n",
        "            edges.append([i, j])\n",
        "\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "print(edge_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gUCGWbPkHbH",
        "outputId": "92990e73-6b69-4cd4-f462-7208e1d98b76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4],\n",
            "        [1, 2, 3, 4, 0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4, 0, 1, 2, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, leaky_relu_alpha):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.bn = nn.BatchNorm1d(output_dim)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_alpha)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        if input_dim != output_dim:\n",
        "            self.residual_connection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.residual_connection = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.fc(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        if self.residual_connection is not None:\n",
        "            residual = self.residual_connection(residual)\n",
        "\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "class SpatialNetwork(nn.Module):\n",
        "    def __init__(self, n_part, n_graph_out, frame_nos, leaky_relu_alpha):\n",
        "        super(SpatialNetwork, self).__init__()\n",
        "\n",
        "        self.n_part = n_part\n",
        "        self.n_graph_out = n_graph_out\n",
        "        self.frame_nos = frame_nos\n",
        "\n",
        "        self.body_parts_fc = nn.ModuleDict({\n",
        "            'arms': nn.Linear(8, n_part),\n",
        "            'legs': nn.Linear(14, n_part),\n",
        "            'torso': nn.Linear(12, n_part),\n",
        "        })\n",
        "\n",
        "        self.self_attn = SelfAttention(num_parts=5, input_dim=n_part)\n",
        "        self.gcn1 = GCNConv(in_channels=n_part, out_channels=n_graph_out, aggr='max', bias=True)\n",
        "        self.bn_gcn1 = nn.BatchNorm1d(n_graph_out)\n",
        "        self.fc6 = ResidualBlock(n_graph_out * frame_nos, 512, leaky_relu_alpha)\n",
        "        self.fc7 = ResidualBlock(512, 256, leaky_relu_alpha)\n",
        "\n",
        "        self.training_attention_scores = []  # Store only the last epoch's training scores\n",
        "        self.training_attention_weights = []\n",
        "\n",
        "    def reset_attention_scores(self):\n",
        "        self.training_attention_scores = []\n",
        "        self.training_attention_weights = []\n",
        "\n",
        "    def forward_body_part(self, x, part):\n",
        "        x = self.body_parts_fc[part](x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x\n",
        "\n",
        "    def forward(self, left_arm, right_arm, left_leg, right_leg, torso, edge_index, is_training=True):\n",
        "        batch_size, features, seq_len = left_arm.shape\n",
        "\n",
        "        outputs = torch.zeros((batch_size, seq_len, self.n_graph_out), device=left_arm.device)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            left_arm_out = self.forward_body_part(left_arm[:, :, t], 'arms')\n",
        "            right_arm_out = self.forward_body_part(right_arm[:, :, t], 'arms')\n",
        "            left_leg_out = self.forward_body_part(left_leg[:, :, t], 'legs')\n",
        "            right_leg_out = self.forward_body_part(right_leg[:, :, t], 'legs')\n",
        "            torso_out = self.forward_body_part(torso[:, :, t], 'torso')\n",
        "\n",
        "            concatenated = torch.stack((left_arm_out, right_arm_out, left_leg_out, right_leg_out, torso_out), dim=1)\n",
        "\n",
        "            attn_output, attn_scores, attn_weights = self.self_attn(concatenated)\n",
        "\n",
        "            if is_training:\n",
        "                # Only accumulate attention scores during training\n",
        "                self.training_attention_scores.append(attn_scores)\n",
        "                self.training_attention_weights.append(attn_weights)\n",
        "\n",
        "            batch_size, num_nodes, feature_dim = attn_output.shape\n",
        "            attn_output = attn_output.reshape(batch_size * num_nodes, feature_dim)\n",
        "\n",
        "            gcn_output = self.gcn1(attn_output, edge_index)\n",
        "            gcn_output = self.bn_gcn1(gcn_output)\n",
        "            gcn_output = F.leaky_relu(gcn_output)\n",
        "            gcn_output = F.dropout(gcn_output, p=0.5, training=self.training)\n",
        "            gcn_output = gcn_output.view(batch_size, num_nodes, -1)\n",
        "            pooled_output = torch.max(gcn_output, dim=1)[0]\n",
        "\n",
        "            outputs[:, t, :] = pooled_output\n",
        "\n",
        "        flattened_outputs = outputs.view(batch_size, -1)\n",
        "        fc6_output = self.fc6(flattened_outputs)\n",
        "        fc7_output = self.fc7(fc6_output)\n",
        "\n",
        "        return fc7_output\n",
        "\n",
        "    def get_last_epoch_attention_scores(self):\n",
        "        # Concatenate the stored attention scores from the last epoch's training phase\n",
        "        if not self.training_attention_scores:\n",
        "            raise ValueError(\"No training attention scores stored. Ensure that the model was trained and attention scores were captured.\")\n",
        "\n",
        "        attention_scores = torch.cat(self.training_attention_scores, dim=0)\n",
        "\n",
        "        # Expecting training batch_size = 318 and seq_len = 75\n",
        "        return attention_scores.view(-1, 75, 5, 5).cpu().detach().numpy()\n",
        "\n",
        "    def get_last_epoch_attention_weights(self):\n",
        "        # Concatenate the stored attention weights from the last epoch's training phase\n",
        "        if not self.training_attention_weights:\n",
        "            raise ValueError(\"No training attention weights stored. Ensure that the model was trained and attention scores were captured.\")\n",
        "\n",
        "        attention_weights = torch.cat(self.training_attention_weights, dim=0)\n",
        "\n",
        "        return attention_weights.view(-1, 75, 5, 5).cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "cz5qrR_SkKBF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_prob, leaky_relu_alpha):\n",
        "        super(TemporalNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout_prob)\n",
        "\n",
        "        self.self_attn = SelfAttention(num_parts=75, input_dim=hidden_size * 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, output_size * input_size)\n",
        "        self.bn1 = nn.BatchNorm1d(output_size * input_size)\n",
        "        self.fc2 = nn.Linear(output_size * input_size, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = ResidualBlock(512, 256, leaky_relu_alpha)  # Assuming ResidualBlock is defined elsewhere\n",
        "\n",
        "        self.stored_attention_scores = []  # Store attention scores\n",
        "        self.stored_attention_weights = []  # Store attention weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Get the output, attention scores, and weights from the SelfAttention module\n",
        "        out, scores, weights = self.self_attn(out)\n",
        "\n",
        "        # Store the scores and weights\n",
        "        self.stored_attention_scores.append(scores)\n",
        "        self.stored_attention_weights.append(weights)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc1(out[:, -1, :])  # Use the last time step's output\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = self.bn1(out)\n",
        "        out = F.leaky_relu(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def clear_attention_weights(self):\n",
        "        self.stored_attention_scores = []\n",
        "        self.stored_attention_weights = []\n",
        "\n",
        "    def get_attention_scores(self):\n",
        "        return self.stored_attention_scores\n",
        "\n",
        "    def get_attention_weights(self):\n",
        "        return self.stored_attention_weights\n"
      ],
      "metadata": {
        "id": "6RLKDlyTwEDH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads=4):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert input_dim % num_heads == 0, \"input_dim must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = input_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv_proj = nn.Linear(input_dim, input_dim * 3)\n",
        "        self.fc_out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, input_dim = x.shape\n",
        "\n",
        "        qkv = self.qkv_proj(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        attn_out = (attn_probs @ v).permute(0, 2, 1, 3).reshape(batch_size, seq_length, input_dim)\n",
        "        return self.fc_out(attn_out)\n",
        "\n",
        "class FinalConcatenation(nn.Module):\n",
        "    def __init__(self, input_dim=512, output_dim=2, dropout_prob=0.5, num_heads=4):\n",
        "        super(FinalConcatenation, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(input_dim=input_dim, num_heads=num_heads)\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(256, output_dim)\n",
        "\n",
        "        self.layer_norm = LayerNorm(input_dim)\n",
        "        self.residual_fc = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # [batch_size, seq_length, input_dim] -> [batch_size, 1, input_dim]\n",
        "\n",
        "        res = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.attention(x)\n",
        "        x += self.residual_fc(res)\n",
        "\n",
        "        x = x.squeeze(1)  # [batch_size, input_dim]\n",
        "\n",
        "        x = self.fc1(x)  # [batch_size, 256]\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)  # [batch_size, output_dim]\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "f-7nNZwGxbjv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            if param.dim() >= 2:\n",
        "                nn.init.kaiming_normal_(param.data, nonlinearity='leaky_relu')\n",
        "            else:\n",
        "                param.data.fill_(1)\n",
        "        elif 'bias' in name:\n",
        "            param.data.fill_(0)"
      ],
      "metadata": {
        "id": "-KH5es_LwLQD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "#got this from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py"
      ],
      "metadata": {
        "id": "0bATk0zxokFp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_initialize_weights(model, gcn_constant=0.0001):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, GCNConv):\n",
        "            init.constant_(module.lin.weight, gcn_constant)\n",
        "            if module.lin.bias is not None:\n",
        "                init.constant_(module.lin.bias, 0)\n",
        "        elif isinstance(module, SelfAttention):\n",
        "            init.kaiming_normal_(module.query.weight, nonlinearity='leaky_relu')\n",
        "            init.kaiming_normal_(module.key.weight, nonlinearity='leaky_relu')\n",
        "            init.kaiming_normal_(module.value.weight, nonlinearity='leaky_relu')\n",
        "            if module.query.bias is not None:\n",
        "                init.constant_(module.query.bias, 0)\n",
        "            if module.key.bias is not None:\n",
        "                init.constant_(module.key.bias, 0)\n",
        "            if module.value.bias is not None:\n",
        "                init.constant_(module.value.bias, 0)\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            init.kaiming_normal_(module.weight, nonlinearity='leaky_relu')\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.BatchNorm1d):\n",
        "            init.constant_(module.weight, 1)\n",
        "            init.constant_(module.bias, 0)"
      ],
      "metadata": {
        "id": "_lK3KYqa8cby"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leaky_relu_alpha = 0.01\n",
        "input_size = 78\n",
        "hidden_size = n_rnn\n",
        "output_size = n_rnn_out\n",
        "num_layers = 4\n",
        "dropout_prob = 0.5"
      ],
      "metadata": {
        "id": "FBbhshTA7Pok"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.1\n",
        "gamma=0\n",
        "learning_rate=0.0001\n",
        "weight_decay=0.001"
      ],
      "metadata": {
        "id": "VIkVO3HX49N8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=2, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        log_probs = nn.functional.log_softmax(inputs, dim=1)\n",
        "        probs = torch.exp(log_probs)\n",
        "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=log_probs.size(1)).float()\n",
        "\n",
        "        focal_weight = torch.pow(1 - probs, self.gamma)\n",
        "        focal_loss = -self.alpha * focal_weight * targets_one_hot * log_probs\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            focal_loss = focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            focal_loss = focal_loss.sum()\n",
        "\n",
        "        return focal_loss\n"
      ],
      "metadata": {
        "id": "-bPrgM9cXA3g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_val_significance_test(dataset, labels, n_splits=5, batch_size=32, baseline=None, **train_params):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    performance_metrics = []\n",
        "\n",
        "    for train_index, val_index in skf.split(labels, labels):\n",
        "        train_loader = create_data_loader(dataset, train_index, batch_size)\n",
        "        val_loader = create_data_loader(dataset, val_index, batch_size)\n",
        "\n",
        "        # Train and evaluate the model on this fold\n",
        "        _, accuracy, _, _ = train_model(**train_params, train_loader=train_loader, val_loader=val_loader)\n",
        "        performance_metrics.append(accuracy)\n",
        "\n",
        "    performance_metrics = np.array(performance_metrics)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    mean_accuracy = np.mean(performance_metrics)\n",
        "    ci_lower, ci_upper = bootstrap_confidence_interval(performance_metrics)\n",
        "\n",
        "    if baseline is not None:\n",
        "        t_stat, p_value = ttest_1samp(performance_metrics, baseline)\n",
        "        print(f\"Mean Performance: {mean_accuracy}, 95% CI: ({ci_lower}, {ci_upper}), Baseline: {baseline}, p-value: {p_value}\")\n",
        "        return mean_accuracy, ci_lower, ci_upper, p_value\n",
        "    else:\n",
        "        print(f\"Mean Performance: {mean_accuracy}, 95% CI: ({ci_lower}, {ci_upper})\")\n",
        "        return mean_accuracy, ci_lower, ci_upper\n"
      ],
      "metadata": {
        "id": "sucwF63kkmCI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import ttest_1samp\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "\n",
        "class_names = ['Class 0', 'Class 1']\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def bootstrap_confidence_interval(data, num_bootstrap_samples=1000, confidence_level=0.95):\n",
        "    n = len(data)\n",
        "    bootstrap_samples = np.random.choice(data, (num_bootstrap_samples, n), replace=True)\n",
        "    bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
        "    lower_bound = np.percentile(bootstrap_means, (1-confidence_level)/2 * 100)\n",
        "    upper_bound = np.percentile(bootstrap_means, (1 + confidence_level)/2 * 100)\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "def create_data_loader(dataset, indices, batch_size):\n",
        "    subset = Subset(dataset, indices)\n",
        "    return DataLoader(subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "def calculate_metrics(all_labels, all_preds):\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "    # Manually calculate F1 score\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "    recall = sensitivity  # recall is the same as sensitivity\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    return accuracy, sensitivity, specificity, f1\n",
        "\n",
        "def train_model(alpha, gamma, learning_rate, weight_decay, train_loader, val_loader, edge_index, class_names,\n",
        "                spatial_net, temporal_net, final_model):\n",
        "    def set_seeds(seed=42):\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    spatial_net = SpatialNetwork(n_part, n_graph_out, frame_nos, leaky_relu_alpha)\n",
        "    custom_initialize_weights(spatial_net)\n",
        "\n",
        "    temporal_net = TemporalNetwork(input_size, hidden_size, output_size, num_layers, dropout_prob, leaky_relu_alpha)\n",
        "    initialize_weights(temporal_net)\n",
        "\n",
        "    final_model = FinalConcatenation()\n",
        "    initialize_weights(final_model)\n",
        "\n",
        "    optimizer = optim.Adam(list(spatial_net.parameters()) + list(temporal_net.parameters()) + list(final_model.parameters()),\n",
        "                           lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.001, step_size_up=2000, mode='triangular2')\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "    num_epochs = 47\n",
        "    criterion = FocalLoss(alpha=alpha, gamma=gamma)\n",
        "\n",
        "    # Store metrics\n",
        "    train_metrics = {'accuracy': [], 'sensitivity': [], 'specificity': [], 'f1': []}\n",
        "    val_metrics = {'accuracy': [], 'sensitivity': [], 'specificity': [], 'f1': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        temporal_net.clear_attention_weights()\n",
        "\n",
        "        spatial_net.train()\n",
        "        temporal_net.train()\n",
        "        final_model.train()\n",
        "\n",
        "        all_train_preds = []\n",
        "        all_train_labels = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            left_arm_train_tensor, right_arm_train_tensor, left_leg_train_tensor, right_leg_train_tensor, torso_train_tensor, reshaped_rnn_train_tensor, train_labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output_spatial_train = spatial_net(left_arm_train_tensor, right_arm_train_tensor, left_leg_train_tensor,\n",
        "                                               right_leg_train_tensor, torso_train_tensor, edge_index)\n",
        "            output_temporal_train = temporal_net(reshaped_rnn_train_tensor)\n",
        "            concatenated_input_train = torch.cat((output_spatial_train, output_temporal_train), dim=1)\n",
        "            final_output_train = final_model(concatenated_input_train)\n",
        "\n",
        "            loss = criterion(final_output_train, train_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            _, train_preds = torch.max(final_output_train, 1)\n",
        "            all_train_preds.extend(train_preds.cpu().numpy())\n",
        "            all_train_labels.extend(train_labels.cpu().numpy())\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(spatial_net.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(temporal_net.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Calculate metrics for training\n",
        "        train_accuracy, train_sensitivity, train_specificity, train_f1 = calculate_metrics(all_train_labels, all_train_preds)\n",
        "        train_metrics['accuracy'].append(train_accuracy)\n",
        "        train_metrics['sensitivity'].append(train_sensitivity)\n",
        "        train_metrics['specificity'].append(train_specificity)\n",
        "        train_metrics['f1'].append(train_f1)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}, Training Accuracy: {train_accuracy:.4f}, Training Sensitivity: {train_sensitivity:.4f}, Training Specificity: {train_specificity:.4f}, Training F1: {train_f1:.4f}\")\n",
        "\n",
        "        spatial_net.eval()\n",
        "        temporal_net.eval()\n",
        "        final_model.eval()\n",
        "\n",
        "        all_val_preds = []\n",
        "        all_val_labels = []\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                left_arm_val_tensor, right_arm_val_tensor, left_leg_val_tensor, right_leg_val_tensor, torso_val_tensor, reshaped_rnn_val_tensor, val_labels = batch\n",
        "\n",
        "                output_spatial_val = spatial_net(left_arm_val_tensor, right_arm_val_tensor, left_leg_val_tensor,\n",
        "                                                 right_leg_val_tensor, torso_val_tensor, edge_index)\n",
        "                output_temporal_val = temporal_net(reshaped_rnn_val_tensor)\n",
        "                concatenated_input_val = torch.cat((output_spatial_val, output_temporal_val), dim=1)\n",
        "                final_output_val = final_model(concatenated_input_val)\n",
        "\n",
        "                val_loss += criterion(final_output_val, val_labels).item()\n",
        "\n",
        "                _, val_preds = torch.max(final_output_val, 1)\n",
        "                all_val_preds.extend(val_preds.cpu().numpy())\n",
        "                all_val_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics for validation\n",
        "        val_accuracy, val_sensitivity, val_specificity, val_f1 = calculate_metrics(all_val_labels, all_val_preds)\n",
        "        val_metrics['accuracy'].append(val_accuracy)\n",
        "        val_metrics['sensitivity'].append(val_sensitivity)\n",
        "        val_metrics['specificity'].append(val_specificity)\n",
        "        val_metrics['f1'].append(val_f1)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Sensitivity: {val_sensitivity:.4f}, Validation Specificity: {val_specificity:.4f}, Validation F1: {val_f1:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "        early_stopping(val_loss, final_model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # Return all five values: validation loss, accuracy, sensitivity, specificity, and F1\n",
        "    final_val_accuracy = val_metrics['accuracy'][-1]\n",
        "    final_val_sensitivity = val_metrics['sensitivity'][-1]\n",
        "    final_val_specificity = val_metrics['specificity'][-1]\n",
        "    final_val_f1 = val_metrics['f1'][-1]\n",
        "\n",
        "    return val_loss, final_val_accuracy, final_val_sensitivity, final_val_specificity, final_val_f1\n",
        "\n",
        "def cross_val_significance_test(dataset, labels, n_splits, batch_size, alpha, gamma, learning_rate, weight_decay,\n",
        "                                edge_index, class_names, spatial_net, temporal_net, final_model, baseline=None):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    accuracies, sensitivities, specificities, f1_scores = [], [], [], []\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "\n",
        "        train_loader = create_data_loader(dataset, train_index, batch_size)\n",
        "        test_loader = create_data_loader(dataset, test_index, batch_size)\n",
        "\n",
        "        val_loss, accuracy, sensitivity, specificity, f1 = train_model(\n",
        "            alpha=alpha,\n",
        "            gamma=gamma,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=test_loader,\n",
        "            edge_index=edge_index,\n",
        "            class_names=class_names,\n",
        "            spatial_net=spatial_net,\n",
        "            temporal_net=temporal_net,\n",
        "            final_model=final_model\n",
        "        )\n",
        "        accuracies.append(accuracy)\n",
        "        sensitivities.append(sensitivity)\n",
        "        specificities.append(specificity)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold {fold + 1}: Accuracy: {accuracy:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # Calculate mean values for each metric directly from stored values\n",
        "    mean_accuracy = np.mean(accuracies)\n",
        "    mean_sensitivity = np.mean(sensitivities)\n",
        "    mean_specificity = np.mean(specificities)\n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "\n",
        "    # Calculate confidence intervals for each metric\n",
        "    ci_accuracy = bootstrap_confidence_interval(accuracies)\n",
        "    ci_sensitivity = bootstrap_confidence_interval(sensitivities)\n",
        "    ci_specificity = bootstrap_confidence_interval(specificities)\n",
        "    ci_f1 = bootstrap_confidence_interval(f1_scores)\n",
        "\n",
        "    # Calculate p-values if baseline is provided\n",
        "    if baseline is not None:\n",
        "        p_accuracy = ttest_1samp(accuracies, baseline).pvalue\n",
        "        p_sensitivity = ttest_1samp(sensitivities, baseline).pvalue\n",
        "        p_specificity = ttest_1samp(specificities, baseline).pvalue\n",
        "        p_f1 = ttest_1samp(f1_scores, baseline).pvalue\n",
        "    else:\n",
        "        p_accuracy = p_sensitivity = p_specificity = p_f1 = None\n",
        "\n",
        "    # Print results with the specified format\n",
        "    print(f\"Mean Accuracy: {mean_accuracy:.4f}, CI: {ci_accuracy}, p-value: {p_accuracy:.4f}\" if p_accuracy is not None else f\"Mean Accuracy: {mean_accuracy:.4f}, CI: {ci_accuracy}, p-value: None\")\n",
        "    print(f\"Mean Sensitivity: {mean_sensitivity:.4f}, CI: {ci_sensitivity}, p-value: {p_sensitivity:.4f}\" if p_sensitivity is not None else f\"Mean Sensitivity: {mean_sensitivity:.4f}, CI: {ci_sensitivity}, p-value: None\")\n",
        "    print(f\"Mean Specificity: {mean_specificity:.4f}, CI: {ci_specificity}, p-value: {p_specificity:.4f}\" if p_specificity is not None else f\"Mean Specificity: {mean_specificity:.4f}, CI: {ci_specificity}, p-value: None\")\n",
        "    print(f\"Mean F1 Score: {mean_f1:.4f}, CI: {ci_f1}, p-value: {p_f1:.4f}\" if p_f1 is not None else f\"Mean F1 Score: {mean_f1:.4f}, CI: {ci_f1}, p-value: None\")\n",
        "\n",
        "    return (mean_accuracy, ci_accuracy, p_accuracy,\n",
        "            mean_sensitivity, ci_sensitivity, p_sensitivity,\n",
        "            mean_specificity, ci_specificity, p_specificity,\n",
        "            mean_f1, ci_f1, p_f1)\n",
        "\n",
        "# Train the models globally\n",
        "spatial_net = SpatialNetwork(n_part, n_graph_out, frame_nos, leaky_relu_alpha)\n",
        "temporal_net = TemporalNetwork(input_size, hidden_size, output_size, num_layers, dropout_prob, leaky_relu_alpha)\n",
        "final_model = FinalConcatenation()\n",
        "\n",
        "# Initialize the models\n",
        "custom_initialize_weights(spatial_net)\n",
        "initialize_weights(temporal_net)\n",
        "initialize_weights(final_model)\n",
        "\n",
        "# Example usage\n",
        "dataset = TensorDataset(left_arm_train_tensor, right_arm_train_tensor, left_leg_train_tensor,\n",
        "                        right_leg_train_tensor, torso_train_tensor, reshaped_rnn_train_tensor, train_labels)\n",
        "\n",
        "mean_accuracy, ci_accuracy, p_accuracy, mean_sensitivity, ci_sensitivity, p_sensitivity, mean_specificity, ci_specificity, p_specificity, mean_f1, ci_f1, p_f1 = cross_val_significance_test(\n",
        "    dataset=dataset,\n",
        "    labels=train_labels,\n",
        "    n_splits=5,\n",
        "    batch_size=32,\n",
        "    baseline=0.5,\n",
        "    alpha=alpha,\n",
        "    gamma=gamma,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    edge_index=edge_index,\n",
        "    class_names=class_names,\n",
        "    spatial_net=spatial_net,\n",
        "    temporal_net=temporal_net,\n",
        "    final_model=final_model\n",
        ")\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy:.4f}, CI: {ci_accuracy}, p-value: {p_accuracy:.4f}\")\n",
        "print(f\"Mean Sensitivity: {mean_sensitivity:.4f}, CI: {ci_sensitivity}, p-value: {p_sensitivity:.4f}\")\n",
        "print(f\"Mean Specificity: {mean_specificity:.4f}, CI: {ci_specificity}, p-value: {p_specificity:.4f}\")\n",
        "print(f\"Mean F1 Score: {mean_f1:.4f}, CI: {ci_f1}, p-value: {p_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1KdwLZDk30N",
        "outputId": "41fdf0c2-7347-4a41-e8c0-88bfda7a86d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold 1/5...\n",
            "Epoch [1/47], Training Loss: 0.035442691296339035, Training Accuracy: 0.5000, Training Sensitivity: 0.7660, Training Specificity: 0.3077, Training F1: 0.5625\n",
            "Epoch [1/47], Validation Loss: 0.0826, Validation Accuracy: 0.4219, Validation Sensitivity: 0.8571, Validation Specificity: 0.0833, Validation F1: 0.5647\n",
            "Validation loss decreased (inf --> 0.082588).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.056636422872543335, Training Accuracy: 0.4330, Training Sensitivity: 0.6316, Training Specificity: 0.2868, Training F1: 0.4858\n",
            "Epoch [2/47], Validation Loss: 0.0783, Validation Accuracy: 0.4375, Validation Sensitivity: 0.9286, Validation Specificity: 0.0556, Validation F1: 0.5909\n",
            "Validation loss decreased (0.082588 --> 0.078319).  Saving model ...\n",
            "Epoch [3/47], Training Loss: 0.041823841631412506, Training Accuracy: 0.4955, Training Sensitivity: 0.6632, Training Specificity: 0.3721, Training F1: 0.5272\n",
            "Epoch [3/47], Validation Loss: 0.0788, Validation Accuracy: 0.4688, Validation Sensitivity: 0.7500, Validation Specificity: 0.2500, Validation F1: 0.5526\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [4/47], Training Loss: 0.061370447278022766, Training Accuracy: 0.4554, Training Sensitivity: 0.5319, Training Specificity: 0.4000, Training F1: 0.4505\n",
            "Epoch [4/47], Validation Loss: 0.0726, Validation Accuracy: 0.5781, Validation Sensitivity: 0.1429, Validation Specificity: 0.9167, Validation F1: 0.2286\n",
            "Validation loss decreased (0.078319 --> 0.072589).  Saving model ...\n",
            "Epoch [5/47], Training Loss: 0.0578276552259922, Training Accuracy: 0.5804, Training Sensitivity: 0.7449, Training Specificity: 0.4524, Training F1: 0.6083\n",
            "Epoch [5/47], Validation Loss: 0.0699, Validation Accuracy: 0.5312, Validation Sensitivity: 0.1429, Validation Specificity: 0.8333, Validation F1: 0.2105\n",
            "Validation loss decreased (0.072589 --> 0.069854).  Saving model ...\n",
            "Epoch [6/47], Training Loss: 0.04365086555480957, Training Accuracy: 0.5625, Training Sensitivity: 0.7041, Training Specificity: 0.4524, Training F1: 0.5847\n",
            "Epoch [6/47], Validation Loss: 0.0684, Validation Accuracy: 0.5781, Validation Sensitivity: 0.1429, Validation Specificity: 0.9167, Validation F1: 0.2286\n",
            "Validation loss decreased (0.069854 --> 0.068430).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.0473637655377388, Training Accuracy: 0.5536, Training Sensitivity: 0.6452, Training Specificity: 0.4885, Training F1: 0.5455\n",
            "Epoch [7/47], Validation Loss: 0.0714, Validation Accuracy: 0.5938, Validation Sensitivity: 0.1071, Validation Specificity: 0.9722, Validation F1: 0.1875\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [8/47], Training Loss: 0.03623129054903984, Training Accuracy: 0.5714, Training Sensitivity: 0.5957, Training Specificity: 0.5538, Training F1: 0.5385\n",
            "Epoch [8/47], Validation Loss: 0.0671, Validation Accuracy: 0.6250, Validation Sensitivity: 0.2143, Validation Specificity: 0.9444, Validation F1: 0.3333\n",
            "Validation loss decreased (0.068430 --> 0.067098).  Saving model ...\n",
            "Epoch [9/47], Training Loss: 0.0436619333922863, Training Accuracy: 0.5536, Training Sensitivity: 0.6022, Training Specificity: 0.5191, Training F1: 0.5283\n",
            "Epoch [9/47], Validation Loss: 0.0627, Validation Accuracy: 0.6406, Validation Sensitivity: 0.1786, Validation Specificity: 1.0000, Validation F1: 0.3030\n",
            "Validation loss decreased (0.067098 --> 0.062685).  Saving model ...\n",
            "Epoch [10/47], Training Loss: 0.04319874197244644, Training Accuracy: 0.5938, Training Sensitivity: 0.6042, Training Specificity: 0.5859, Training F1: 0.5604\n",
            "Epoch [10/47], Validation Loss: 0.0651, Validation Accuracy: 0.6094, Validation Sensitivity: 0.1071, Validation Specificity: 1.0000, Validation F1: 0.1935\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [11/47], Training Loss: 0.03403723984956741, Training Accuracy: 0.5893, Training Sensitivity: 0.6344, Training Specificity: 0.5573, Training F1: 0.5619\n",
            "Epoch [11/47], Validation Loss: 0.0660, Validation Accuracy: 0.6094, Validation Sensitivity: 0.1071, Validation Specificity: 1.0000, Validation F1: 0.1935\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [12/47], Training Loss: 0.0390317440032959, Training Accuracy: 0.5938, Training Sensitivity: 0.6264, Training Specificity: 0.5714, Training F1: 0.5561\n",
            "Epoch [12/47], Validation Loss: 0.0657, Validation Accuracy: 0.6094, Validation Sensitivity: 0.1071, Validation Specificity: 1.0000, Validation F1: 0.1935\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [13/47], Training Loss: 0.02819342538714409, Training Accuracy: 0.6607, Training Sensitivity: 0.6598, Training Specificity: 0.6614, Training F1: 0.6275\n",
            "Epoch [13/47], Validation Loss: 0.0664, Validation Accuracy: 0.5938, Validation Sensitivity: 0.1071, Validation Specificity: 0.9722, Validation F1: 0.1875\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [14/47], Training Loss: 0.04569162428379059, Training Accuracy: 0.6429, Training Sensitivity: 0.6196, Training Specificity: 0.6591, Training F1: 0.5876\n",
            "Epoch [14/47], Validation Loss: 0.0633, Validation Accuracy: 0.6406, Validation Sensitivity: 0.1786, Validation Specificity: 1.0000, Validation F1: 0.3030\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [15/47], Training Loss: 0.02713192254304886, Training Accuracy: 0.6741, Training Sensitivity: 0.6289, Training Specificity: 0.7087, Training F1: 0.6256\n",
            "Epoch [15/47], Validation Loss: 0.0654, Validation Accuracy: 0.6094, Validation Sensitivity: 0.1071, Validation Specificity: 1.0000, Validation F1: 0.1935\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [16/47], Training Loss: 0.03292177617549896, Training Accuracy: 0.6473, Training Sensitivity: 0.6774, Training Specificity: 0.6260, Training F1: 0.6146\n",
            "Epoch [16/47], Validation Loss: 0.0872, Validation Accuracy: 0.5625, Validation Sensitivity: 0.0000, Validation Specificity: 1.0000, Validation F1: 0.0000\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [17/47], Training Loss: 0.04363447055220604, Training Accuracy: 0.6652, Training Sensitivity: 0.6105, Training Specificity: 0.7054, Training F1: 0.6073\n",
            "Epoch [17/47], Validation Loss: 0.1150, Validation Accuracy: 0.5938, Validation Sensitivity: 0.1071, Validation Specificity: 0.9722, Validation F1: 0.1875\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [18/47], Training Loss: 0.017573164775967598, Training Accuracy: 0.6830, Training Sensitivity: 0.6042, Training Specificity: 0.7422, Training F1: 0.6203\n",
            "Epoch [18/47], Validation Loss: 0.0609, Validation Accuracy: 0.7500, Validation Sensitivity: 0.7500, Validation Specificity: 0.7500, Validation F1: 0.7241\n",
            "Validation loss decreased (0.062685 --> 0.060945).  Saving model ...\n",
            "Epoch [19/47], Training Loss: 0.021182868629693985, Training Accuracy: 0.7723, Training Sensitivity: 0.6869, Training Specificity: 0.8400, Training F1: 0.7273\n",
            "Epoch [19/47], Validation Loss: 0.0886, Validation Accuracy: 0.7031, Validation Sensitivity: 0.5357, Validation Specificity: 0.8333, Validation F1: 0.6122\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [20/47], Training Loss: 0.020709536969661713, Training Accuracy: 0.7812, Training Sensitivity: 0.8065, Training Specificity: 0.7634, Training F1: 0.7538\n",
            "Epoch [20/47], Validation Loss: 0.1159, Validation Accuracy: 0.7188, Validation Sensitivity: 0.5714, Validation Specificity: 0.8333, Validation F1: 0.6400\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [21/47], Training Loss: 0.033425670117139816, Training Accuracy: 0.7812, Training Sensitivity: 0.7708, Training Specificity: 0.7891, Training F1: 0.7513\n",
            "Epoch [21/47], Validation Loss: 0.0859, Validation Accuracy: 0.7969, Validation Sensitivity: 0.7857, Validation Specificity: 0.8056, Validation F1: 0.7719\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [22/47], Training Loss: 0.03175056725740433, Training Accuracy: 0.7679, Training Sensitivity: 0.7835, Training Specificity: 0.7559, Training F1: 0.7451\n",
            "Epoch [22/47], Validation Loss: 0.1036, Validation Accuracy: 0.7500, Validation Sensitivity: 0.6786, Validation Specificity: 0.8056, Validation F1: 0.7037\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [23/47], Training Loss: 0.026397785171866417, Training Accuracy: 0.7857, Training Sensitivity: 0.7368, Training Specificity: 0.8217, Training F1: 0.7447\n",
            "Epoch [23/47], Validation Loss: 0.0911, Validation Accuracy: 0.7812, Validation Sensitivity: 0.7500, Validation Specificity: 0.8056, Validation F1: 0.7500\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [24/47], Training Loss: 0.01234932616353035, Training Accuracy: 0.8080, Training Sensitivity: 0.7872, Training Specificity: 0.8231, Training F1: 0.7749\n",
            "Epoch [24/47], Validation Loss: 0.0790, Validation Accuracy: 0.7656, Validation Sensitivity: 0.7500, Validation Specificity: 0.7778, Validation F1: 0.7368\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [25/47], Training Loss: 0.037284817546606064, Training Accuracy: 0.8705, Training Sensitivity: 0.9247, Training Specificity: 0.8321, Training F1: 0.8557\n",
            "Epoch [25/47], Validation Loss: 0.1784, Validation Accuracy: 0.6406, Validation Sensitivity: 0.2857, Validation Specificity: 0.9167, Validation F1: 0.4103\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [26/47], Training Loss: 0.019830698147416115, Training Accuracy: 0.8527, Training Sensitivity: 0.8526, Training Specificity: 0.8527, Training F1: 0.8308\n",
            "Epoch [26/47], Validation Loss: 0.0697, Validation Accuracy: 0.7656, Validation Sensitivity: 0.8571, Validation Specificity: 0.6944, Validation F1: 0.7619\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [27/47], Training Loss: 0.024485258385539055, Training Accuracy: 0.8170, Training Sensitivity: 0.8229, Training Specificity: 0.8125, Training F1: 0.7940\n",
            "Epoch [27/47], Validation Loss: 0.0915, Validation Accuracy: 0.6875, Validation Sensitivity: 0.3214, Validation Specificity: 0.9722, Validation F1: 0.4737\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [28/47], Training Loss: 0.017187708988785744, Training Accuracy: 0.8482, Training Sensitivity: 0.8247, Training Specificity: 0.8661, Training F1: 0.8247\n",
            "Epoch [28/47], Validation Loss: 0.1229, Validation Accuracy: 0.7188, Validation Sensitivity: 0.3571, Validation Specificity: 1.0000, Validation F1: 0.5263\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 1: Accuracy: 0.7188, Sensitivity: 0.3571, Specificity: 1.0000, F1: 0.5263\n",
            "Processing fold 2/5...\n",
            "Epoch [1/47], Training Loss: 0.03223652020096779, Training Accuracy: 0.5759, Training Sensitivity: 0.5049, Training Specificity: 0.6364, Training F1: 0.5226\n",
            "Epoch [1/47], Validation Loss: 0.0771, Validation Accuracy: 0.4531, Validation Sensitivity: 0.8519, Validation Specificity: 0.1622, Validation F1: 0.5679\n",
            "Validation loss decreased (inf --> 0.077143).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.03838234767317772, Training Accuracy: 0.5268, Training Sensitivity: 0.4375, Training Specificity: 0.5938, Training F1: 0.4421\n",
            "Epoch [2/47], Validation Loss: 0.0744, Validation Accuracy: 0.4688, Validation Sensitivity: 0.8148, Validation Specificity: 0.2162, Validation F1: 0.5641\n",
            "Validation loss decreased (0.077143 --> 0.074432).  Saving model ...\n",
            "Epoch [3/47], Training Loss: 0.04674118384718895, Training Accuracy: 0.5268, Training Sensitivity: 0.5052, Training Specificity: 0.5433, Training F1: 0.4804\n",
            "Epoch [3/47], Validation Loss: 0.0748, Validation Accuracy: 0.5000, Validation Sensitivity: 0.8519, Validation Specificity: 0.2432, Validation F1: 0.5897\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [4/47], Training Loss: 0.04592158645391464, Training Accuracy: 0.5446, Training Sensitivity: 0.4227, Training Specificity: 0.6378, Training F1: 0.4457\n",
            "Epoch [4/47], Validation Loss: 0.0776, Validation Accuracy: 0.5000, Validation Sensitivity: 0.9259, Validation Specificity: 0.1892, Validation F1: 0.6098\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [5/47], Training Loss: 0.03414132073521614, Training Accuracy: 0.5402, Training Sensitivity: 0.5319, Training Specificity: 0.5462, Training F1: 0.4926\n",
            "Epoch [5/47], Validation Loss: 0.0681, Validation Accuracy: 0.6250, Validation Sensitivity: 0.8519, Validation Specificity: 0.4595, Validation F1: 0.6571\n",
            "Validation loss decreased (0.074432 --> 0.068105).  Saving model ...\n",
            "Epoch [6/47], Training Loss: 0.059103429317474365, Training Accuracy: 0.6071, Training Sensitivity: 0.4200, Training Specificity: 0.7581, Training F1: 0.4884\n",
            "Epoch [6/47], Validation Loss: 0.0662, Validation Accuracy: 0.6875, Validation Sensitivity: 0.8148, Validation Specificity: 0.5946, Validation F1: 0.6875\n",
            "Validation loss decreased (0.068105 --> 0.066221).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.045427292585372925, Training Accuracy: 0.6071, Training Sensitivity: 0.5213, Training Specificity: 0.6692, Training F1: 0.5269\n",
            "Epoch [7/47], Validation Loss: 0.0589, Validation Accuracy: 0.7344, Validation Sensitivity: 0.6296, Validation Specificity: 0.8108, Validation F1: 0.6667\n",
            "Validation loss decreased (0.066221 --> 0.058926).  Saving model ...\n",
            "Epoch [8/47], Training Loss: 0.032101213932037354, Training Accuracy: 0.5938, Training Sensitivity: 0.5158, Training Specificity: 0.6512, Training F1: 0.5185\n",
            "Epoch [8/47], Validation Loss: 0.0603, Validation Accuracy: 0.7969, Validation Sensitivity: 0.8148, Validation Specificity: 0.7838, Validation F1: 0.7719\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [9/47], Training Loss: 0.029760485514998436, Training Accuracy: 0.6875, Training Sensitivity: 0.6837, Training Specificity: 0.6905, Training F1: 0.6569\n",
            "Epoch [9/47], Validation Loss: 0.0994, Validation Accuracy: 0.6719, Validation Sensitivity: 0.8148, Validation Specificity: 0.5676, Validation F1: 0.6769\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [10/47], Training Loss: 0.03183756768703461, Training Accuracy: 0.7188, Training Sensitivity: 0.6989, Training Specificity: 0.7328, Training F1: 0.6736\n",
            "Epoch [10/47], Validation Loss: 0.0872, Validation Accuracy: 0.7500, Validation Sensitivity: 0.8148, Validation Specificity: 0.7027, Validation F1: 0.7333\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [11/47], Training Loss: 0.02794632315635681, Training Accuracy: 0.7098, Training Sensitivity: 0.6915, Training Specificity: 0.7231, Training F1: 0.6667\n",
            "Epoch [11/47], Validation Loss: 0.0882, Validation Accuracy: 0.7500, Validation Sensitivity: 0.8148, Validation Specificity: 0.7027, Validation F1: 0.7333\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [12/47], Training Loss: 0.039376333355903625, Training Accuracy: 0.7366, Training Sensitivity: 0.7128, Training Specificity: 0.7538, Training F1: 0.6943\n",
            "Epoch [12/47], Validation Loss: 0.0916, Validation Accuracy: 0.7812, Validation Sensitivity: 0.8148, Validation Specificity: 0.7568, Validation F1: 0.7586\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [13/47], Training Loss: 0.02143222466111183, Training Accuracy: 0.7991, Training Sensitivity: 0.7849, Training Specificity: 0.8092, Training F1: 0.7644\n",
            "Epoch [13/47], Validation Loss: 0.0850, Validation Accuracy: 0.7500, Validation Sensitivity: 0.8148, Validation Specificity: 0.7027, Validation F1: 0.7333\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [14/47], Training Loss: 0.019143296405673027, Training Accuracy: 0.8214, Training Sensitivity: 0.7959, Training Specificity: 0.8413, Training F1: 0.7959\n",
            "Epoch [14/47], Validation Loss: 0.1464, Validation Accuracy: 0.6250, Validation Sensitivity: 0.9630, Validation Specificity: 0.3784, Validation F1: 0.6842\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [15/47], Training Loss: 0.011652294546365738, Training Accuracy: 0.8214, Training Sensitivity: 0.8351, Training Specificity: 0.8110, Training F1: 0.8020\n",
            "Epoch [15/47], Validation Loss: 0.0666, Validation Accuracy: 0.7812, Validation Sensitivity: 0.8519, Validation Specificity: 0.7297, Validation F1: 0.7667\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [16/47], Training Loss: 0.032688938081264496, Training Accuracy: 0.8304, Training Sensitivity: 0.7938, Training Specificity: 0.8583, Training F1: 0.8021\n",
            "Epoch [16/47], Validation Loss: 0.0713, Validation Accuracy: 0.8281, Validation Sensitivity: 0.8148, Validation Specificity: 0.8378, Validation F1: 0.8000\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [17/47], Training Loss: 0.02494330331683159, Training Accuracy: 0.8214, Training Sensitivity: 0.7835, Training Specificity: 0.8504, Training F1: 0.7917\n",
            "Epoch [17/47], Validation Loss: 0.0872, Validation Accuracy: 0.7344, Validation Sensitivity: 0.8889, Validation Specificity: 0.6216, Validation F1: 0.7385\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 2: Accuracy: 0.7344, Sensitivity: 0.8889, Specificity: 0.6216, F1: 0.7385\n",
            "Processing fold 3/5...\n",
            "Epoch [1/47], Training Loss: 0.048897869884967804, Training Accuracy: 0.5223, Training Sensitivity: 0.5053, Training Specificity: 0.5349, Training F1: 0.4729\n",
            "Epoch [1/47], Validation Loss: 0.0695, Validation Accuracy: 0.6094, Validation Sensitivity: 1.0000, Validation Specificity: 0.3243, Validation F1: 0.6835\n",
            "Validation loss decreased (inf --> 0.069528).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.038620732724666595, Training Accuracy: 0.4955, Training Sensitivity: 0.4747, Training Specificity: 0.5120, Training F1: 0.4541\n",
            "Epoch [2/47], Validation Loss: 0.0637, Validation Accuracy: 0.6406, Validation Sensitivity: 0.9630, Validation Specificity: 0.4054, Validation F1: 0.6933\n",
            "Validation loss decreased (0.069528 --> 0.063721).  Saving model ...\n",
            "Epoch [3/47], Training Loss: 0.038940515369176865, Training Accuracy: 0.5491, Training Sensitivity: 0.4362, Training Specificity: 0.6308, Training F1: 0.4481\n",
            "Epoch [3/47], Validation Loss: 0.0608, Validation Accuracy: 0.6250, Validation Sensitivity: 0.9259, Validation Specificity: 0.4054, Validation F1: 0.6757\n",
            "Validation loss decreased (0.063721 --> 0.060801).  Saving model ...\n",
            "Epoch [4/47], Training Loss: 0.0634429082274437, Training Accuracy: 0.5179, Training Sensitivity: 0.4700, Training Specificity: 0.5565, Training F1: 0.4653\n",
            "Epoch [4/47], Validation Loss: 0.0638, Validation Accuracy: 0.5781, Validation Sensitivity: 0.9259, Validation Specificity: 0.3243, Validation F1: 0.6494\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [5/47], Training Loss: 0.04110472649335861, Training Accuracy: 0.6071, Training Sensitivity: 0.6020, Training Specificity: 0.6111, Training F1: 0.5728\n",
            "Epoch [5/47], Validation Loss: 0.0650, Validation Accuracy: 0.6094, Validation Sensitivity: 0.9630, Validation Specificity: 0.3514, Validation F1: 0.6753\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [6/47], Training Loss: 0.0350484736263752, Training Accuracy: 0.6071, Training Sensitivity: 0.5426, Training Specificity: 0.6538, Training F1: 0.5368\n",
            "Epoch [6/47], Validation Loss: 0.0681, Validation Accuracy: 0.6094, Validation Sensitivity: 0.9630, Validation Specificity: 0.3514, Validation F1: 0.6753\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [7/47], Training Loss: 0.04333791136741638, Training Accuracy: 0.6161, Training Sensitivity: 0.5275, Training Specificity: 0.6767, Training F1: 0.5275\n",
            "Epoch [7/47], Validation Loss: 0.0584, Validation Accuracy: 0.6406, Validation Sensitivity: 0.9259, Validation Specificity: 0.4324, Validation F1: 0.6849\n",
            "Validation loss decreased (0.060801 --> 0.058380).  Saving model ...\n",
            "Epoch [8/47], Training Loss: 0.05079982057213783, Training Accuracy: 0.6116, Training Sensitivity: 0.5567, Training Specificity: 0.6535, Training F1: 0.5538\n",
            "Epoch [8/47], Validation Loss: 0.0558, Validation Accuracy: 0.6562, Validation Sensitivity: 0.8519, Validation Specificity: 0.5135, Validation F1: 0.6765\n",
            "Validation loss decreased (0.058380 --> 0.055773).  Saving model ...\n",
            "Epoch [9/47], Training Loss: 0.03606729581952095, Training Accuracy: 0.6339, Training Sensitivity: 0.5579, Training Specificity: 0.6899, Training F1: 0.5638\n",
            "Epoch [9/47], Validation Loss: 0.0502, Validation Accuracy: 0.7031, Validation Sensitivity: 0.5926, Validation Specificity: 0.7838, Validation F1: 0.6275\n",
            "Validation loss decreased (0.055773 --> 0.050197).  Saving model ...\n",
            "Epoch [10/47], Training Loss: 0.027365053072571754, Training Accuracy: 0.6250, Training Sensitivity: 0.4845, Training Specificity: 0.7323, Training F1: 0.5281\n",
            "Epoch [10/47], Validation Loss: 0.0492, Validation Accuracy: 0.7500, Validation Sensitivity: 0.6667, Validation Specificity: 0.8108, Validation F1: 0.6923\n",
            "Validation loss decreased (0.050197 --> 0.049166).  Saving model ...\n",
            "Epoch [11/47], Training Loss: 0.034908559173345566, Training Accuracy: 0.6473, Training Sensitivity: 0.5684, Training Specificity: 0.7054, Training F1: 0.5775\n",
            "Epoch [11/47], Validation Loss: 0.0703, Validation Accuracy: 0.6250, Validation Sensitivity: 0.9259, Validation Specificity: 0.4054, Validation F1: 0.6757\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [12/47], Training Loss: 0.015518970787525177, Training Accuracy: 0.7188, Training Sensitivity: 0.7188, Training Specificity: 0.7188, Training F1: 0.6866\n",
            "Epoch [12/47], Validation Loss: 0.0958, Validation Accuracy: 0.6406, Validation Sensitivity: 1.0000, Validation Specificity: 0.3784, Validation F1: 0.7013\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [13/47], Training Loss: 0.032456301152706146, Training Accuracy: 0.6473, Training Sensitivity: 0.5464, Training Specificity: 0.7244, Training F1: 0.5730\n",
            "Epoch [13/47], Validation Loss: 0.0409, Validation Accuracy: 0.8125, Validation Sensitivity: 0.8148, Validation Specificity: 0.8108, Validation F1: 0.7857\n",
            "Validation loss decreased (0.049166 --> 0.040911).  Saving model ...\n",
            "Epoch [14/47], Training Loss: 0.035192977637052536, Training Accuracy: 0.7723, Training Sensitivity: 0.7449, Training Specificity: 0.7937, Training F1: 0.7411\n",
            "Epoch [14/47], Validation Loss: 0.0486, Validation Accuracy: 0.8281, Validation Sensitivity: 0.8889, Validation Specificity: 0.7838, Validation F1: 0.8136\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [15/47], Training Loss: 0.023605098947882652, Training Accuracy: 0.7500, Training Sensitivity: 0.7609, Training Specificity: 0.7424, Training F1: 0.7143\n",
            "Epoch [15/47], Validation Loss: 0.0496, Validation Accuracy: 0.7656, Validation Sensitivity: 0.6296, Validation Specificity: 0.8649, Validation F1: 0.6939\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [16/47], Training Loss: 0.0276140458881855, Training Accuracy: 0.7545, Training Sensitivity: 0.6735, Training Specificity: 0.8175, Training F1: 0.7059\n",
            "Epoch [16/47], Validation Loss: 0.0750, Validation Accuracy: 0.8125, Validation Sensitivity: 1.0000, Validation Specificity: 0.6757, Validation F1: 0.8182\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [17/47], Training Loss: 0.02207155153155327, Training Accuracy: 0.7411, Training Sensitivity: 0.7158, Training Specificity: 0.7597, Training F1: 0.7010\n",
            "Epoch [17/47], Validation Loss: 0.0481, Validation Accuracy: 0.8281, Validation Sensitivity: 0.8889, Validation Specificity: 0.7838, Validation F1: 0.8136\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [18/47], Training Loss: 0.020184023305773735, Training Accuracy: 0.7857, Training Sensitivity: 0.7604, Training Specificity: 0.8047, Training F1: 0.7526\n",
            "Epoch [18/47], Validation Loss: 0.0474, Validation Accuracy: 0.8594, Validation Sensitivity: 0.8889, Validation Specificity: 0.8378, Validation F1: 0.8421\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [19/47], Training Loss: 0.017145805060863495, Training Accuracy: 0.8438, Training Sensitivity: 0.8300, Training Specificity: 0.8548, Training F1: 0.8259\n",
            "Epoch [19/47], Validation Loss: 0.0460, Validation Accuracy: 0.8594, Validation Sensitivity: 0.8889, Validation Specificity: 0.8378, Validation F1: 0.8421\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [20/47], Training Loss: 0.013577092438936234, Training Accuracy: 0.8125, Training Sensitivity: 0.8211, Training Specificity: 0.8062, Training F1: 0.7879\n",
            "Epoch [20/47], Validation Loss: 0.0992, Validation Accuracy: 0.6406, Validation Sensitivity: 0.9630, Validation Specificity: 0.4054, Validation F1: 0.6933\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [21/47], Training Loss: 0.027305135503411293, Training Accuracy: 0.7812, Training Sensitivity: 0.8200, Training Specificity: 0.7500, Training F1: 0.7700\n",
            "Epoch [21/47], Validation Loss: 0.0478, Validation Accuracy: 0.7812, Validation Sensitivity: 0.6296, Validation Specificity: 0.8919, Validation F1: 0.7083\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [22/47], Training Loss: 0.021407119929790497, Training Accuracy: 0.7946, Training Sensitivity: 0.7474, Training Specificity: 0.8295, Training F1: 0.7553\n",
            "Epoch [22/47], Validation Loss: 0.0413, Validation Accuracy: 0.7969, Validation Sensitivity: 0.7407, Validation Specificity: 0.8378, Validation F1: 0.7547\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [23/47], Training Loss: 0.0248443391174078, Training Accuracy: 0.7991, Training Sensitivity: 0.8200, Training Specificity: 0.7823, Training F1: 0.7847\n",
            "Epoch [23/47], Validation Loss: 0.0618, Validation Accuracy: 0.8281, Validation Sensitivity: 0.9630, Validation Specificity: 0.7297, Validation F1: 0.8254\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 3: Accuracy: 0.8281, Sensitivity: 0.9630, Specificity: 0.7297, F1: 0.8254\n",
            "Processing fold 4/5...\n",
            "Epoch [1/47], Training Loss: 0.03916209191083908, Training Accuracy: 0.5045, Training Sensitivity: 0.3776, Training Specificity: 0.6032, Training F1: 0.4000\n",
            "Epoch [1/47], Validation Loss: 0.0340, Validation Accuracy: 0.6875, Validation Sensitivity: 0.1000, Validation Specificity: 0.9545, Validation F1: 0.1667\n",
            "Validation loss decreased (inf --> 0.033959).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.031331345438957214, Training Accuracy: 0.5446, Training Sensitivity: 0.3878, Training Specificity: 0.6667, Training F1: 0.4270\n",
            "Epoch [2/47], Validation Loss: 0.0361, Validation Accuracy: 0.5938, Validation Sensitivity: 0.6471, Validation Specificity: 0.5333, Validation F1: 0.6286\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [3/47], Training Loss: 0.052089251577854156, Training Accuracy: 0.4688, Training Sensitivity: 0.3830, Training Specificity: 0.5308, Training F1: 0.3770\n",
            "Epoch [3/47], Validation Loss: 0.0348, Validation Accuracy: 0.5625, Validation Sensitivity: 0.8000, Validation Specificity: 0.3529, Validation F1: 0.6316\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [4/47], Training Loss: 0.045587312430143356, Training Accuracy: 0.5134, Training Sensitivity: 0.3958, Training Specificity: 0.6016, Training F1: 0.4108\n",
            "Epoch [4/47], Validation Loss: 0.0379, Validation Accuracy: 0.4688, Validation Sensitivity: 0.8750, Validation Specificity: 0.3333, Validation F1: 0.4516\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [5/47], Training Loss: 0.028695642948150635, Training Accuracy: 0.5670, Training Sensitivity: 0.4615, Training Specificity: 0.6391, Training F1: 0.4641\n",
            "Epoch [5/47], Validation Loss: 0.0343, Validation Accuracy: 0.5625, Validation Sensitivity: 0.8462, Validation Specificity: 0.3684, Validation F1: 0.6111\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [6/47], Training Loss: 0.05048474669456482, Training Accuracy: 0.5134, Training Sensitivity: 0.4624, Training Specificity: 0.5496, Training F1: 0.4410\n",
            "Epoch [6/47], Validation Loss: 0.0319, Validation Accuracy: 0.6250, Validation Sensitivity: 1.0000, Validation Specificity: 0.2941, Validation F1: 0.7143\n",
            "Validation loss decreased (0.033959 --> 0.031931).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.0313873365521431, Training Accuracy: 0.5804, Training Sensitivity: 0.5102, Training Specificity: 0.6349, Training F1: 0.5155\n",
            "Epoch [7/47], Validation Loss: 0.0330, Validation Accuracy: 0.6250, Validation Sensitivity: 0.8333, Validation Specificity: 0.5000, Validation F1: 0.6250\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [8/47], Training Loss: 0.035148318856954575, Training Accuracy: 0.5670, Training Sensitivity: 0.4894, Training Specificity: 0.6231, Training F1: 0.4868\n",
            "Epoch [8/47], Validation Loss: 0.0299, Validation Accuracy: 0.7188, Validation Sensitivity: 0.8889, Validation Specificity: 0.6522, Validation F1: 0.6400\n",
            "Validation loss decreased (0.031931 --> 0.029867).  Saving model ...\n",
            "Epoch [9/47], Training Loss: 0.034956954419612885, Training Accuracy: 0.6116, Training Sensitivity: 0.4444, Training Specificity: 0.7239, Training F1: 0.4790\n",
            "Epoch [9/47], Validation Loss: 0.0319, Validation Accuracy: 0.6250, Validation Sensitivity: 0.4000, Validation Specificity: 0.7273, Validation F1: 0.4000\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [10/47], Training Loss: 0.02725343219935894, Training Accuracy: 0.6116, Training Sensitivity: 0.5816, Training Specificity: 0.6349, Training F1: 0.5672\n",
            "Epoch [10/47], Validation Loss: 0.0335, Validation Accuracy: 0.5625, Validation Sensitivity: 0.9091, Validation Specificity: 0.3810, Validation F1: 0.5882\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [11/47], Training Loss: 0.024109717458486557, Training Accuracy: 0.6741, Training Sensitivity: 0.6500, Training Specificity: 0.6935, Training F1: 0.6404\n",
            "Epoch [11/47], Validation Loss: 0.0264, Validation Accuracy: 0.8125, Validation Sensitivity: 0.8333, Validation Specificity: 0.8000, Validation F1: 0.7692\n",
            "Validation loss decreased (0.029867 --> 0.026431).  Saving model ...\n",
            "Epoch [12/47], Training Loss: 0.03053336963057518, Training Accuracy: 0.6295, Training Sensitivity: 0.5700, Training Specificity: 0.6774, Training F1: 0.5787\n",
            "Epoch [12/47], Validation Loss: 0.0198, Validation Accuracy: 0.9062, Validation Sensitivity: 0.8750, Validation Specificity: 0.9375, Validation F1: 0.9032\n",
            "Validation loss decreased (0.026431 --> 0.019835).  Saving model ...\n",
            "Epoch [13/47], Training Loss: 0.0260256789624691, Training Accuracy: 0.6339, Training Sensitivity: 0.5670, Training Specificity: 0.6850, Training F1: 0.5729\n",
            "Epoch [13/47], Validation Loss: 0.0239, Validation Accuracy: 0.7500, Validation Sensitivity: 0.6923, Validation Specificity: 0.7895, Validation F1: 0.6923\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [14/47], Training Loss: 0.03348952531814575, Training Accuracy: 0.6116, Training Sensitivity: 0.5521, Training Specificity: 0.6562, Training F1: 0.5492\n",
            "Epoch [14/47], Validation Loss: 0.0183, Validation Accuracy: 0.7812, Validation Sensitivity: 0.6250, Validation Specificity: 0.9375, Validation F1: 0.7407\n",
            "Validation loss decreased (0.019835 --> 0.018348).  Saving model ...\n",
            "Epoch [15/47], Training Loss: 0.03147310018539429, Training Accuracy: 0.6652, Training Sensitivity: 0.6429, Training Specificity: 0.6825, Training F1: 0.6269\n",
            "Epoch [15/47], Validation Loss: 0.0164, Validation Accuracy: 0.8438, Validation Sensitivity: 0.6667, Validation Specificity: 0.9500, Validation F1: 0.7619\n",
            "Validation loss decreased (0.018348 --> 0.016424).  Saving model ...\n",
            "Epoch [16/47], Training Loss: 0.02922791801393032, Training Accuracy: 0.7098, Training Sensitivity: 0.7368, Training Specificity: 0.6899, Training F1: 0.6829\n",
            "Epoch [16/47], Validation Loss: 0.0257, Validation Accuracy: 0.7500, Validation Sensitivity: 0.9231, Validation Specificity: 0.6316, Validation F1: 0.7500\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [17/47], Training Loss: 0.022567743435502052, Training Accuracy: 0.7589, Training Sensitivity: 0.6970, Training Specificity: 0.8080, Training F1: 0.7188\n",
            "Epoch [17/47], Validation Loss: 0.0172, Validation Accuracy: 0.7812, Validation Sensitivity: 0.5385, Validation Specificity: 0.9474, Validation F1: 0.6667\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [18/47], Training Loss: 0.026089394465088844, Training Accuracy: 0.7366, Training Sensitivity: 0.6774, Training Specificity: 0.7786, Training F1: 0.6811\n",
            "Epoch [18/47], Validation Loss: 0.0148, Validation Accuracy: 0.9062, Validation Sensitivity: 1.0000, Validation Specificity: 0.8421, Validation F1: 0.8966\n",
            "Validation loss decreased (0.016424 --> 0.014823).  Saving model ...\n",
            "Epoch [19/47], Training Loss: 0.024955876171588898, Training Accuracy: 0.7812, Training Sensitivity: 0.7263, Training Specificity: 0.8217, Training F1: 0.7380\n",
            "Epoch [19/47], Validation Loss: 0.0167, Validation Accuracy: 0.8438, Validation Sensitivity: 1.0000, Validation Specificity: 0.7222, Validation F1: 0.8485\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [20/47], Training Loss: 0.017518550157546997, Training Accuracy: 0.7946, Training Sensitivity: 0.7723, Training Specificity: 0.8130, Training F1: 0.7723\n",
            "Epoch [20/47], Validation Loss: 0.0153, Validation Accuracy: 0.8750, Validation Sensitivity: 1.0000, Validation Specificity: 0.8000, Validation F1: 0.8571\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [21/47], Training Loss: 0.029489193111658096, Training Accuracy: 0.8304, Training Sensitivity: 0.8632, Training Specificity: 0.8062, Training F1: 0.8119\n",
            "Epoch [21/47], Validation Loss: 0.0131, Validation Accuracy: 0.9062, Validation Sensitivity: 1.0000, Validation Specificity: 0.8421, Validation F1: 0.8966\n",
            "Validation loss decreased (0.014823 --> 0.013070).  Saving model ...\n",
            "Epoch [22/47], Training Loss: 0.022598518058657646, Training Accuracy: 0.8304, Training Sensitivity: 0.8105, Training Specificity: 0.8450, Training F1: 0.8021\n",
            "Epoch [22/47], Validation Loss: 0.0080, Validation Accuracy: 0.9062, Validation Sensitivity: 0.8333, Validation Specificity: 0.9500, Validation F1: 0.8696\n",
            "Validation loss decreased (0.013070 --> 0.007956).  Saving model ...\n",
            "Epoch [23/47], Training Loss: 0.022336019203066826, Training Accuracy: 0.8393, Training Sensitivity: 0.8000, Training Specificity: 0.8682, Training F1: 0.8085\n",
            "Epoch [23/47], Validation Loss: 0.0091, Validation Accuracy: 0.9062, Validation Sensitivity: 1.0000, Validation Specificity: 0.8421, Validation F1: 0.8966\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [24/47], Training Loss: 0.03282361477613449, Training Accuracy: 0.8259, Training Sensitivity: 0.7889, Training Specificity: 0.8507, Training F1: 0.7845\n",
            "Epoch [24/47], Validation Loss: 0.0249, Validation Accuracy: 0.8125, Validation Sensitivity: 0.5714, Validation Specificity: 1.0000, Validation F1: 0.7273\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [25/47], Training Loss: 0.018281925469636917, Training Accuracy: 0.8393, Training Sensitivity: 0.8478, Training Specificity: 0.8333, Training F1: 0.8125\n",
            "Epoch [25/47], Validation Loss: 0.0485, Validation Accuracy: 0.6562, Validation Sensitivity: 1.0000, Validation Specificity: 0.3529, Validation F1: 0.7317\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [26/47], Training Loss: 0.016138631850481033, Training Accuracy: 0.8304, Training Sensitivity: 0.8229, Training Specificity: 0.8359, Training F1: 0.8061\n",
            "Epoch [26/47], Validation Loss: 0.0145, Validation Accuracy: 0.8438, Validation Sensitivity: 0.6875, Validation Specificity: 1.0000, Validation F1: 0.8148\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [27/47], Training Loss: 0.018857797607779503, Training Accuracy: 0.8571, Training Sensitivity: 0.8247, Training Specificity: 0.8819, Training F1: 0.8333\n",
            "Epoch [27/47], Validation Loss: 0.0073, Validation Accuracy: 0.9375, Validation Sensitivity: 1.0000, Validation Specificity: 0.9000, Validation F1: 0.9231\n",
            "Validation loss decreased (0.007956 --> 0.007298).  Saving model ...\n",
            "Epoch [28/47], Training Loss: 0.013906306587159634, Training Accuracy: 0.8705, Training Sensitivity: 0.8878, Training Specificity: 0.8571, Training F1: 0.8571\n",
            "Epoch [28/47], Validation Loss: 0.0066, Validation Accuracy: 0.9062, Validation Sensitivity: 0.8182, Validation Specificity: 0.9524, Validation F1: 0.8571\n",
            "Validation loss decreased (0.007298 --> 0.006628).  Saving model ...\n",
            "Epoch [29/47], Training Loss: 0.010435796342790127, Training Accuracy: 0.9062, Training Sensitivity: 0.8817, Training Specificity: 0.9237, Training F1: 0.8865\n",
            "Epoch [29/47], Validation Loss: 0.0115, Validation Accuracy: 0.9062, Validation Sensitivity: 1.0000, Validation Specificity: 0.8421, Validation F1: 0.8966\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [30/47], Training Loss: 0.013001612387597561, Training Accuracy: 0.8929, Training Sensitivity: 0.9247, Training Specificity: 0.8702, Training F1: 0.8776\n",
            "Epoch [30/47], Validation Loss: 0.0105, Validation Accuracy: 0.9062, Validation Sensitivity: 0.7273, Validation Specificity: 1.0000, Validation F1: 0.8421\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [31/47], Training Loss: 0.012414230033755302, Training Accuracy: 0.8571, Training Sensitivity: 0.7895, Training Specificity: 0.9070, Training F1: 0.8242\n",
            "Epoch [31/47], Validation Loss: 0.0155, Validation Accuracy: 0.8438, Validation Sensitivity: 1.0000, Validation Specificity: 0.7222, Validation F1: 0.8485\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [32/47], Training Loss: 0.011741021648049355, Training Accuracy: 0.8839, Training Sensitivity: 0.9072, Training Specificity: 0.8661, Training F1: 0.8713\n",
            "Epoch [32/47], Validation Loss: 0.0044, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9444, Validation F1: 0.9655\n",
            "Validation loss decreased (0.006628 --> 0.004387).  Saving model ...\n",
            "Epoch [33/47], Training Loss: 0.014112027361989021, Training Accuracy: 0.8973, Training Sensitivity: 0.9043, Training Specificity: 0.8923, Training F1: 0.8808\n",
            "Epoch [33/47], Validation Loss: 0.0275, Validation Accuracy: 0.8438, Validation Sensitivity: 1.0000, Validation Specificity: 0.7500, Validation F1: 0.8276\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [34/47], Training Loss: 0.019547108560800552, Training Accuracy: 0.8750, Training Sensitivity: 0.9474, Training Specificity: 0.8217, Training F1: 0.8654\n",
            "Epoch [34/47], Validation Loss: 0.0084, Validation Accuracy: 0.9375, Validation Sensitivity: 1.0000, Validation Specificity: 0.8824, Validation F1: 0.9375\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [35/47], Training Loss: 0.0035638846457004547, Training Accuracy: 0.8929, Training Sensitivity: 0.8936, Training Specificity: 0.8923, Training F1: 0.8750\n",
            "Epoch [35/47], Validation Loss: 0.0216, Validation Accuracy: 0.8125, Validation Sensitivity: 0.5714, Validation Specificity: 1.0000, Validation F1: 0.7273\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [36/47], Training Loss: 0.016745736822485924, Training Accuracy: 0.8705, Training Sensitivity: 0.8265, Training Specificity: 0.9048, Training F1: 0.8482\n",
            "Epoch [36/47], Validation Loss: 0.0053, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9375, Validation F1: 0.9697\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [37/47], Training Loss: 0.00673215975984931, Training Accuracy: 0.9152, Training Sensitivity: 0.9149, Training Specificity: 0.9154, Training F1: 0.9005\n",
            "Epoch [37/47], Validation Loss: 0.0156, Validation Accuracy: 0.8750, Validation Sensitivity: 0.6923, Validation Specificity: 1.0000, Validation F1: 0.8182\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [38/47], Training Loss: 0.02039962448179722, Training Accuracy: 0.8973, Training Sensitivity: 0.8737, Training Specificity: 0.9147, Training F1: 0.8783\n",
            "Epoch [38/47], Validation Loss: 0.0035, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9333, Validation F1: 0.9714\n",
            "Validation loss decreased (0.004387 --> 0.003530).  Saving model ...\n",
            "Epoch [39/47], Training Loss: 0.012892316095530987, Training Accuracy: 0.9062, Training Sensitivity: 0.8696, Training Specificity: 0.9318, Training F1: 0.8840\n",
            "Epoch [39/47], Validation Loss: 0.0134, Validation Accuracy: 0.8438, Validation Sensitivity: 0.7857, Validation Specificity: 0.8889, Validation F1: 0.8148\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [40/47], Training Loss: 0.011705462820827961, Training Accuracy: 0.9107, Training Sensitivity: 0.8824, Training Specificity: 0.9344, Training F1: 0.9000\n",
            "Epoch [40/47], Validation Loss: 0.0334, Validation Accuracy: 0.8438, Validation Sensitivity: 0.5833, Validation Specificity: 1.0000, Validation F1: 0.7368\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [41/47], Training Loss: 0.013848920352756977, Training Accuracy: 0.9107, Training Sensitivity: 0.8586, Training Specificity: 0.9520, Training F1: 0.8947\n",
            "Epoch [41/47], Validation Loss: 0.0058, Validation Accuracy: 0.9375, Validation Sensitivity: 0.8333, Validation Specificity: 1.0000, Validation F1: 0.9091\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [42/47], Training Loss: 0.005645579658448696, Training Accuracy: 0.9107, Training Sensitivity: 0.8936, Training Specificity: 0.9231, Training F1: 0.8936\n",
            "Epoch [42/47], Validation Loss: 0.0041, Validation Accuracy: 0.9375, Validation Sensitivity: 1.0000, Validation Specificity: 0.9048, Validation F1: 0.9167\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [43/47], Training Loss: 0.00629880465567112, Training Accuracy: 0.9375, Training Sensitivity: 0.9462, Training Specificity: 0.9313, Training F1: 0.9263\n",
            "Epoch [43/47], Validation Loss: 0.0068, Validation Accuracy: 0.9688, Validation Sensitivity: 1.0000, Validation Specificity: 0.9412, Validation F1: 0.9677\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [44/47], Training Loss: 0.016404306516051292, Training Accuracy: 0.9241, Training Sensitivity: 0.9091, Training Specificity: 0.9360, Training F1: 0.9137\n",
            "Epoch [44/47], Validation Loss: 0.0158, Validation Accuracy: 0.8750, Validation Sensitivity: 1.0000, Validation Specificity: 0.8000, Validation F1: 0.8571\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [45/47], Training Loss: 0.009355171583592892, Training Accuracy: 0.9241, Training Sensitivity: 0.9263, Training Specificity: 0.9225, Training F1: 0.9119\n",
            "Epoch [45/47], Validation Loss: 0.0499, Validation Accuracy: 0.7812, Validation Sensitivity: 0.3636, Validation Specificity: 1.0000, Validation F1: 0.5333\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [46/47], Training Loss: 0.01067895907908678, Training Accuracy: 0.8884, Training Sensitivity: 0.8660, Training Specificity: 0.9055, Training F1: 0.8705\n",
            "Epoch [46/47], Validation Loss: 0.0165, Validation Accuracy: 0.9375, Validation Sensitivity: 1.0000, Validation Specificity: 0.9000, Validation F1: 0.9231\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [47/47], Training Loss: 0.0051201763562858105, Training Accuracy: 0.9286, Training Sensitivity: 0.9362, Training Specificity: 0.9231, Training F1: 0.9167\n",
            "Epoch [47/47], Validation Loss: 0.0012, Validation Accuracy: 1.0000, Validation Sensitivity: 1.0000, Validation Specificity: 1.0000, Validation F1: 1.0000\n",
            "Validation loss decreased (0.003530 --> 0.001180).  Saving model ...\n",
            "Fold 4: Accuracy: 1.0000, Sensitivity: 1.0000, Specificity: 1.0000, F1: 1.0000\n",
            "Processing fold 5/5...\n",
            "Epoch [1/47], Training Loss: 0.04388211667537689, Training Accuracy: 0.5045, Training Sensitivity: 0.3939, Training Specificity: 0.5920, Training F1: 0.4127\n",
            "Epoch [1/47], Validation Loss: 0.0357, Validation Accuracy: 0.4688, Validation Sensitivity: 1.0000, Validation Specificity: 0.1905, Validation F1: 0.5641\n",
            "Validation loss decreased (inf --> 0.035697).  Saving model ...\n",
            "Epoch [2/47], Training Loss: 0.04328138008713722, Training Accuracy: 0.5536, Training Sensitivity: 0.4194, Training Specificity: 0.6489, Training F1: 0.4382\n",
            "Epoch [2/47], Validation Loss: 0.0442, Validation Accuracy: 0.4375, Validation Sensitivity: 1.0000, Validation Specificity: 0.0526, Validation F1: 0.5909\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [3/47], Training Loss: 0.046836063265800476, Training Accuracy: 0.5670, Training Sensitivity: 0.5000, Training Specificity: 0.6172, Training F1: 0.4974\n",
            "Epoch [3/47], Validation Loss: 0.0422, Validation Accuracy: 0.5312, Validation Sensitivity: 1.0000, Validation Specificity: 0.0625, Validation F1: 0.6809\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [4/47], Training Loss: 0.04033080115914345, Training Accuracy: 0.5938, Training Sensitivity: 0.4608, Training Specificity: 0.7049, Training F1: 0.5081\n",
            "Epoch [4/47], Validation Loss: 0.0535, Validation Accuracy: 0.3125, Validation Sensitivity: 1.0000, Validation Specificity: 0.0833, Validation F1: 0.4211\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [5/47], Training Loss: 0.06125874072313309, Training Accuracy: 0.5580, Training Sensitivity: 0.4624, Training Specificity: 0.6260, Training F1: 0.4649\n",
            "Epoch [5/47], Validation Loss: 0.0351, Validation Accuracy: 0.5625, Validation Sensitivity: 1.0000, Validation Specificity: 0.1250, Validation F1: 0.6957\n",
            "Validation loss decreased (0.035697 --> 0.035121).  Saving model ...\n",
            "Epoch [6/47], Training Loss: 0.04285227507352829, Training Accuracy: 0.5759, Training Sensitivity: 0.4624, Training Specificity: 0.6565, Training F1: 0.4751\n",
            "Epoch [6/47], Validation Loss: 0.0308, Validation Accuracy: 0.5938, Validation Sensitivity: 1.0000, Validation Specificity: 0.2353, Validation F1: 0.6977\n",
            "Validation loss decreased (0.035121 --> 0.030790).  Saving model ...\n",
            "Epoch [7/47], Training Loss: 0.04364434629678726, Training Accuracy: 0.5804, Training Sensitivity: 0.4490, Training Specificity: 0.6825, Training F1: 0.4835\n",
            "Epoch [7/47], Validation Loss: 0.0382, Validation Accuracy: 0.5000, Validation Sensitivity: 1.0000, Validation Specificity: 0.2000, Validation F1: 0.6000\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [8/47], Training Loss: 0.04151828587055206, Training Accuracy: 0.5491, Training Sensitivity: 0.4792, Training Specificity: 0.6016, Training F1: 0.4767\n",
            "Epoch [8/47], Validation Loss: 0.0385, Validation Accuracy: 0.5625, Validation Sensitivity: 1.0000, Validation Specificity: 0.2222, Validation F1: 0.6667\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [9/47], Training Loss: 0.044912777841091156, Training Accuracy: 0.5938, Training Sensitivity: 0.4894, Training Specificity: 0.6692, Training F1: 0.5027\n",
            "Epoch [9/47], Validation Loss: 0.0312, Validation Accuracy: 0.6250, Validation Sensitivity: 0.9167, Validation Specificity: 0.4500, Validation F1: 0.6471\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [10/47], Training Loss: 0.034479789435863495, Training Accuracy: 0.6562, Training Sensitivity: 0.6200, Training Specificity: 0.6855, Training F1: 0.6169\n",
            "Epoch [10/47], Validation Loss: 0.0436, Validation Accuracy: 0.5625, Validation Sensitivity: 1.0000, Validation Specificity: 0.2222, Validation F1: 0.6667\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [11/47], Training Loss: 0.03840417414903641, Training Accuracy: 0.6161, Training Sensitivity: 0.5104, Training Specificity: 0.6953, Training F1: 0.5326\n",
            "Epoch [11/47], Validation Loss: 0.0361, Validation Accuracy: 0.6250, Validation Sensitivity: 0.9231, Validation Specificity: 0.4211, Validation F1: 0.6667\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [12/47], Training Loss: 0.052618712186813354, Training Accuracy: 0.6161, Training Sensitivity: 0.5612, Training Specificity: 0.6587, Training F1: 0.5612\n",
            "Epoch [12/47], Validation Loss: 0.0371, Validation Accuracy: 0.6250, Validation Sensitivity: 0.9231, Validation Specificity: 0.4211, Validation F1: 0.6667\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [13/47], Training Loss: 0.037846796214580536, Training Accuracy: 0.6295, Training Sensitivity: 0.6000, Training Specificity: 0.6512, Training F1: 0.5787\n",
            "Epoch [13/47], Validation Loss: 0.0350, Validation Accuracy: 0.6875, Validation Sensitivity: 1.0000, Validation Specificity: 0.4118, Validation F1: 0.7500\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [14/47], Training Loss: 0.018720677122473717, Training Accuracy: 0.6920, Training Sensitivity: 0.6526, Training Specificity: 0.7209, Training F1: 0.6425\n",
            "Epoch [14/47], Validation Loss: 0.0287, Validation Accuracy: 0.7812, Validation Sensitivity: 0.8462, Validation Specificity: 0.7368, Validation F1: 0.7586\n",
            "Validation loss decreased (0.030790 --> 0.028711).  Saving model ...\n",
            "Epoch [15/47], Training Loss: 0.03548120707273483, Training Accuracy: 0.6830, Training Sensitivity: 0.6562, Training Specificity: 0.7031, Training F1: 0.6396\n",
            "Epoch [15/47], Validation Loss: 0.0278, Validation Accuracy: 0.7188, Validation Sensitivity: 1.0000, Validation Specificity: 0.4000, Validation F1: 0.7907\n",
            "Validation loss decreased (0.028711 --> 0.027822).  Saving model ...\n",
            "Epoch [16/47], Training Loss: 0.031226376071572304, Training Accuracy: 0.7366, Training Sensitivity: 0.7097, Training Specificity: 0.7557, Training F1: 0.6911\n",
            "Epoch [16/47], Validation Loss: 0.0495, Validation Accuracy: 0.6875, Validation Sensitivity: 0.5333, Validation Specificity: 0.8235, Validation F1: 0.6154\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [17/47], Training Loss: 0.02766798436641693, Training Accuracy: 0.7411, Training Sensitivity: 0.7255, Training Specificity: 0.7541, Training F1: 0.7184\n",
            "Epoch [17/47], Validation Loss: 0.0330, Validation Accuracy: 0.7500, Validation Sensitivity: 1.0000, Validation Specificity: 0.5556, Validation F1: 0.7778\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [18/47], Training Loss: 0.03491838276386261, Training Accuracy: 0.7946, Training Sensitivity: 0.8000, Training Specificity: 0.7907, Training F1: 0.7677\n",
            "Epoch [18/47], Validation Loss: 0.0388, Validation Accuracy: 0.7188, Validation Sensitivity: 1.0000, Validation Specificity: 0.4375, Validation F1: 0.7805\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [19/47], Training Loss: 0.024169031530618668, Training Accuracy: 0.7545, Training Sensitivity: 0.8041, Training Specificity: 0.7165, Training F1: 0.7393\n",
            "Epoch [19/47], Validation Loss: 0.0228, Validation Accuracy: 0.8125, Validation Sensitivity: 0.9286, Validation Specificity: 0.7222, Validation F1: 0.8125\n",
            "Validation loss decreased (0.027822 --> 0.022759).  Saving model ...\n",
            "Epoch [20/47], Training Loss: 0.02486269176006317, Training Accuracy: 0.7857, Training Sensitivity: 0.7172, Training Specificity: 0.8400, Training F1: 0.7474\n",
            "Epoch [20/47], Validation Loss: 0.0219, Validation Accuracy: 0.8125, Validation Sensitivity: 0.8462, Validation Specificity: 0.7895, Validation F1: 0.7857\n",
            "Validation loss decreased (0.022759 --> 0.021903).  Saving model ...\n",
            "Epoch [21/47], Training Loss: 0.018175512552261353, Training Accuracy: 0.8125, Training Sensitivity: 0.8265, Training Specificity: 0.8016, Training F1: 0.7941\n",
            "Epoch [21/47], Validation Loss: 0.0221, Validation Accuracy: 0.8125, Validation Sensitivity: 0.8462, Validation Specificity: 0.7895, Validation F1: 0.7857\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [22/47], Training Loss: 0.01434486173093319, Training Accuracy: 0.8616, Training Sensitivity: 0.8586, Training Specificity: 0.8640, Training F1: 0.8458\n",
            "Epoch [22/47], Validation Loss: 0.0151, Validation Accuracy: 0.9062, Validation Sensitivity: 0.9333, Validation Specificity: 0.8824, Validation F1: 0.9032\n",
            "Validation loss decreased (0.021903 --> 0.015050).  Saving model ...\n",
            "Epoch [23/47], Training Loss: 0.024049634113907814, Training Accuracy: 0.8438, Training Sensitivity: 0.8687, Training Specificity: 0.8240, Training F1: 0.8309\n",
            "Epoch [23/47], Validation Loss: 0.0351, Validation Accuracy: 0.8438, Validation Sensitivity: 0.8750, Validation Specificity: 0.8125, Validation F1: 0.8485\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [24/47], Training Loss: 0.020508691668510437, Training Accuracy: 0.8438, Training Sensitivity: 0.9022, Training Specificity: 0.8030, Training F1: 0.8259\n",
            "Epoch [24/47], Validation Loss: 0.0449, Validation Accuracy: 0.8125, Validation Sensitivity: 0.7273, Validation Specificity: 0.8571, Validation F1: 0.7273\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [25/47], Training Loss: 0.029590534046292305, Training Accuracy: 0.8705, Training Sensitivity: 0.8817, Training Specificity: 0.8626, Training F1: 0.8497\n",
            "Epoch [25/47], Validation Loss: 0.0357, Validation Accuracy: 0.8125, Validation Sensitivity: 0.9333, Validation Specificity: 0.7059, Validation F1: 0.8235\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [26/47], Training Loss: 0.013899551704525948, Training Accuracy: 0.8795, Training Sensitivity: 0.9381, Training Specificity: 0.8346, Training F1: 0.8708\n",
            "Epoch [26/47], Validation Loss: 0.0420, Validation Accuracy: 0.7188, Validation Sensitivity: 0.9091, Validation Specificity: 0.6190, Validation F1: 0.6897\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [27/47], Training Loss: 0.026169832795858383, Training Accuracy: 0.8304, Training Sensitivity: 0.8200, Training Specificity: 0.8387, Training F1: 0.8119\n",
            "Epoch [27/47], Validation Loss: 0.0146, Validation Accuracy: 0.9062, Validation Sensitivity: 1.0000, Validation Specificity: 0.8421, Validation F1: 0.8966\n",
            "Validation loss decreased (0.015050 --> 0.014648).  Saving model ...\n",
            "Epoch [28/47], Training Loss: 0.029874784871935844, Training Accuracy: 0.8839, Training Sensitivity: 0.8817, Training Specificity: 0.8855, Training F1: 0.8632\n",
            "Epoch [28/47], Validation Loss: 0.0131, Validation Accuracy: 0.9375, Validation Sensitivity: 1.0000, Validation Specificity: 0.8889, Validation F1: 0.9333\n",
            "Validation loss decreased (0.014648 --> 0.013064).  Saving model ...\n",
            "Epoch [29/47], Training Loss: 0.014843403361737728, Training Accuracy: 0.8571, Training Sensitivity: 0.8936, Training Specificity: 0.8308, Training F1: 0.8400\n",
            "Epoch [29/47], Validation Loss: 0.0315, Validation Accuracy: 0.8125, Validation Sensitivity: 1.0000, Validation Specificity: 0.6842, Validation F1: 0.8125\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [30/47], Training Loss: 0.02661275304853916, Training Accuracy: 0.8750, Training Sensitivity: 0.9000, Training Specificity: 0.8548, Training F1: 0.8654\n",
            "Epoch [30/47], Validation Loss: 0.0438, Validation Accuracy: 0.7188, Validation Sensitivity: 0.9231, Validation Specificity: 0.5789, Validation F1: 0.7273\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [31/47], Training Loss: 0.012267920188605785, Training Accuracy: 0.9018, Training Sensitivity: 0.8866, Training Specificity: 0.9134, Training F1: 0.8866\n",
            "Epoch [31/47], Validation Loss: 0.0312, Validation Accuracy: 0.8125, Validation Sensitivity: 0.7500, Validation Specificity: 0.8750, Validation F1: 0.8000\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [32/47], Training Loss: 0.010638002306222916, Training Accuracy: 0.8884, Training Sensitivity: 0.9043, Training Specificity: 0.8769, Training F1: 0.8718\n",
            "Epoch [32/47], Validation Loss: 0.0119, Validation Accuracy: 0.9062, Validation Sensitivity: 0.9286, Validation Specificity: 0.8889, Validation F1: 0.8966\n",
            "Validation loss decreased (0.013064 --> 0.011878).  Saving model ...\n",
            "Epoch [33/47], Training Loss: 0.04816737025976181, Training Accuracy: 0.8348, Training Sensitivity: 0.7629, Training Specificity: 0.8898, Training F1: 0.8000\n",
            "Epoch [33/47], Validation Loss: 0.0468, Validation Accuracy: 0.7812, Validation Sensitivity: 1.0000, Validation Specificity: 0.6818, Validation F1: 0.7407\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [34/47], Training Loss: 0.012444167397916317, Training Accuracy: 0.8438, Training Sensitivity: 0.8969, Training Specificity: 0.8031, Training F1: 0.8325\n",
            "Epoch [34/47], Validation Loss: 0.0203, Validation Accuracy: 0.8438, Validation Sensitivity: 1.0000, Validation Specificity: 0.6875, Validation F1: 0.8649\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [35/47], Training Loss: 0.016070857644081116, Training Accuracy: 0.9018, Training Sensitivity: 0.8586, Training Specificity: 0.9360, Training F1: 0.8854\n",
            "Epoch [35/47], Validation Loss: 0.0236, Validation Accuracy: 0.9062, Validation Sensitivity: 0.9286, Validation Specificity: 0.8889, Validation F1: 0.8966\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [36/47], Training Loss: 0.014309881255030632, Training Accuracy: 0.8973, Training Sensitivity: 0.9362, Training Specificity: 0.8692, Training F1: 0.8844\n",
            "Epoch [36/47], Validation Loss: 0.0313, Validation Accuracy: 0.8438, Validation Sensitivity: 0.8750, Validation Specificity: 0.8125, Validation F1: 0.8485\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [37/47], Training Loss: 0.013533949851989746, Training Accuracy: 0.8973, Training Sensitivity: 0.8958, Training Specificity: 0.8984, Training F1: 0.8821\n",
            "Epoch [37/47], Validation Loss: 0.0291, Validation Accuracy: 0.8438, Validation Sensitivity: 0.9412, Validation Specificity: 0.7333, Validation F1: 0.8649\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [38/47], Training Loss: 0.013471892103552818, Training Accuracy: 0.8750, Training Sensitivity: 0.9149, Training Specificity: 0.8462, Training F1: 0.8600\n",
            "Epoch [38/47], Validation Loss: 0.0219, Validation Accuracy: 0.7188, Validation Sensitivity: 0.5333, Validation Specificity: 0.8824, Validation F1: 0.6400\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [39/47], Training Loss: 0.02885759249329567, Training Accuracy: 0.8661, Training Sensitivity: 0.8384, Training Specificity: 0.8880, Training F1: 0.8469\n",
            "Epoch [39/47], Validation Loss: 0.0374, Validation Accuracy: 0.8438, Validation Sensitivity: 1.0000, Validation Specificity: 0.7059, Validation F1: 0.8571\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [40/47], Training Loss: 0.00988090317696333, Training Accuracy: 0.9241, Training Sensitivity: 0.9271, Training Specificity: 0.9219, Training F1: 0.9128\n",
            "Epoch [40/47], Validation Loss: 0.0397, Validation Accuracy: 0.7500, Validation Sensitivity: 0.8462, Validation Specificity: 0.6842, Validation F1: 0.7333\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch [41/47], Training Loss: 0.010566347278654575, Training Accuracy: 0.8973, Training Sensitivity: 0.8646, Training Specificity: 0.9219, Training F1: 0.8783\n",
            "Epoch [41/47], Validation Loss: 0.0167, Validation Accuracy: 0.8750, Validation Sensitivity: 1.0000, Validation Specificity: 0.7895, Validation F1: 0.8667\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch [42/47], Training Loss: 0.010615349747240543, Training Accuracy: 0.8973, Training Sensitivity: 0.9394, Training Specificity: 0.8640, Training F1: 0.8900\n",
            "Epoch [42/47], Validation Loss: 0.0189, Validation Accuracy: 0.8438, Validation Sensitivity: 0.9412, Validation Specificity: 0.7333, Validation F1: 0.8649\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            "Fold 5: Accuracy: 0.8438, Sensitivity: 0.9412, Specificity: 0.7333, F1: 0.8649\n",
            "Mean Accuracy: 0.8250, CI: (0.746875, 0.915625), p-value: 0.0029\n",
            "Mean Sensitivity: 0.8300, CI: (0.595113600995954, 0.9703703703703704), p-value: 0.0509\n",
            "Mean Specificity: 0.8169, CI: (0.6872072072072072, 0.9466666666666667), p-value: 0.0149\n",
            "Mean F1 Score: 0.7910, CI: (0.6459482038429407, 0.918962577962578), p-value: 0.0207\n",
            "Mean Accuracy: 0.8250, CI: (0.746875, 0.915625), p-value: 0.0029\n",
            "Mean Sensitivity: 0.8300, CI: (0.595113600995954, 0.9703703703703704), p-value: 0.0509\n",
            "Mean Specificity: 0.8169, CI: (0.6872072072072072, 0.9466666666666667), p-value: 0.0149\n",
            "Mean F1 Score: 0.7910, CI: (0.6459482038429407, 0.918962577962578), p-value: 0.0207\n"
          ]
        }
      ]
    }
  ]
}